[
  {
    "id": "NRxydtWup1S",
    "original": "yG_vy3vZYR",
    "number": 113,
    "cdate": 1663849812800,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849812800,
    "tmdate": 1760256840734,
    "tddate": null,
    "forum": "NRxydtWup1S",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling",
      "authorids": [
        "~Keyu_Tian1",
        "~Yi_Jiang2",
        "~qishuai_diao1",
        "~Chen_Lin2",
        "~Liwei_Wang1",
        "~Zehuan_Yuan1"
      ],
      "authors": [
        "Keyu Tian",
        "Yi Jiang",
        "qishuai diao",
        "Chen Lin",
        "Liwei Wang",
        "Zehuan Yuan"
      ],
      "keywords": [
        "Self-Supervised Learning",
        "Masked Autoencoding",
        "Masked Pre-training",
        "Masked Modeling",
        "Convolutional Neural Networks"
      ],
      "TL;DR": "This paper presents a simple yet powerful framework to pre-train convolutional network (convnet) with Sparse masKed modeling.",
      "abstract": "We identify and overcome two key obstacles in extending the success of BERT-style pre-training, or masked image modeling, to convolutional networks (convnets): (i) convolution operation cannot handle irregular, randomly masked input images; (ii) the single-scale nature of BERT pre-training is inconsistent with convnet’s hierarchical structure. For (i), we treat unmasked pixels as sparse voxels of 3D point clouds and use sparse convolution to encode. This is the first use of sparse convolution for 2D masked modeling. For (ii), we develop a hierarchical decoder to reconstruct images from multi-scale encoded features. Our method, called Sparse masKed modeling (SparK), is general: it can be used directly on any convolutional model without backbone modifications. We validate it on both classical (ResNet) and modern (ConvNeXt) models: on three downstream tasks, it surpasses both state-of-the-art contrastive learning and transformer-based masked modeling by similarly large margins (around +1.0%). The improvements on object detection and instance segmentation are more significant (up to +3.5%), validating the strong transferability of features learned. We also find SparK’s favorable scaling behavior by observing more gains on larger networks. All of these findings support the promising future of generative pre-training on convnets. Both codes and pre-trained models have been released at https://github.com/keyu-tian/SparK.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Unsupervised and Self-supervised learning",
      "paperhash": "tian|designing_bert_for_convolutional_networks_sparse_and_hierarchical_masked_modeling",
      "pdf": "/pdf/1f583ce7b466371efb133c5c74c8283ffc7fb6f7.pdf",
      "_bibtex": "@inproceedings{\ntian2023designing,\ntitle={Designing {BERT} for Convolutional Networks: Sparse and Hierarchical Masked Modeling},\nauthor={Keyu Tian and Yi Jiang and qishuai diao and Chen Lin and Liwei Wang and Zehuan Yuan},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=NRxydtWup1S}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/designing-bert-for-convolutional-networks/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279085418,
    "odate": 1664468100000,
    "details": {
      "replyCount": 17,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "zEn1BhaNYsC",
    "original": "JWgYu-tT6xb",
    "number": 156,
    "cdate": 1663849818110,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849818110,
    "tmdate": 1677640447685,
    "tddate": null,
    "forum": "zEn1BhaNYsC",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Minimax Optimal Kernel Operator Learning via Multilevel Training",
      "authorids": [
        "~Jikai_Jin1",
        "~Yiping_Lu1",
        "~Jose_Blanchet1",
        "~Lexing_Ying1"
      ],
      "authors": [
        "Jikai Jin",
        "Yiping Lu",
        "Jose Blanchet",
        "Lexing Ying"
      ],
      "keywords": [],
      "abstract": "Learning mappings between infinite-dimensional function spaces have achieved empirical success in many disciplines of machine learning, including generative modeling, functional data analysis, causal inference, and multi-agent reinforcement learning. In this paper, we study the statistical limit of learning a Hilbert-Schmidt operator between two infinite-dimensional Sobolev reproducing kernel Hilbert spaces. We establish the information-theoretic lower bound in terms of the Sobolev Hilbert-Schmidt norm and show that a regularization that learns the spectral components below the bias contour and ignores the ones above the variance contour can achieve the optimal learning rate. At the same time, the spectral components between the bias and variance contours give us flexibility in designing computationally feasible machine learning algorithms. Based on this observation, we develop a multilevel kernel operator learning algorithm that is optimal when learning linear operators between infinite-dimensional function spaces.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Machine Learning for Sciences (eg biology, physics, health sciences, social sciences, climate/sustainability )",
      "paperhash": "jin|minimax_optimal_kernel_operator_learning_via_multilevel_training",
      "pdf": "/pdf/5133e05de4997ce07895732d586909263b656b92.pdf",
      "_bibtex": "@inproceedings{\njin2023minimax,\ntitle={Minimax Optimal Kernel Operator Learning via Multilevel Training},\nauthor={Jikai Jin and Yiping Lu and Jose Blanchet and Lexing Ying},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=zEn1BhaNYsC}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "supplementary_material": ""
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279087296,
    "odate": 1664468100000,
    "details": {
      "replyCount": 17,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "awnvqZja69",
    "original": "IC-EA9-GM-j",
    "number": 210,
    "cdate": 1663849824286,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849824286,
    "tmdate": 1760256838126,
    "tddate": null,
    "forum": "awnvqZja69",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Image as Set of Points",
      "authorids": [
        "~Xu_Ma2",
        "~Yuqian_Zhou2",
        "~Huan_Wang3",
        "~Can_Qin1",
        "~Bin_Sun1",
        "~Chang_Liu13",
        "~Yun_Fu1"
      ],
      "authors": [
        "Xu Ma",
        "Yuqian Zhou",
        "Huan Wang",
        "Can Qin",
        "Bin Sun",
        "Chang Liu",
        "Yun Fu"
      ],
      "keywords": [
        "Clustering",
        "Image Processing",
        "Context Cluster",
        "Representation"
      ],
      "TL;DR": "We introduce Context Cluster, a new paradigm that considers an image as a set of point and employs clustering method for feature extraction.",
      "abstract": "\nWhat is an image, and how to extract latent features? \nConvolutional Networks (ConvNets) consider an image as organized pixels in a rectangular shape and extract features via convolutional operation in a local region; Vision Transformers (ViTs) treat an image as a sequence of patches and extract features via attention mechanism in a global range. In this work, we introduce a straightforward and promising paradigm for visual representation, which is called Context Clusters. Context clusters (CoCs) view an image as a set of unorganized points and extract features via a simplified clustering algorithm. In detail, each point includes the raw feature (e.g., color) and positional information (e.g., coordinates), and a simplified clustering algorithm is employed to group and extract deep features hierarchically. Our CoCs are convolution- and attention-free, only relying on clustering algorithm for spatial interaction. Owing to the simple design, we show CoCs endow gratifying interpretability via the visualization of the clustering process.  \nOur CoCs aim at providing a new perspective on image and visual representation, which may enjoy broad applications in different domains and exhibit profound insights. Even though we are not targeting SOTA performance, COCs still achieve comparable or even better performance than ConvNets or ViTs on several benchmarks.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "ma|image_as_set_of_points",
      "pdf": "/pdf/839da9c992ee84a8fa5be183d987fa55966e54ff.pdf",
      "_bibtex": "@inproceedings{\nma2023image,\ntitle={Image as Set of Points},\nauthor={Xu Ma and Yuqian Zhou and Huan Wang and Can Qin and Bin Sun and Chang Liu and Yun Fu},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=awnvqZja69}\n}",
      "venue": "ICLR 2023 notable top 5%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/image-as-set-of-points/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279089156,
    "odate": 1664468100000,
    "details": {
      "replyCount": 20,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "k7p_YAO7yE",
    "original": "1LRUGyy_9qL",
    "number": 214,
    "cdate": 1663849824750,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849824750,
    "tmdate": 1760256837952,
    "tddate": null,
    "forum": "k7p_YAO7yE",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction",
      "authorids": [
        "~Bencheng_Liao1",
        "~Shaoyu_Chen1",
        "~Xinggang_Wang1",
        "~Tianheng_Cheng1",
        "~Qian_Zhang7",
        "~Wenyu_Liu3",
        "~Chang_Huang3"
      ],
      "authors": [
        "Bencheng Liao",
        "Shaoyu Chen",
        "Xinggang Wang",
        "Tianheng Cheng",
        "Qian Zhang",
        "Wenyu Liu",
        "Chang Huang"
      ],
      "keywords": [
        "Autonomous Driving",
        "Online Vectorized HD Map Construction",
        "End-to-End"
      ],
      "abstract": "High-definition (HD) map provides abundant and precise environmental information of the driving scene, serving as a fundamental and indispensable component for planning in autonomous driving system. We present MapTR, a structured end-to-end Transformer for efficient online vectorized HD map construction. We propose a unified permutation-equivalent modeling approach, i.e., modeling map element as a point set with a group of equivalent permutations, which accurately describes the shape of map element and stabilizes the learning process. We design a hierarchical query embedding scheme to flexibly encode structured map information and perform hierarchical bipartite matching for map element learning. MapTR achieves the best performance and efficiency with only camera input among existing vectorized map construction approaches on nuScenes dataset. In particular, MapTR-nano runs at real-time inference speed ($25.1$ FPS) on RTX 3090, $8\\times$ faster than the existing state-of-the-art camera-based method while achieving $5.0$ higher mAP. Even compared with the existing state-of-the-art multi-modality method, MapTR-nano achieves $0.7$ higher mAP and $8\\times$ faster inference speed, and MapTR-tiny achieves $13.5$ higher mAP and $3\\times$ faster inference speed. Abundant qualitative results show that MapTR maintains stable and robust map construction quality in complex and various driving scenes. MapTR is of great application value in autonomous driving. Code and more demos are available at https://github.com/hustvl/MapTR.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Applications (eg, speech processing, computer vision, NLP)",
      "paperhash": "liao|maptr_structured_modeling_and_learning_for_online_vectorized_hd_map_construction",
      "TL;DR": "We present a structured end-to-end framework for efficient online vectorized HD map construction.",
      "pdf": "/pdf/f0aa5f3818d2d071eed47bfd84263b7b217b437a.pdf",
      "supplementary_material": "/attachment/b03de645fd5a4e9560131b071f18dbc33600b21b.zip",
      "_bibtex": "@inproceedings{\nliao2023maptr,\ntitle={Map{TR}: Structured Modeling and Learning for Online Vectorized {HD} Map Construction},\nauthor={Bencheng Liao and Shaoyu Chen and Xinggang Wang and Tianheng Cheng and Qian Zhang and Wenyu Liu and Chang Huang},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=k7p_YAO7yE}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/maptr-structured-modeling-and-learning-for/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279089657,
    "odate": 1664468100000,
    "details": {
      "replyCount": 14,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "FeWvD0L_a4",
    "original": "BcSK4kBHmsQ",
    "number": 219,
    "cdate": 1663849825354,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849825354,
    "tmdate": 1677744956404,
    "tddate": null,
    "forum": "FeWvD0L_a4",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Learnable Behavior Control: Breaking Atari Human World Records via Sample-Efficient Behavior Selection",
      "authorids": [
        "~Jiajun_Fan1",
        "~Yuzheng_Zhuang1",
        "~Yuecheng_Liu2",
        "~Jianye_HAO1",
        "~Bin_Wang12",
        "~Jiangcheng_Zhu1",
        "~Hao_Wang25",
        "~Shu-Tao_Xia1"
      ],
      "authors": [
        "Jiajun Fan",
        "Yuzheng Zhuang",
        "Yuecheng Liu",
        "Jianye HAO",
        "Bin Wang",
        "Jiangcheng Zhu",
        "Hao Wang",
        "Shu-Tao Xia"
      ],
      "keywords": [
        "Deep Reinforcement Learning",
        "The Arcade Learning Environment",
        "Human World Records",
        "Behavioral Control"
      ],
      "TL;DR": "We have constructed a general framework to control the behaviors in RL  and achieved SOTA performance in Atari 1B benchmark.",
      "abstract": "The exploration problem is one of the main challenges in deep reinforcement learning (RL). Recent promising works tried to handle the problem with population-based methods, which collect samples with diverse behaviors derived from a population of different exploratory policies. Adaptive policy selection has been adopted for behavior control. However, the behavior selection space is largely limited by the predefined policy population, which further limits behavior diversity.  In this paper, we propose a general framework called Learnable Behavioral Control (LBC) to address the limitation, which a) enables a significantly enlarged behavior selection space via formulating a hybrid behavior mapping from all policies; b) constructs a unified learnable process for behavior selection. We introduce LBC into distributed off-policy actor-critic methods and achieve behavior control via optimizing the selection of the behavior mappings with bandit-based meta-controllers. Our agents have achieved 10077.52% mean human normalized score and surpassed 24 human world records within 1B training frames in the Arcade Learning Environment, which demonstrates our significant state-of-the-art (SOTA) performance without degrading the sample efficiency.",
      "pdf": "/pdf/6576875018fe482d865d62a571a8b8df3278b360.pdf",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "supplementary_material": "/attachment/6746eb48c97dfbd01313f76908b65a38fde4a943.zip",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Reinforcement Learning (eg, decision and control, planning, hierarchical RL, robotics)",
      "paperhash": "fan|learnable_behavior_control_breaking_atari_human_world_records_via_sampleefficient_behavior_selection",
      "_bibtex": "@inproceedings{\nfan2023learnable,\ntitle={Learnable Behavior Control: Breaking Atari Human World Records via Sample-Efficient Behavior Selection},\nauthor={Jiajun Fan and Yuzheng Zhuang and Yuecheng Liu and Jianye HAO and Bin Wang and Jiangcheng Zhu and Hao Wang and Shu-Tao Xia},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=FeWvD0L_a4}\n}",
      "venue": "ICLR 2023 notable top 5%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279089896,
    "odate": 1664468100000,
    "details": {
      "replyCount": 6,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "nUmCcZ5RKF",
    "original": "7T1PFvGWBhV",
    "number": 220,
    "cdate": 1663849825475,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849825475,
    "tmdate": 1760256837649,
    "tddate": null,
    "forum": "nUmCcZ5RKF",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "IS SYNTHETIC DATA FROM GENERATIVE MODELS READY FOR IMAGE RECOGNITION?",
      "authorids": [
        "~Ruifei_He1",
        "~Shuyang_Sun1",
        "~Xin_Yu6",
        "~Chuhui_Xue2",
        "~Wenqing_Zhang1",
        "~Philip_Torr1",
        "~Song_Bai3",
        "~XIAOJUAN_QI2"
      ],
      "authors": [
        "Ruifei He",
        "Shuyang Sun",
        "Xin Yu",
        "Chuhui Xue",
        "Wenqing Zhang",
        "Philip Torr",
        "Song Bai",
        "XIAOJUAN QI"
      ],
      "keywords": [
        "data generation",
        "image recognition",
        "text-to-image synthesis"
      ],
      "abstract": "Recent text-to-image generation models have shown promising results in generating high-fidelity photo-realistic images. Though the results are astonishing to human eyes, how applicable these generated images are for recognition tasks remains under-explored. In this work, we extensively study whether and how synthetic images generated from state-of-the-art text-to-image generation models can be used for image recognition tasks, and focus on two perspectives: synthetic data for improving classification models in the data-scare settings (i.e. zero-shot and few-shot), and synthetic data for large-scale model pre-training for transfer learning. We showcase the powerfulness and shortcomings of synthetic data from existing generative models, and propose strategies for better applying synthetic data for recognition tasks. Code: https://github.com/CVMI-Lab/SyntheticData. ",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Applications (eg, speech processing, computer vision, NLP)",
      "paperhash": "he|is_synthetic_data_from_generative_models_ready_for_image_recognition",
      "TL;DR": "We present the first study on the state-of-the-art text-to-image generation models for image recognition.",
      "pdf": "/pdf/530478d6bc03dbd80ae4d0e00c93647edd522adc.pdf",
      "supplementary_material": "/attachment/090c20f07b954f9d113c8781398891207d0f9dd9.zip",
      "_bibtex": "@inproceedings{\nhe2023is,\ntitle={{IS} {SYNTHETIC} {DATA} {FROM} {GENERATIVE} {MODELS} {READY} {FOR} {IMAGE} {RECOGNITION}?},\nauthor={Ruifei He and Shuyang Sun and Xin Yu and Chuhui Xue and Wenqing Zhang and Philip Torr and Song Bai and XIAOJUAN QI},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=nUmCcZ5RKF}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/is-synthetic-data-from-generative-models/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279089910,
    "odate": 1664468100000,
    "details": {
      "replyCount": 21,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "NAQvF08TcyG",
    "original": "UYZbDk6R2yM",
    "number": 317,
    "cdate": 1663849835866,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849835866,
    "tmdate": 1762941760227,
    "tddate": null,
    "forum": "NAQvF08TcyG",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion",
      "authorids": [
        "~Rinon_Gal1",
        "~Yuval_Alaluf1",
        "~Yuval_Atzmon1",
        "~Or_Patashnik1",
        "~Amit_Haim_Bermano2",
        "~Gal_Chechik1",
        "~Daniel_Cohen-Or1"
      ],
      "authors": [
        "Rinon Gal",
        "Yuval Alaluf",
        "Yuval Atzmon",
        "Or Patashnik",
        "Amit Haim Bermano",
        "Gal Chechik",
        "Daniel Cohen-Or"
      ],
      "keywords": [
        "Personalized generation",
        "text-to-image",
        "inversion"
      ],
      "TL;DR": "We present the task of personalized text-to-image generation, and introduce an inversion-based method that allows us to synthesize novel scenes of user-provided visual concepts, guided by natural language instructions.",
      "abstract": "Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes.\nIn other words, we ask: how can we use language-guided models to turn *our* cat into a painting, or imagine a new product based on *our* favorite toy? \nHere we present a simple approach that allows such creative freedom. Using only $3$-$5$ images of a user-provided concept, like an object or a style, we learn to represent it through new ``words\" in the embedding space of a frozen text-to-image model.\nThese ``words\" can be composed into natural language sentences, guiding *personalized* creation in an intuitive way.\nNotably, we find evidence that a *single* word embedding is sufficient for capturing unique and varied concepts. \nWe compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks. Our code, data and new words will be available.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Generative models",
      "paperhash": "gal|an_image_is_worth_one_word_personalizing_texttoimage_generation_using_textual_inversion",
      "pdf": "/pdf/dd5c5803a1a63bd0d148c2be26a9ee612d1615f8.pdf",
      "_bibtex": "@inproceedings{\ngal2023an,\ntitle={An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion},\nauthor={Rinon Gal and Yuval Alaluf and Yuval Atzmon and Or Patashnik and Amit Haim Bermano and Gal Chechik and Daniel Cohen-Or},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=NAQvF08TcyG}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/an-image-is-worth-one-word-personalizing-text/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279093889,
    "odate": 1664468100000,
    "details": {
      "replyCount": 15,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "D1Iqfm7WTkk",
    "original": "Is50w_OpRw",
    "number": 318,
    "cdate": 1663849835984,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849835984,
    "tmdate": 1677574895371,
    "tddate": null,
    "forum": "D1Iqfm7WTkk",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Neural ePDOs: Spatially Adaptive Equivariant Partial Differential Operator Based  Networks",
      "authorids": [
        "~Lingshen_He1",
        "~Yuxuan_Chen2",
        "~Zhengyang_Shen3",
        "~Yibo_Yang2",
        "~Zhouchen_Lin1"
      ],
      "authors": [
        "Lingshen He",
        "Yuxuan Chen",
        "Zhengyang Shen",
        "Yibo Yang",
        "Zhouchen Lin"
      ],
      "keywords": [
        "Equivariance",
        "Partial differential operators"
      ],
      "TL;DR": "We propose a novel spatial adaptive equivariant PDOs-based network which achieves superior performance than previous works. ",
      "abstract": "Endowing deep learning models with symmetry priors can lead to a considerable performance improvement. As an interesting bridge between physics and deep learning, the equivariant partial differential operators (PDOs) have drawn much researchers' attention recently. However, to ensure the PDOs translation equivariance, previous works have to require coefficient matrices to be constant and spatially shared for their linearity, which could lead to the sub-optimal feature learning at each position. In this work, we propose a novel nonlinear PDOs scheme that is both spatially adaptive and translation equivariant. The coefficient matrices are obtained by local features through a generator rather than spatially shared. Besides, we establish a new theory on incorporating more equivariance like rotations for such PDOs. Based on our theoretical results, we efficiently implement the generator with an equivariant multilayer perceptron (EMLP). As such equivariant PDOs are generated by neural networks, we call them Neural ePDOs. In experiments, we show that our method can significantly improve previous works with smaller model size in various datasets. Especially, we achieve the state-of-the-art performance on the MNIST-rot dataset with only half parameters of the previous best model.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "he|neural_epdos_spatially_adaptive_equivariant_partial_differential_operator_based_networks",
      "pdf": "/pdf/c4b5cb80999f0dac523cd50129e5c768bdbbcaf9.pdf",
      "supplementary_material": "/attachment/2cf284dafe3be848e309cb9aa5d9f95414e91523.zip",
      "_bibtex": "@inproceedings{\nhe2023neural,\ntitle={Neural e{PDO}s: Spatially Adaptive Equivariant Partial Differential Operator Based  Networks},\nauthor={Lingshen He and Yuxuan Chen and Zhengyang Shen and Yibo Yang and Zhouchen Lin},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=D1Iqfm7WTkk}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279093892,
    "odate": 1664468100000,
    "details": {
      "replyCount": 15,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "WbxHAzkeQcn",
    "original": "TYYjM8ZBjU",
    "number": 320,
    "cdate": 1663849836215,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849836215,
    "tmdate": 1760256833895,
    "tddate": null,
    "forum": "WbxHAzkeQcn",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Neural Networks and the Chomsky Hierarchy",
      "authorids": [
        "~Gregoire_Deletang1",
        "~Anian_Ruoss1",
        "~Jordi_Grau-Moya2",
        "~Tim_Genewein1",
        "~Li_Kevin_Wenliang1",
        "~Elliot_Catt1",
        "~Chris_Cundy1",
        "~Marcus_Hutter1",
        "~Shane_Legg1",
        "~Joel_Veness2",
        "~Pedro_A_Ortega1"
      ],
      "authors": [
        "Gregoire Deletang",
        "Anian Ruoss",
        "Jordi Grau-Moya",
        "Tim Genewein",
        "Li Kevin Wenliang",
        "Elliot Catt",
        "Chris Cundy",
        "Marcus Hutter",
        "Shane Legg",
        "Joel Veness",
        "Pedro A Ortega"
      ],
      "keywords": [
        "length generalization",
        "memory-augmented neural networks",
        "recurrent neural networks"
      ],
      "abstract": "Reliable generalization lies at the heart of safe ML and AI. However, understanding when and how neural networks generalize remains one of the most important unsolved problems in the field. In this work, we conduct an extensive empirical study (20'910 models, 15 tasks) to investigate whether insights from the theory of computation can predict the limits of neural network generalization in practice. We demonstrate that grouping tasks according to the Chomsky hierarchy allows us to forecast whether certain architectures will be able to generalize to out-of-distribution inputs. This includes negative results where even extensive amounts of data and training time never lead to any non-trivial generalization, despite models having sufficient capacity to fit the training data perfectly. Our results show that, for our subset of tasks, RNNs and Transformers fail to generalize on non-regular tasks, LSTMs can solve regular and counter-language tasks, and only networks augmented with structured memory (such as a stack or memory tape) can successfully generalize on context-free and context-sensitive tasks.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "paperhash": "deletang|neural_networks_and_the_chomsky_hierarchy",
      "TL;DR": "Large-scale empirical study to determine the computational complexity class of a number of neural network architectures, which allows forecasting limitations on generalization capabilities.",
      "pdf": "/pdf/e3f8464e2b508de864e993df3d7e0162aa25d7ff.pdf",
      "supplementary_material": "/attachment/272b6261239e64c09fa9d924fea847dc30baf41f.zip",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "_bibtex": "@inproceedings{\ndeletang2023neural,\ntitle={Neural Networks and the Chomsky Hierarchy},\nauthor={Gregoire Deletang and Anian Ruoss and Jordi Grau-Moya and Tim Genewein and Li Kevin Wenliang and Elliot Catt and Chris Cundy and Marcus Hutter and Shane Legg and Joel Veness and Pedro A Ortega},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=WbxHAzkeQcn}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/neural-networks-and-the-chomsky-hierarchy/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279093898,
    "odate": 1664468100000,
    "details": {
      "replyCount": 17,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "y5W8tpojhtJ",
    "original": "MbVzgl5Mkjz",
    "number": 337,
    "cdate": 1663849838323,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849838323,
    "tmdate": 1760256833436,
    "tddate": null,
    "forum": "y5W8tpojhtJ",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Neural Collapse Inspired Feature-Classifier Alignment for Few-Shot Class-Incremental Learning",
      "authorids": [
        "~Yibo_Yang2",
        "~Haobo_Yuan1",
        "~Xiangtai_Li1",
        "~Zhouchen_Lin1",
        "~Philip_Torr1",
        "~Dacheng_Tao1"
      ],
      "authors": [
        "Yibo Yang",
        "Haobo Yuan",
        "Xiangtai Li",
        "Zhouchen Lin",
        "Philip Torr",
        "Dacheng Tao"
      ],
      "keywords": [
        "few-shot class-incremental learning",
        "neural collapse"
      ],
      "TL;DR": "An interpretable solution inspired by neural collapse for few-shot class-incremental learning",
      "abstract": "Few-shot class-incremental learning (FSCIL) has been a challenging problem as only a few training samples are accessible for each novel class in the new sessions. Finetuning the backbone or adjusting the classifier prototypes trained in the prior sessions would inevitably cause a misalignment between the feature and classifier of old classes, which explains the well-known catastrophic forgetting problem. In this paper, we deal with this misalignment dilemma in FSCIL inspired by the recently discovered phenomenon named neural collapse, which reveals that the last-layer features of the same class will collapse into a vertex, and the vertices of all classes are aligned with the classifier prototypes, which are formed as a simplex equiangular tight frame (ETF). It corresponds to an optimal geometric structure for classification due to the maximized Fisher Discriminant Ratio. We propose a neural collapse inspired framework for FSCIL. A group of classifier prototypes are pre-assigned as a simplex ETF for the whole label space, including the base session and all the incremental sessions. During training, the classifier prototypes are not learnable, and we adopt a novel loss function that drives the features into their corresponding prototypes. Theoretical analysis shows that our method holds the neural collapse optimality and does not break the feature-classifier alignment in an incremental fashion. Experiments on the miniImageNet, CUB-200, and CIFAR-100 datasets demonstrate that our proposed framework outperforms the state-of-the-art performances. Code address: https://github.com/NeuralCollapseApplications/FSCIL ",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "yang|neural_collapse_inspired_featureclassifier_alignment_for_fewshot_classincremental_learning",
      "pdf": "/pdf/0ffbc09764bcd3fed340d49d2404429bae5277f5.pdf",
      "_bibtex": "@inproceedings{\nyang2023neural,\ntitle={Neural Collapse Inspired Feature-Classifier Alignment for Few-Shot Class-Incremental Learning},\nauthor={Yibo Yang and Haobo Yuan and Xiangtai Li and Zhouchen Lin and Philip Torr and Dacheng Tao},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=y5W8tpojhtJ}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/neural-collapse-inspired-feature-classifier/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279094852,
    "odate": 1664468100000,
    "details": {
      "replyCount": 19,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "JroZRaRw7Eu",
    "original": "ut6ha7EfMAM",
    "number": 391,
    "cdate": 1663849843866,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849843866,
    "tmdate": 1760256831730,
    "tddate": null,
    "forum": "JroZRaRw7Eu",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Token Merging: Your ViT But Faster",
      "authorids": [
        "~Daniel_Bolya1",
        "~Cheng-Yang_Fu1",
        "~Xiaoliang_Dai1",
        "~Peizhao_Zhang1",
        "~Christoph_Feichtenhofer4",
        "~Judy_Hoffman1"
      ],
      "authors": [
        "Daniel Bolya",
        "Cheng-Yang Fu",
        "Xiaoliang Dai",
        "Peizhao Zhang",
        "Christoph Feichtenhofer",
        "Judy Hoffman"
      ],
      "keywords": [
        "token merging",
        "token pruning",
        "inference speed",
        "training speed",
        "throughput",
        "off-the-shelf",
        "fine tuning"
      ],
      "TL;DR": "We merge tokens in a ViT at runtime using a fast custom matching algorithm. Our method, ToMe, can increase training and inference speed, lower training memory, and can be applied with and without training.",
      "abstract": "We introduce Token Merging (ToMe), a simple method to increase the throughput of existing ViT models without needing to train. ToMe gradually combines similar tokens in a transformer using a general and light-weight matching algorithm that is as fast as pruning while being more accurate. Off-the-shelf, ToMe can 2x the throughput of state-of-the-art ViT-L @ 512 and ViT-H @ 518 models on images and 2.2x the throughput of ViT-L on video with only a 0.2-0.3% accuracy drop in each case. ToMe can also easily be applied during training, improving in practice training speed up to 2x for MAE fine-tuning on video. Training with ToMe further minimizes accuracy drop, leading to 2x the throughput of ViT-B on audio for only a 0.4% mAP drop. Qualitatively, we find that ToMe merges object parts into one token, even over multiple frames of video. Overall, ToMe’s accuracy and speed are competitive with state-of-the-art on images, video, and audio.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "bolya|token_merging_your_vit_but_faster",
      "pdf": "/pdf/ef10c4387f0309b8f942d720fdb3ed5bc6ec5b30.pdf",
      "_bibtex": "@inproceedings{\nbolya2023token,\ntitle={Token Merging: Your ViT But Faster},\nauthor={Daniel Bolya and Cheng-Yang Fu and Xiaoliang Dai and Peizhao Zhang and Christoph Feichtenhofer and Judy Hoffman},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=JroZRaRw7Eu}\n}",
      "venue": "ICLR 2023 notable top 5%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/token-merging-your-vit-but-faster/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279096914,
    "odate": 1664468100000,
    "details": {
      "replyCount": 13,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "1C_kSW1-k0",
    "original": "yOoqiJQHS4B",
    "number": 409,
    "cdate": 1663849845958,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849845958,
    "tmdate": 1677703100194,
    "tddate": null,
    "forum": "1C_kSW1-k0",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "STREET: A MULTI-TASK STRUCTURED REASONING AND EXPLANATION BENCHMARK",
      "authorids": [
        "~Danilo_Neves_Ribeiro1",
        "~Shen_Wang2",
        "~Xiaofei_Ma1",
        "~Henghui_Zhu1",
        "~Rui_Dong1",
        "~Deguang_Kong1",
        "~Juliette_Burger1",
        "~Anjelica_Ramos1",
        "~zhiheng_huang4",
        "~William_Yang_Wang2",
        "~George_Karypis1",
        "~Bing_Xiang2",
        "~Dan_Roth3"
      ],
      "authors": [
        "Danilo Neves Ribeiro",
        "Shen Wang",
        "Xiaofei Ma",
        "Henghui Zhu",
        "Rui Dong",
        "Deguang Kong",
        "Juliette Burger",
        "Anjelica Ramos",
        "zhiheng huang",
        "William Yang Wang",
        "George Karypis",
        "Bing Xiang",
        "Dan Roth"
      ],
      "keywords": [
        "natural language understanding",
        "question answering",
        "structured explanations",
        "soft reasoning",
        "dataset"
      ],
      "abstract": "We introduce STREET, a unified multi-task and multi-domain natural language reasoning and explanation benchmark. Unlike most existing question-answering (QA) datasets, we expect models to not only answer questions, but also produce step-by-step structured explanations describing how premises in the question are used to produce intermediate conclusions that can prove the correctness of a certain answer. We perform extensive evaluation with popular language models such as few-shot prompting GPT-3 and fine-tuned T5. We find that these models still lag behind human performance when producing such structured reasoning steps. We believe this work will provide a way for the community to better train and test systems on multi-step reasoning and explanations in natural language.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "paperhash": "ribeiro|street_a_multitask_structured_reasoning_and_explanation_benchmark",
      "TL;DR": "We introduce STREET, a unified multi-task and multi-domain natural language reasoning and explanation benchmark.",
      "pdf": "/pdf/1b74d54ce93b0d4d1558e20806f96d4b743468ea.pdf",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Infrastructure (eg, datasets, competitions, implementations, libraries)",
      "_bibtex": "@inproceedings{\nribeiro2023street,\ntitle={{STREET}: A {MULTI}-{TASK} {STRUCTURED} {REASONING} {AND} {EXPLANATION} {BENCHMARK}},\nauthor={Danilo Neves Ribeiro and Shen Wang and Xiaofei Ma and Henghui Zhu and Rui Dong and Deguang Kong and Juliette Burger and Anjelica Ramos and zhiheng huang and William Yang Wang and George Karypis and Bing Xiang and Dan Roth},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=1C_kSW1-k0}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279097621,
    "odate": 1664468100000,
    "details": {
      "replyCount": 12,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "GMRodZ8OlVr",
    "original": "Fcwxmib2vF8",
    "number": 420,
    "cdate": 1663849847329,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849847329,
    "tmdate": 1760256830744,
    "tddate": null,
    "forum": "GMRodZ8OlVr",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "ISS: Image as Stepping Stone for Text-Guided 3D Shape Generation",
      "authorids": [
        "~Zhengzhe_Liu1",
        "~Peng_Dai3",
        "~Ruihui_Li1",
        "~XIAOJUAN_QI2",
        "~Chi-Wing_Fu2"
      ],
      "authors": [
        "Zhengzhe Liu",
        "Peng Dai",
        "Ruihui Li",
        "XIAOJUAN QI",
        "Chi-Wing Fu"
      ],
      "keywords": [
        "Text",
        "3D shape",
        "CLIP",
        "differentiable rendering"
      ],
      "abstract": "Text-guided 3D shape generation remains challenging due to the absence of large paired text-shape dataset, the substantial semantic gap between these two modalities, and the structural complexity of 3D shapes. This paper presents a new framework called Image as Stepping Stone (ISS) for the task by introducing 2D image as a stepping stone to connect the two modalities and to eliminate the need for paired text-shape data. Our key contribution is a two-stage feature-space-alignment approach that maps CLIP features to shapes by harnessing a pre-trained single-view reconstruction (SVR) model with multi-view supervisions: first map the CLIP image feature to the detail-rich shape space in the SVR model, then map the CLIP text feature to the shape space and optimize the mapping by encouraging CLIP consistency between the input text and the rendered images. Further, we formulate a textguided shape stylization module to dress up the output shapes with novel structures and textures. Beyond existing works on 3D shape generation from text, our new approach is general for creating shapes in a broad range of categories, without requiring paired text-shape data. Experimental results manifest that our approach outperforms the state-of-the-arts and our baselines in terms of fidelity and consistency with text. Further, our approach can stylize the generated shapes with both realistic and fantasy structures and textures. Codes are available at https://github.com/liuzhengzhe/ISS-Image-as-Stepping-Stone-for-Text-Guided-3D-Shape-Generation.",
      "pdf": "/pdf/ed35cc59666dab7baf7735f5b066cdda25ff209c.pdf",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "supplementary_material": "/attachment/cd376a997dab9aeecf90bb23091d4829c0f003a8.zip",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Applications (eg, speech processing, computer vision, NLP)",
      "paperhash": "liu|iss_image_as_stepping_stone_for_textguided_3d_shape_generation",
      "TL;DR": "An efficient text-guided 3D shape generation framework without needing paired text and shape. ",
      "_bibtex": "@inproceedings{\nliu2023iss,\ntitle={{ISS}: Image as Stepping Stone for Text-Guided 3D Shape Generation},\nauthor={Zhengzhe Liu and Peng Dai and Ruihui Li and XIAOJUAN QI and Chi-Wing Fu},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=GMRodZ8OlVr}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/iss-image-as-stepping-stone-for-text-guided/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279098110,
    "odate": 1664468100000,
    "details": {
      "replyCount": 16,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "KbYevcLjnc",
    "original": "Q1Aoj9qNkj",
    "number": 429,
    "cdate": 1663849848707,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849848707,
    "tmdate": 1677746565233,
    "tddate": null,
    "forum": "KbYevcLjnc",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "PEER: A Collaborative Language Model",
      "authorids": [
        "~Timo_Schick1",
        "~Jane_A._Yu1",
        "~Zhengbao_Jiang2",
        "~Fabio_Petroni2",
        "~Patrick_Lewis2",
        "~Gautier_Izacard1",
        "qingfeiyou@fb.com",
        "~Christoforos_Nalmpantis1",
        "~Edouard_Grave1",
        "~Sebastian_Riedel1"
      ],
      "authors": [
        "Timo Schick",
        "Jane A. Yu",
        "Zhengbao Jiang",
        "Fabio Petroni",
        "Patrick Lewis",
        "Gautier Izacard",
        "Qingfei You",
        "Christoforos Nalmpantis",
        "Edouard Grave",
        "Sebastian Riedel"
      ],
      "keywords": [
        "Language Models",
        "Controllability",
        "Prompting",
        "Zero-Shot Learning",
        "Editing"
      ],
      "TL;DR": "We introduce PEER, a language model trained to mimic the collaborative editing process by which humans often write text.",
      "abstract": "Textual content is often the output of a collaborative writing process: We start with an initial draft, ask for suggestions, and repeatedly make changes.\nAgnostic of this process, today’s language models are trained to generate only the final result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions.\nTo address these shortcomings, we introduce PEER, a collaborative language model that is trained to imitate the entire writing process itself. PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to infill various parts of the writing process, enabling the use of self-training techniques for increasing the quality, amount and diversity of training data. This unlocks PEER's full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that PEER achieves strong performance across various domains and editing tasks.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "paperhash": "schick|peer_a_collaborative_language_model",
      "pdf": "/pdf/e50eaf58c25ddb7ed0ec57bcc796b131b7046154.pdf",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Applications (eg, speech processing, computer vision, NLP)",
      "_bibtex": "@inproceedings{\nschick2023peer,\ntitle={{PEER}: A Collaborative Language Model},\nauthor={Timo Schick and Jane A. Yu and Zhengbao Jiang and Fabio Petroni and Patrick Lewis and Gautier Izacard and Qingfei You and Christoforos Nalmpantis and Edouard Grave and Sebastian Riedel},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=KbYevcLjnc}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279098327,
    "odate": 1664468100000,
    "details": {
      "replyCount": 9,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "RecZ9nB9Q4",
    "original": "xWBBtH5z8S",
    "number": 432,
    "cdate": 1663849849059,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849849059,
    "tmdate": 1677766473128,
    "tddate": null,
    "forum": "RecZ9nB9Q4",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Sparse Mixture-of-Experts are Domain Generalizable Learners",
      "authorids": [
        "~Bo_Li23",
        "~Yifei_Shen1",
        "~Jingkang_Yang1",
        "~Yezhen_Wang1",
        "~Jiawei_Ren1",
        "~Tong_Che1",
        "~Jun_Zhang25",
        "~Ziwei_Liu1"
      ],
      "authors": [
        "Bo Li",
        "Yifei Shen",
        "Jingkang Yang",
        "Yezhen Wang",
        "Jiawei Ren",
        "Tong Che",
        "Jun Zhang",
        "Ziwei Liu"
      ],
      "keywords": [
        "domain generalization",
        "mixture-of-experts",
        "algorithmic alignment",
        "visual attributes"
      ],
      "TL;DR": "We theoretically investigate the impact of backbone architecture on DG. We propose a novel SOTA model Generalizable Mixture-of-Experts (GMoE) for DG.",
      "abstract": "Human visual perception can easily generalize to out-of-distributed visual data, which is far beyond the capability of modern machine learning models. Domain generalization (DG) aims to close this gap, with existing DG methods mainly focusing on the loss function design. In this paper, we propose to explore an orthogonal direction, i.e., the design of the backbone architecture. It is motivated by an empirical finding that transformer-based models trained with empirical risk minimization (ERM) outperform CNN-based models employing state-of-the-art (SOTA) DG algorithms on multiple DG datasets. We develop a formal framework to characterize a network's robustness to distribution shifts by studying its architecture's alignment with the correlations in the dataset. This analysis guides us to propose a novel DG model built upon vision transformers, namely \\emph{Generalizable Mixture-of-Experts (GMoE)}. Extensive experiments on DomainBed demonstrate that GMoE trained with ERM outperforms SOTA DG baselines by a large margin. Moreover, GMoE is complementary to existing DG methods and its performance is substantially improved when trained with DG algorithms.",
      "pdf": "/pdf/7bdb46ea980861f27d1fc50dacde68ac444c5231.pdf",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "li|sparse_mixtureofexperts_are_domain_generalizable_learners",
      "supplementary_material": "/attachment/78b2842e3cfed8bd0a85fcfe29c68bb81c9e412f.zip",
      "_bibtex": "@inproceedings{\nli2023sparse,\ntitle={Sparse Mixture-of-Experts are Domain Generalizable Learners},\nauthor={Bo Li and Yifei Shen and Jingkang Yang and Yezhen Wang and Jiawei Ren and Tong Che and Jun Zhang and Ziwei Liu},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=RecZ9nB9Q4}\n}",
      "venue": "ICLR 2023 notable top 5%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279098412,
    "odate": 1664468100000,
    "details": {
      "replyCount": 15,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "UkU05GOH7_6",
    "original": "nOjozy4TJ1I",
    "number": 449,
    "cdate": 1663849850874,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849850874,
    "tmdate": 1677592970796,
    "tddate": null,
    "forum": "UkU05GOH7_6",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Generating Diverse Cooperative Agents by Learning Incompatible Policies",
      "authorids": [
        "~Rujikorn_Charakorn1",
        "~Poramate_Manoonpong1",
        "~Nat_Dilokthanakul1"
      ],
      "authors": [
        "Rujikorn Charakorn",
        "Poramate Manoonpong",
        "Nat Dilokthanakul"
      ],
      "keywords": [
        "multi-agent systems",
        "cooperation",
        "collaboration",
        "reinforcement learning",
        "diversity",
        "robustness"
      ],
      "TL;DR": "We show that incompatible poclies are not similar. LIPO generates diverse cooperative partners by learning a population of incompatible policies.",
      "abstract": "Training a robust cooperative agent requires diverse partner agents. However, obtaining those agents is difficult. Previous works aim to learn diverse behaviors by changing the state-action distribution of agents. But, without information about the task's goal, the diversified agents are not guided to find other important, albeit sub-optimal, solutions: the agents might learn only variations of the same solution. In this work, we propose to learn diverse behaviors via policy compatibility. Conceptually, policy compatibility measures whether policies of interest can coordinate effectively. We theoretically show that incompatible policies are not similar. Thus, policy compatibility—which has been used exclusively as a measure of robustness—can be used as a proxy for learning diverse behaviors. Then, we incorporate the proposed objective into a population-based training scheme to allow concurrent training of multiple agents. Additionally, we use state-action information to induce local variations of each policy. Empirically, the proposed method consistently discovers more solutions than baseline methods across various multi-goal cooperative environments. Finally, in multi-recipe Overcooked, we show that our method produces populations of behaviorally diverse agents, which enables generalist agents trained with such a population to be more robust.\n\nSee our project page at https://bit.ly/marl-lipo\n",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Reinforcement Learning (eg, decision and control, planning, hierarchical RL, robotics)",
      "paperhash": "charakorn|generating_diverse_cooperative_agents_by_learning_incompatible_policies",
      "pdf": "/pdf/ac9e4f47a8a7afc2d31fe69575bb97700dd88071.pdf",
      "supplementary_material": "/attachment/8a7836bf00c2ebc37033b5c24b64acc6cb8e4074.zip",
      "_bibtex": "@inproceedings{\ncharakorn2023generating,\ntitle={Generating Diverse Cooperative Agents by Learning Incompatible Policies},\nauthor={Rujikorn Charakorn and Poramate Manoonpong and Nat Dilokthanakul},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=UkU05GOH7_6}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279100427,
    "odate": 1664468100000,
    "details": {
      "replyCount": 13,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "r9hNv76KoT3",
    "original": "Rmnbgzx7BO",
    "number": 453,
    "cdate": 1663849851345,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849851345,
    "tmdate": 1760256829549,
    "tddate": null,
    "forum": "r9hNv76KoT3",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Rethinking the Expressive Power of GNNs via Graph Biconnectivity",
      "authorids": [
        "~Bohang_Zhang1",
        "~Shengjie_Luo1",
        "~Liwei_Wang1",
        "~Di_He1"
      ],
      "authors": [
        "Bohang Zhang",
        "Shengjie Luo",
        "Liwei Wang",
        "Di He"
      ],
      "keywords": [
        "Graph Neural Networks",
        "Expressive Power",
        "Weisfeiler-Lehman test",
        "Graph Transformer",
        "Biconnectivity"
      ],
      "abstract": "Designing expressive Graph Neural Networks (GNNs) is a central topic in learning graph-structured data. While numerous approaches have been proposed to improve GNNs with respect to the Weisfeiler-Lehman (WL) test, for most of them, there is still a lack of deep understanding of what additional power they can systematically and provably gain. In this paper, we take a fundamentally different perspective to study the expressive power of GNNs beyond the WL test. Specifically, we introduce a novel class of expressivity metrics via graph biconnectivity and highlight their importance in both theory and practice. As biconnectivity can be easily calculated using simple algorithms that have linear computational costs, it is natural to expect that popular GNNs can learn it easily as well. However, after a thorough review of prior GNN architectures, we surprisingly find that most of them are not expressive for any of these metrics. The only exception is the ESAN framework (Bevilacqua et al., 2022), for which we give a theoretical justification of its power. We proceed to introduce a principled and more efficient approach, called the Generalized Distance Weisfeiler-Lehman (GD-WL), which is provably expressive for all biconnectivity metrics. Practically, we show GD-WL can be implemented by a Transformer-like architecture that preserves expressiveness and enjoys full parallelizability. A set of experiments on both synthetic and real datasets demonstrates that our approach can consistently outperform prior GNN architectures.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "zhang|rethinking_the_expressive_power_of_gnns_via_graph_biconnectivity",
      "pdf": "/pdf/be0ebeff1b3c008481709874f052f374a1d68dec.pdf",
      "_bibtex": "@inproceedings{\nzhang2023rethinking,\ntitle={Rethinking the Expressive Power of {GNN}s via Graph Biconnectivity},\nauthor={Bohang Zhang and Shengjie Luo and Liwei Wang and Di He},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=r9hNv76KoT3}\n}",
      "venue": "ICLR 2023 notable top 5%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/rethinking-the-expressive-power-of-gnns-via/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279101311,
    "odate": 1664468100000,
    "details": {
      "replyCount": 13,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "U2WjB9xxZ9q",
    "original": "KuMsq_UbqRu",
    "number": 463,
    "cdate": 1663849852599,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849852599,
    "tmdate": 1760256829299,
    "tddate": null,
    "forum": "U2WjB9xxZ9q",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "3D generation on ImageNet",
      "authorids": [
        "~Ivan_Skorokhodov1",
        "~Aliaksandr_Siarohin1",
        "~Yinghao_Xu1",
        "~Jian_Ren2",
        "~Hsin-Ying_Lee2",
        "~Peter_Wonka1",
        "~Sergey_Tulyakov1"
      ],
      "authors": [
        "Ivan Skorokhodov",
        "Aliaksandr Siarohin",
        "Yinghao Xu",
        "Jian Ren",
        "Hsin-Ying Lee",
        "Peter Wonka",
        "Sergey Tulyakov"
      ],
      "keywords": [
        "3d-generation",
        "gans",
        "generative adversarial networks",
        "knowledge distillation",
        "nerf",
        "stylegan",
        "radiance fields",
        "volume rendering"
      ],
      "TL;DR": "3D generation on ImageNet",
      "abstract": "All existing 3D-from-2D generators are designed for well-curated single-category datasets, where all the objects have (approximately) the same scale, 3D location, and orientation, and the camera always points to the center of the scene. This makes them inapplicable to diverse, in-the-wild datasets of non-alignable scenes rendered from arbitrary camera poses. In this work, we develop a 3D generator with Generic Priors (3DGP): a 3D synthesis framework with more general assumptions about the training data, and show that it scales to very challenging datasets, like ImageNet. Our model is based on three new ideas. First, we incorporate an inaccurate off-the-shelf depth estimator into 3D GAN training via a special depth adaptation module to handle the imprecision. Then, we create a flexible camera model and a regularization strategy for it to learn its distribution parameters during training. Finally, we extend the recent ideas of transferring knowledge from pretrained classifiers into GANs for patch-wise trained models by employing a simple distillation-based technique on top of the discriminator. It achieves more stable training than the existing methods and speeds up the convergence by at least 40%. We explore our model on four datasets: SDIP Dogs $256^2$, SDIP Elephants $256^2$, LSUN Horses $256^2$, and ImageNet $256^2$ and demonstrate that 3DGP outperforms the recent state-of-the-art in terms of both texture and geometry quality. Code and visualizations: https://snap-research.github.io/3dgp.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Generative models",
      "paperhash": "skorokhodov|3d_generation_on_imagenet",
      "pdf": "/pdf/303cbc4bcfff52f24148569ddc61d7213ad090eb.pdf",
      "_bibtex": "@inproceedings{\nskorokhodov2023d,\ntitle={3D generation on ImageNet},\nauthor={Ivan Skorokhodov and Aliaksandr Siarohin and Yinghao Xu and Jian Ren and Hsin-Ying Lee and Peter Wonka and Sergey Tulyakov},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=U2WjB9xxZ9q}\n}",
      "venue": "ICLR 2023 notable top 5%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/3d-generation-on-imagenet/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279103873,
    "odate": 1664468100000,
    "details": {
      "replyCount": 20,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "DSy8tP4WctmZ",
    "original": "1PZdjzk64aJ",
    "number": 495,
    "cdate": 1663849855994,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849855994,
    "tmdate": 1760256828143,
    "tddate": null,
    "forum": "DSy8tP4WctmZ",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Voxurf: Voxel-based Efficient and Accurate Neural Surface Reconstruction",
      "authorids": [
        "~Tong_Wu2",
        "~Jiaqi_Wang1",
        "~Xingang_Pan1",
        "~Xudong_XU1",
        "~Christian_Theobalt2",
        "~Ziwei_Liu1",
        "~Dahua_Lin1"
      ],
      "authors": [
        "Tong Wu",
        "Jiaqi Wang",
        "Xingang Pan",
        "Xudong XU",
        "Christian Theobalt",
        "Ziwei Liu",
        "Dahua Lin"
      ],
      "keywords": [
        "Surface Reconstruction",
        "Neural Radiance Field"
      ],
      "abstract": "Neural surface reconstruction aims to reconstruct accurate 3D surfaces based on multi-view images. Previous methods based on neural volume rendering mostly train a fully implicit model with MLPs, which typically require hours of training for a single scene. Recent efforts explore the explicit volumetric representation to accelerate the optimization via memorizing significant information with learnable voxel grids. However, existing voxel-based methods often struggle in reconstructing fine-grained geometry, even when combined with an SDF-based volume rendering scheme. We reveal that this is because 1) the voxel grids tend to break the color-geometry dependency that facilitates fine-geometry learning, and 2) the under-constrained voxel grids lack spatial coherence and are vulnerable to local minima. In this work, we present Voxurf, a voxel-based surface reconstruction approach that is both efficient and accurate. Voxurf addresses the aforementioned issues via several key designs, including 1) a two-stage training procedure that attains a coherent coarse shape and recovers fine details successively, 2) a dual color network that maintains color-geometry dependency, and 3) a hierarchical geometry feature to encourage information propagation across voxels. Extensive experiments show that Voxurf achieves high efficiency and high quality at the same time. On the DTU benchmark, Voxurf achieves higher reconstruction quality with a 20x training speedup compared to previous fully implicit methods. Our code is publicly available at https://github.com/wutong16/Voxurf/.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "wu|voxurf_voxelbased_efficient_and_accurate_neural_surface_reconstruction",
      "TL;DR": "We present Voxurf, a voxel-based approach for efficient and accurate neural surface reconstruction.",
      "pdf": "/pdf/8385b49620c0d807cbd7621fce00e5a6302d95ef.pdf",
      "supplementary_material": "/attachment/babbd940967b424157f96e3f6a6ba4e961958b05.zip",
      "_bibtex": "@inproceedings{\nwu2023voxurf,\ntitle={Voxurf: Voxel-based Efficient and Accurate Neural Surface Reconstruction},\nauthor={Tong Wu and Jiaqi Wang and Xingang Pan and Xudong XU and Christian Theobalt and Ziwei Liu and Dahua Lin},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=DSy8tP4WctmZ}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/voxurf-voxel-based-efficient-and-accurate/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279107024,
    "odate": 1664468100000,
    "details": {
      "replyCount": 11,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "1NAzMofMnWl",
    "original": "WjG2nLOm37G",
    "number": 496,
    "cdate": 1663849856124,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849856124,
    "tmdate": 1760256828001,
    "tddate": null,
    "forum": "1NAzMofMnWl",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "DaxBench: Benchmarking Deformable Object Manipulation with Differentiable Physics",
      "authorids": [
        "~Siwei_Chen3",
        "~Yiqing_Xu1",
        "~Cunjun_Yu1",
        "~Linfeng_Li2",
        "~Xiao_Ma2",
        "~Zhongwen_Xu1",
        "~David_Hsu1"
      ],
      "authors": [
        "Siwei Chen",
        "Yiqing Xu",
        "Cunjun Yu",
        "Linfeng Li",
        "Xiao Ma",
        "Zhongwen Xu",
        "David Hsu"
      ],
      "keywords": [
        "deformable object manipulation",
        "differentiable physics",
        "benchmark"
      ],
      "abstract": "Deformable object manipulation (DOM) is a long-standing challenge in robotics and has attracted significant interest recently. This paper presents DaXBench, a differentiable simulation framework for DOM. While existing work often focuses on a specific type of deformable objects, DaXBench supports fluid, rope, cloth ...; it provides a general-purpose benchmark to evaluate widely different DOM methods, including planning, imitation learning, and reinforcement learning. DaXBench combines recent advances in deformable object simulation with JAX, a high-performance computational framework. All DOM tasks in DaXBench are wrapped with the OpenAI Gym API for easy integration with DOM algorithms. We hope that DaXBench provides to the research community a comprehensive, standardized benchmark and a valuable tool to support the development and evaluation of new DOM methods. The code and video are available online.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Infrastructure (eg, datasets, competitions, implementations, libraries)",
      "paperhash": "chen|daxbench_benchmarking_deformable_object_manipulation_with_differentiable_physics",
      "pdf": "/pdf/3c5184bef72b67b8b06885038e921049f56dc94e.pdf",
      "supplementary_material": "/attachment/b0144952beb552a91c4702cb44ce0530e1818bc1.zip",
      "_bibtex": "@inproceedings{\nchen2023daxbench,\ntitle={DaxBench: Benchmarking Deformable Object Manipulation with Differentiable Physics},\nauthor={Siwei Chen and Yiqing Xu and Cunjun Yu and Linfeng Li and Xiao Ma and Zhongwen Xu and David Hsu},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=1NAzMofMnWl}\n}",
      "venue": "ICLR 2023 notable top 5%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2210.13066/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279107248,
    "odate": 1664468100000,
    "details": {
      "replyCount": 9,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "g7U9jD_2CUr",
    "original": "S2gHPcwsDH6",
    "number": 503,
    "cdate": 1663849856847,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849856847,
    "tmdate": 1760256827747,
    "tddate": null,
    "forum": "g7U9jD_2CUr",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "EVA3D: Compositional 3D Human Generation from 2D Image Collections",
      "authorids": [
        "~Fangzhou_Hong1",
        "~Zhaoxi_Chen1",
        "~Yushi_LAN1",
        "~Liang_Pan2",
        "~Ziwei_Liu1"
      ],
      "authors": [
        "Fangzhou Hong",
        "Zhaoxi Chen",
        "Yushi LAN",
        "Liang Pan",
        "Ziwei Liu"
      ],
      "keywords": [
        "3D Human Generation",
        "Human NeRF",
        "Inverse Graphics"
      ],
      "abstract": "Inverse graphics aims to recover 3D models from 2D observations. Utilizing differentiable rendering, recent 3D-aware generative models have shown impressive results of rigid object generation using 2D images. However, it remains challenging to generate articulated objects, like human bodies, due to their complexity and diversity in poses and appearances. In this work, we propose, EVA3D, an unconditional 3D human generative model learned from 2D image collections only. EVA3D can sample 3D humans with detailed geometry and render high-quality images (up to 512x256) without bells and whistles (e.g. super resolution). At the core of EVA3D is a compositional human NeRF representation, which divides the human body into local parts. Each part is represented by an individual volume. This compositional representation enables 1) inherent human priors, 2) adaptive allocation of network parameters, 3) efficient training and rendering. Moreover, to accommodate for the characteristics of sparse 2D human image collections (e.g. imbalanced pose distribution), we propose a pose-guided sampling strategy for better GAN learning. Extensive experiments validate that EVA3D achieves state-of-the-art 3D human generation performance regarding both geometry and texture quality. Notably, EVA3D demonstrates great potential and scalability to \"inverse-graphics\" diverse human bodies with a clean framework.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Generative models",
      "paperhash": "hong|eva3d_compositional_3d_human_generation_from_2d_image_collections",
      "TL;DR": "We propose EVA3D, a high-quality unconditional 3D human generative model learned from 2D image collections.",
      "pdf": "/pdf/554f7af511783002653244a77c3f8ac31ae45c7c.pdf",
      "supplementary_material": "/attachment/b66de03e8980bca410d7badcb5f49a558141273e.zip",
      "_bibtex": "@inproceedings{\nhong2023evad,\ntitle={{EVA}3D: Compositional 3D Human Generation from 2D Image Collections},\nauthor={Fangzhou Hong and Zhaoxi Chen and Yushi LAN and Liang Pan and Ziwei Liu},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=g7U9jD_2CUr}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/eva3d-compositional-3d-human-generation-from/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279107430,
    "odate": 1664468100000,
    "details": {
      "replyCount": 9,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "3Y5Uhf5KgGK",
    "original": "fpmzgBNNPCv",
    "number": 510,
    "cdate": 1663849857564,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849857564,
    "tmdate": 1677588798683,
    "tddate": null,
    "forum": "3Y5Uhf5KgGK",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "No Reason for No Supervision: Improved Generalization in Supervised Models",
      "authorids": [
        "~Mert_Bülent_Sarıyıldız1",
        "~Yannis_Kalantidis2",
        "~Karteek_Alahari1",
        "~Diane_Larlus1"
      ],
      "authors": [
        "Mert Bülent Sarıyıldız",
        "Yannis Kalantidis",
        "Karteek Alahari",
        "Diane Larlus"
      ],
      "keywords": [
        "supervised learning",
        "transfer learning",
        "representation learning"
      ],
      "abstract": "We consider the problem of training a deep neural network on a given classification task, e.g., ImageNet-1K (IN1K), so that it excels at both the training task as well as at other (future) transfer tasks. These two seemingly contradictory properties impose a trade-off between improving the model’s generalization and maintaining its performance on the original task. Models trained with self-supervised learning tend to generalize better than their supervised counterparts for transfer learning; yet, they still lag behind supervised models on IN1K. In this paper, we propose a supervised learning setup that leverages the best of both worlds. We extensively analyze supervised training using multi-scale crops for data augmentation and an expendable projector head, and reveal that the design of the projector allows us to control the trade-off between performance on the training task and transferability. We further replace the last layer of class weights with class prototypes computed on the fly using a memory bank and derive two models: t-ReX that achieves a new state of the art for transfer learning and outperforms top methods such as DINO and PAWS on IN1K, and t-ReX* that matches the highly optimized RSB-A1 model on IN1K while performing better on transfer tasks.\nCode and pretrained models: https://europe.naverlabs.com/t-rex",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "saryldz|no_reason_for_no_supervision_improved_generalization_in_supervised_models",
      "pdf": "/pdf/ebca9fb533c934341267ac07467bc1bb652f422e.pdf",
      "_bibtex": "@inproceedings{\nsar{\\i}y{\\i}ld{\\i}z2023no,\ntitle={No Reason for No Supervision: Improved Generalization in Supervised Models},\nauthor={Mert B{\\\"u}lent Sar{\\i}y{\\i}ld{\\i}z and Yannis Kalantidis and Karteek Alahari and Diane Larlus},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=3Y5Uhf5KgGK}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279108129,
    "odate": 1664468100000,
    "details": {
      "replyCount": 14,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "2QGJXyMNoPz",
    "original": "sm5dUvZQhqX",
    "number": 537,
    "cdate": 1663849860323,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849860323,
    "tmdate": 1677783040434,
    "tddate": null,
    "forum": "2QGJXyMNoPz",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "MocoSFL: enabling cross-client collaborative self-supervised learning",
      "authorids": [
        "~Jingtao_Li1",
        "~Lingjuan_Lyu1",
        "daisuke.iso@sony.com",
        "~Chaitali_Chakrabarti1",
        "~Michael_Spranger2"
      ],
      "authors": [
        "Jingtao Li",
        "Lingjuan Lyu",
        "Daisuke Iso",
        "Chaitali Chakrabarti",
        "Michael Spranger"
      ],
      "keywords": [
        "Self-supervised Learning",
        "Collaborative Learning",
        "Split Federated Learning",
        "Momentum Contrast"
      ],
      "TL;DR": "Existing collaborative SSL schemes are not suitable for cross-client applications because of their expensive computation and local data requirements. To address these issues, we propose MocoSFL based on Split Federated Learning and MoCo.",
      "abstract": "Existing collaborative self-supervised learning (SSL) schemes are not suitable for cross-client applications because of their expensive computation and large local data requirements. To address these issues, we propose MocoSFL, a collaborative SSL framework based on Split Federated Learning (SFL) and Momentum Contrast (MoCo). In MocoSFL, the large backbone model is split into a small client-side model and a large server-side model, and only the small client-side model is processed locally on the client's local devices. MocoSFL has three key components: (i) vector concatenation which enables the use of small batch size and reduces computation and memory requirements by orders of magnitude; (ii) feature sharing that helps achieve high accuracy regardless of the quality and volume of local data; (iii) frequent synchronization that helps achieve better non-IID performance because of smaller local model divergence. For a 1,000-client case with non-IID data (each client only has data from 2 random classes of CIFAR-10), MocoSFL can achieve over 84% accuracy with ResNet-18 model. Next we present TAResSFL module that significantly improves the resistance to privacy threats and communication overhead with small sacrifice in accuracy for a MocoSFL system. On a Raspberry Pi 4B device, the MocoSFL-based scheme requires less than 1MB of memory and less than 40MB of communication, and consumes less than 5W power. The code is available at https://github.com/SonyAI/MocoSFL.",
      "pdf": "/pdf/e7d98a4942f9fa3e0236bec53218b97e0792f3ee.pdf",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Unsupervised and Self-supervised learning",
      "paperhash": "li|mocosfl_enabling_crossclient_collaborative_selfsupervised_learning",
      "_bibtex": "@inproceedings{\nli2023mocosfl,\ntitle={Moco{SFL}: enabling cross-client collaborative self-supervised learning},\nauthor={Jingtao Li and Lingjuan Lyu and Daisuke Iso and Chaitali Chakrabarti and Michael Spranger},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=2QGJXyMNoPz}\n}",
      "venue": "ICLR 2023 notable top 5%",
      "venueid": "ICLR.cc/2023/Conference",
      "supplementary_material": "/attachment/ebcf468f684d290bcc742c1331dad3c7f5b890e2.zip"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279109373,
    "odate": 1664468100000,
    "details": {
      "replyCount": 14,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "B73niNjbPs",
    "original": "xpoR6Ke25g",
    "number": 584,
    "cdate": 1663849865203,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849865203,
    "tmdate": 1760256824701,
    "tddate": null,
    "forum": "B73niNjbPs",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Continuous PDE Dynamics Forecasting with Implicit Neural Representations",
      "authorids": [
        "~Yuan_Yin1",
        "~Matthieu_Kirchmeyer1",
        "~Jean-Yves_Franceschi1",
        "~Alain_Rakotomamonjy1",
        "~patrick_gallinari1"
      ],
      "authors": [
        "Yuan Yin",
        "Matthieu Kirchmeyer",
        "Jean-Yves Franceschi",
        "Alain Rakotomamonjy",
        "patrick gallinari"
      ],
      "keywords": [
        "spatiotemporal forecasting",
        "Partial Differential Equations",
        "PDEs",
        "Implicit Neural Representations",
        "INRs",
        "continuous models",
        "generalization",
        "dynamical systems",
        "physics"
      ],
      "TL;DR": "We propose a continuous-time, continuous-space data-driven PDE forecasting model with extensive spatiotemporal extrapolation capabilities including generalization to unseen sparse meshes and resolutions.",
      "abstract": "Effective data-driven PDE forecasting methods often rely on fixed spatial and / or temporal discretizations. This raises limitations in real-world applications like weather prediction where flexible extrapolation at arbitrary spatiotemporal locations is required. We address this problem by introducing a new data-driven approach, DINo, that models a PDE's flow with continuous-time dynamics of spatially continuous functions. This is achieved by embedding spatial observations independently of their discretization via Implicit Neural Representations in a small latent space temporally driven by a learned ODE. This separate and flexible treatment of time and space makes DINo the first data-driven model to combine the following advantages. It extrapolates at arbitrary spatial and temporal locations; it can learn from sparse irregular grids or manifolds; at test time, it generalizes to new grids or resolutions. DINo outperforms alternative neural PDE forecasters in a variety of challenging generalization scenarios on representative PDE systems.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Machine Learning for Sciences (eg biology, physics, health sciences, social sciences, climate/sustainability )",
      "paperhash": "yin|continuous_pde_dynamics_forecasting_with_implicit_neural_representations",
      "pdf": "/pdf/7870a5e000f8b6cb07adaf5eaf38552eecc48b6a.pdf",
      "_bibtex": "@inproceedings{\nyin2023continuous,\ntitle={Continuous {PDE} Dynamics Forecasting with Implicit Neural Representations},\nauthor={Yuan Yin and Matthieu Kirchmeyer and Jean-Yves Franceschi and Alain Rakotomamonjy and patrick gallinari},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=B73niNjbPs}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "supplementary_material": "/attachment/bf8ec3fb30847132318787051601182e5d15d563.zip",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/continuous-pde-dynamics-forecasting-with/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279112234,
    "odate": 1664468100000,
    "details": {
      "replyCount": 21,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "g2YraF75Tj",
    "original": "lmbGDK7W4bj",
    "number": 593,
    "cdate": 1663849865788,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849865788,
    "tmdate": 1760256824366,
    "tddate": null,
    "forum": "g2YraF75Tj",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Towards Stable Test-time Adaptation in Dynamic Wild World",
      "authorids": [
        "~Shuaicheng_Niu1",
        "~Jiaxiang_Wu1",
        "~Yifan_Zhang1",
        "~Zhiquan_Wen1",
        "~Yaofo_Chen1",
        "~Peilin_Zhao2",
        "~Mingkui_Tan2"
      ],
      "authors": [
        "Shuaicheng Niu",
        "Jiaxiang Wu",
        "Yifan Zhang",
        "Zhiquan Wen",
        "Yaofo Chen",
        "Peilin Zhao",
        "Mingkui Tan"
      ],
      "keywords": [
        "Test-time adaptation",
        "Roustness"
      ],
      "TL;DR": "Propose a Sharpness-aware and Reliable entropy minimization method to make online test-time adaptation stable under wild test scenarios 1) small batch sizes; 2) mixed distribution shifts; 3) imbalanced online label distribution shifts.",
      "abstract": "Test-time adaptation (TTA) has shown to be effective at tackling distribution shifts between training and testing data by adapting a given model on test samples. However, the online model updating of TTA may be unstable and this is often a key obstacle preventing existing TTA methods from being deployed in the real world. Specifically, TTA may fail to improve or even harm the model performance when test data have: 1) mixed distribution shifts, 2) small batch sizes, and 3) online imbalanced label distribution shifts, which are quite common in practice. In this paper, we investigate the unstable reasons and find that the batch norm layer is a crucial factor hindering TTA stability. Conversely, TTA can perform more stably with batch-agnostic norm layers, i.e., group or layer norm. However, we observe that TTA with group and layer norms does not always succeed and still suffers many failure cases. By digging into the failure cases, we find that certain noisy test samples with large gradients may disturb the model adaption and result in collapsed trivial solutions, i.e., assigning the same class label for all samples. To address the above collapse issue, we propose a sharpness-aware and reliable entropy minimization method, called SAR, for further stabilizing TTA from two aspects: 1) remove partial noisy samples with large gradients, 2) encourage model weights to go to a flat minimum so that the model is robust to the remaining noisy samples. Promising results demonstrate that SAR performs more stably than prior methods and is computationally efficient under the above wild test scenarios. ",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Unsupervised and Self-supervised learning",
      "paperhash": "niu|towards_stable_testtime_adaptation_in_dynamic_wild_world",
      "pdf": "/pdf/4bf9a568654ef33fe83fe18f5e34b489be3ca06b.pdf",
      "_bibtex": "@inproceedings{\nniu2023towards,\ntitle={Towards Stable Test-time Adaptation in Dynamic Wild World},\nauthor={Shuaicheng Niu and Jiaxiang Wu and Yifan Zhang and Zhiquan Wen and Yaofo Chen and Peilin Zhao and Mingkui Tan},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=g2YraF75Tj}\n}",
      "venue": "ICLR 2023 notable top 5%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 7 code implementations](https://www.catalyzex.com/paper/towards-stable-test-time-adaptation-in/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279112665,
    "odate": 1664468100000,
    "details": {
      "replyCount": 21,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "CsKwavjr7A",
    "original": "qTKaLmcW3M",
    "number": 606,
    "cdate": 1663849867076,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849867076,
    "tmdate": 1760256823373,
    "tddate": null,
    "forum": "CsKwavjr7A",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Visual Recognition with Deep Nearest Centroids",
      "authorids": [
        "~Wenguan_Wang4",
        "~Cheng_Han1",
        "~Tianfei_Zhou2",
        "~Dongfang_Liu1"
      ],
      "authors": [
        "Wenguan Wang",
        "Cheng Han",
        "Tianfei Zhou",
        "Dongfang Liu"
      ],
      "keywords": [
        "Nearest centroids classifier",
        "Cased-base reasoning",
        "Image classification",
        "Image segmentation",
        "Explainable neural networks"
      ],
      "abstract": "We devise deep nearest centroids (DNC), a conceptually elegant yet surprisingly effective network for large-scale visual recognition, by revisiting Nearest Centroids, one of the most classic and simple classifiers. Current deep models learn the classifier in a fully parametric manner, ignoring the latent data structure and lacking simplicity and explainability. DNC instead conducts nonparametric, case-based reasoning; it utilizes sub-centroids of training samples to describe class distributions and clearly explains the classification as the proximity of test data and the class sub-centroids in the feature space. Due to the distance-based nature, the network output dimensionality is flexible, and all the learnable parameters are only for data embedding. That means all the knowledge learnt for ImageNet classification can be completely transferred for pixel recognition learning, under the ‘pre-training and fine-tuning’ paradigm. Apart from its nested simplicity and intuitive decision-making mechanism, DNC can even possess ad-hoc explainability when the sub-centroids are selected as actual training images that humans can view and inspect. Compared with parametric counterparts, DNC performs better on image classification (CIFAR-10, ImageNet) and greatly boots pixel recognition (ADE20K, Cityscapes), with improved transparency and fewer learnable parameters, using various network architectures (ResNet, Swin) and segmentation models (FCN, DeepLabV3, Swin). We feel this work brings fundamental insights into related fields. Our code is available at https://github.com/ChengHan111/DNC.",
      "pdf": "/pdf/cda95db26061bc6fad92b050d82b6ff54e19d475.pdf",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "wang|visual_recognition_with_deep_nearest_centroids",
      "_bibtex": "@inproceedings{\nwang2023visual,\ntitle={Visual Recognition with Deep Nearest Centroids},\nauthor={Wenguan Wang and Cheng Han and Tianfei Zhou and Dongfang Liu},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=CsKwavjr7A}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/visual-recognition-with-deep-nearest/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279113300,
    "odate": 1664468100000,
    "details": {
      "replyCount": 31,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "CPdc77SQfQ5",
    "original": "6GDvvge9l7",
    "number": 653,
    "cdate": 1663849871456,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849871456,
    "tmdate": 1677728998208,
    "tddate": null,
    "forum": "CPdc77SQfQ5",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Win: Weight-Decay-Integrated Nesterov Acceleration for Adaptive Gradient Algorithms",
      "authorids": [
        "~Pan_Zhou3",
        "~Xingyu_Xie1",
        "~Shuicheng_YAN3"
      ],
      "authors": [
        "Pan Zhou",
        "Xingyu Xie",
        "Shuicheng YAN"
      ],
      "keywords": [
        "Optimization acceleration in deep learning",
        "network optimizers",
        "deep learning optimizer",
        "deep learning algorithm"
      ],
      "TL;DR": "We  propose a new and general Weight-decay-Integrated Nesterov acceleration for adaptive  algorithms to enhance their convergence speed, and also analyze their convergence  justify their convergence superiority. ",
      "abstract": "Training deep  networks on  large-scale datasets is computationally challenging.  In this work, we explore the problem of ``\\textit{how to  accelerate  adaptive gradient algorithms in a general manner}\", and aim to provide practical efficiency-boosting insights.   To this end, we   propose an effective and general   {Weight-decay-Integrated Nesterov acceleration} (Win) to accelerate adaptive  algorithms. Taking AdamW and Adam as examples,  we minimize a dynamical   loss per iteration which combines the vanilla training loss and a dynamic regularizer inspired by proximal point method (PPM) to improve the convexity of the problem. To introduce Nesterov-alike-acceleration into AdamW and Adam,  we respectively  use the  first- and second-order Taylor approximations of vanilla loss  to  update the variable  twice. In this way,  we arrive at  our Win acceleration  for AdamW and Adam that uses  a conservative step  and a  reckless step to update twice and then linearly combines these two updates for acceleration. Next,  we  extend  Win acceleration to LAMB and SGD. Our transparent acceleration derivation  could  provide insights for  other accelerated methods and their integration into  adaptive algorithms.  Besides, we prove the convergence of Win-accelerated adaptive  algorithms and  justify their convergence superiority over their non-accelerated counterparts by taking AdamW and Adam as examples.  Experimental results testify to the faster convergence speed and superior performance of our Win-accelerated AdamW, Adam, LAMB and SGD over their non-accelerated counterparts on vision classification tasks and  language modeling tasks with both CNN and Transformer backbones.  We hope Win  shall be a default acceleration option for  popular optimizers in deep learning community to improve the training efficiency. Code will be released at \\url{https://github.com/sail-sg/win}.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "zhou|win_weightdecayintegrated_nesterov_acceleration_for_adaptive_gradient_algorithms",
      "pdf": "/pdf/b3453f304fc9650f5fcaa04d42bafe01e1c5bd1a.pdf",
      "_bibtex": "@inproceedings{\nzhou2023win,\ntitle={Win: Weight-Decay-Integrated Nesterov Acceleration for Adaptive Gradient Algorithms},\nauthor={Pan Zhou and Xingyu Xie and Shuicheng YAN},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=CPdc77SQfQ5}\n}",
      "supplementary_material": "/attachment/8315eae99791a485116b3ad3b92bd7b664ab30c3.zip",
      "venue": "ICLR 2023 notable top 5%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279115578,
    "odate": 1664468100000,
    "details": {
      "replyCount": 16,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "ueYYgo2pSSU",
    "original": "XThy-VPfuER",
    "number": 657,
    "cdate": 1663849871946,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849871946,
    "tmdate": 1760256821505,
    "tddate": null,
    "forum": "ueYYgo2pSSU",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Offline RL with No OOD Actions: In-Sample Learning via Implicit Value Regularization",
      "authorids": [
        "~Haoran_Xu4",
        "~Li_Jiang4",
        "~Jianxiong_Li1",
        "~Zhuoran_Yang1",
        "~Zhaoran_Wang1",
        "~Victor_Wai_Kin_Chan1",
        "~Xianyuan_Zhan1"
      ],
      "authors": [
        "Haoran Xu",
        "Li Jiang",
        "Jianxiong Li",
        "Zhuoran Yang",
        "Zhaoran Wang",
        "Victor Wai Kin Chan",
        "Xianyuan Zhan"
      ],
      "keywords": [
        "Deep Reinforcement Learning",
        "Offline Reinforcement Learning",
        "Value Regularization",
        "Continuous Control"
      ],
      "TL;DR": "We show that some form of Implicit Value Regularization (IVR) will result in the In-sample Learning paradigm in offline RL. We also propose a practical algorithm based on the IVR framework, which obtains new SOTA results.",
      "abstract": "Most offline reinforcement learning (RL) methods suffer from the trade-off between improving the policy to surpass the behavior policy and constraining the policy to limit the deviation from the behavior policy as computing $Q$-values using out-of-distribution (OOD) actions will suffer from errors due to distributional shift. The recent proposed \\textit{In-sample Learning} paradigm (i.e., IQL), which improves the policy by quantile regression using only data samples, shows great promise because it learns an optimal policy without querying the value function of any unseen actions. However, it remains unclear how this type of method handles the distributional shift in learning the value function. In this work, we make a key finding that the in-sample learning paradigm arises under the \\textit{Implicit Value Regularization} (IVR) framework. This gives a deeper understanding of why the in-sample learning paradigm works, i.e., it applies implicit value regularization to the policy. Based on the IVR framework, we further propose two practical algorithms, Sparse $Q$-learning (SQL) and Exponential $Q$-learning (EQL), which adopt the same value regularization used in existing works, but in a complete in-sample manner. Compared with IQL, we find that our algorithms introduce sparsity in learning the value function, making them more robust in noisy data regimes. We also verify the effectiveness of SQL and EQL on D4RL benchmark datasets and show the benefits of in-sample learning by comparing them with CQL in small data regimes. Code is available at \\url{https://github.com/ryanxhr/SQL}.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Reinforcement Learning (eg, decision and control, planning, hierarchical RL, robotics)",
      "paperhash": "xu|offline_rl_with_no_ood_actions_insample_learning_via_implicit_value_regularization",
      "pdf": "/pdf/dbd2c001478b511324bdbec3a393c6f1552fbb3d.pdf",
      "_bibtex": "@inproceedings{\nxu2023offline,\ntitle={Offline {RL} with No {OOD} Actions: In-Sample Learning via Implicit Value Regularization},\nauthor={Haoran Xu and Li Jiang and Jianxiong Li and Zhuoran Yang and Zhaoran Wang and Victor Wai Kin Chan and Xianyuan Zhan},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=ueYYgo2pSSU}\n}",
      "venue": "ICLR 2023 notable top 5%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/offline-rl-with-no-ood-actions-in-sample/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279115745,
    "odate": 1664468100000,
    "details": {
      "replyCount": 15,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "SJ1kSyO2jwu",
    "original": "1pqHnTwBdZn",
    "number": 698,
    "cdate": 1663849875127,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849875127,
    "tmdate": 1762941767011,
    "tddate": null,
    "forum": "SJ1kSyO2jwu",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Human Motion Diffusion Model",
      "authorids": [
        "~Guy_Tevet1",
        "~Sigal_Raab1",
        "~Brian_Gordon1",
        "~Yoni_Shafir1",
        "~Daniel_Cohen-Or1",
        "~Amit_Haim_Bermano2"
      ],
      "authors": [
        "Guy Tevet",
        "Sigal Raab",
        "Brian Gordon",
        "Yoni Shafir",
        "Daniel Cohen-Or",
        "Amit Haim Bermano"
      ],
      "keywords": [],
      "abstract": "Natural and expressive human motion generation is the holy grail of computer animation.\nIt is a challenging task, due to the diversity of possible motion, human perceptual sensitivity to it, and the difficulty of accurately describing it. Therefore, current generative solutions are either low-quality or limited in expressiveness. \nDiffusion models are promising candidates for the human motion domain since they\nhave already shown remarkable generative capabilities in other domains, and their many-to-many nature. \nIn this paper, we introduce Motion Diffusion Model (MDM), a carefully adapted classifier-free diffusion-based generative model for human motion data.  MDM is transformer-based, combining insights from motion generation literature. \nA notable design-choice is that it predicts the sample itself rather than the noise in each step to facilitate the use of established geometric losses on the locations and velocities of the motion, such as the foot contact loss. As we demonstrate, MDM is a generic approach, enabling different modes of conditioning, and different generation tasks. We show that our model is trained with lightweight resources and yet achieves state-of-the-art results on leading benchmarks for text-to-motion, action-to-motion, and unconditioned motion generation. ",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Applications (eg, speech processing, computer vision, NLP)",
      "paperhash": "tevet|human_motion_diffusion_model",
      "pdf": "/pdf/f0e30bdff6d93fdd5a01526aaea18c2fec384fc0.pdf",
      "supplementary_material": "/attachment/74f8649e23bb80a0a4b2c910705fd1764e157648.zip",
      "_bibtex": "@inproceedings{\ntevet2023human,\ntitle={Human Motion Diffusion Model},\nauthor={Guy Tevet and Sigal Raab and Brian Gordon and Yoni Shafir and Daniel Cohen-Or and Amit Haim Bermano},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=SJ1kSyO2jwu}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/human-motion-diffusion-model/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279117832,
    "odate": 1664468100000,
    "details": {
      "replyCount": 17,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "cp5PvcI6w8_",
    "original": "JWq4_fVz18w",
    "number": 706,
    "cdate": 1663849875964,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849875964,
    "tmdate": 1677760171517,
    "tddate": null,
    "forum": "cp5PvcI6w8_",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second",
      "authorids": [
        "~Noah_Hollmann1",
        "~Samuel_Müller1",
        "~Katharina_Eggensperger1",
        "~Frank_Hutter1"
      ],
      "authors": [
        "Noah Hollmann",
        "Samuel Müller",
        "Katharina Eggensperger",
        "Frank Hutter"
      ],
      "keywords": [
        "Tabular Data",
        "AutoML",
        "Green AI",
        "Bayesian prediction",
        "Causal Reasoning",
        "Real-time Machine Learning"
      ],
      "TL;DR": "We present TabPFN, a trained Transformer that learned to solve small tabular data classification problems at SOTA level in less than a second by training on synthetic data generated by integrating principles from causal reasoning and simplicity. ",
      "abstract": "We present TabPFN, a trained Transformer that can do supervised classification for small tabular datasets in less than a second, needs no hyperparameter tuning and is competitive with state-of-the-art classification methods.\nTabPFN is fully entailed in the weights of our network, which accepts training and test samples as a set-valued input and yields predictions for the entire test set in a single forward pass.\nTabPFN is a Prior-Data Fitted Network (PFN) and is trained offline once, to approximate Bayesian inference on synthetic datasets drawn from our prior.\nThis prior incorporates ideas from causal reasoning: It entails a large space of structural causal models with a preference for simple structures.\nOn the $18$ datasets in the OpenML-CC18 suite that contain up to 1000 training data points, up to 100 purely numerical features without missing values, and up to 10 classes, we show that our method clearly outperforms boosted trees and performs on par with complex state-of-the-art AutoML systems with up to $230\\times$ speedup.\nThis increases to a $5\\,700\\times$ speedup when using a GPU. We also validate these results on an additional 67 small numerical datasets from OpenML.\nWe provide all our code, the trained TabPFN, an interactive browser demo and a Colab notebook at https://github.com/automl/TabPFN.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "hollmann|tabpfn_a_transformer_that_solves_small_tabular_classification_problems_in_a_second",
      "pdf": "/pdf/a14bada70718d8e2f05879f7f5dd162a0adbe28c.pdf",
      "_bibtex": "@inproceedings{\nhollmann2023tabpfn,\ntitle={Tab{PFN}: A Transformer That Solves Small Tabular Classification Problems in a Second},\nauthor={Noah Hollmann and Samuel M{\\\"u}ller and Katharina Eggensperger and Frank Hutter},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=cp5PvcI6w8_}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "supplementary_material": "/attachment/a107af382e892457e4ae320cf7c780ca3b07610f.zip"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279118183,
    "odate": 1664468100000,
    "details": {
      "replyCount": 24,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "qLOaeRvteqbx",
    "original": "ASMNqC5eGaE",
    "number": 715,
    "cdate": 1663849877060,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849877060,
    "tmdate": 1760256819871,
    "tddate": null,
    "forum": "qLOaeRvteqbx",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Disparate Impact in Differential Privacy from Gradient Misalignment",
      "authorids": [
        "~Maria_S._Esipova1",
        "~Atiyeh_Ashari_Ghomi1",
        "~Yaqiao_Luo2",
        "~Jesse_C_Cresswell1"
      ],
      "authors": [
        "Maria S. Esipova",
        "Atiyeh Ashari Ghomi",
        "Yaqiao Luo",
        "Jesse C Cresswell"
      ],
      "keywords": [
        "Differential privacy",
        "fairness",
        "privacy"
      ],
      "TL;DR": "DPSGD can have unfair outcomes on protected groups because of direction errors caused by per-sample gradient clipping, but unfairness can be dramatically reduced with a global clipping technique.",
      "abstract": "As machine learning becomes more widespread throughout society, aspects including data privacy and fairness must be carefully considered, and are crucial for deployment in highly regulated industries. Unfortunately, the application of privacy enhancing technologies can worsen unfair tendencies in models. In particular, one of the most widely used techniques for private model training, differentially private stochastic gradient descent (DPSGD), frequently intensifies disparate impact on groups within data. In this work we study the fine-grained causes of unfairness in DPSGD and identify gradient misalignment due to inequitable gradient clipping as the most significant source. This observation leads us to a new method for reducing unfairness by preventing gradient misalignment in DPSGD.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "paperhash": "esipova|disparate_impact_in_differential_privacy_from_gradient_misalignment",
      "pdf": "/pdf/6101c980f75602f74261af8068d5b04eb74e1476.pdf",
      "supplementary_material": "/attachment/183a34edf3b3ad187c2311e607bb2b9298d34328.zip",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Social Aspects of Machine Learning (eg, AI safety, fairness, privacy, interpretability, human-AI interaction, ethics)",
      "_bibtex": "@inproceedings{\nesipova2023disparate,\ntitle={Disparate Impact in Differential Privacy from Gradient Misalignment},\nauthor={Maria S. Esipova and Atiyeh Ashari Ghomi and Yaqiao Luo and Jesse C Cresswell},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=qLOaeRvteqbx}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/disparate-impact-in-differential-privacy-from/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279120463,
    "odate": 1664468100000,
    "details": {
      "replyCount": 17,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "CWmvjOEhgH-",
    "original": "RbXCu908Dy",
    "number": 719,
    "cdate": 1663849877417,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849877417,
    "tmdate": 1760256819617,
    "tddate": null,
    "forum": "CWmvjOEhgH-",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "MPCFORMER: FAST, PERFORMANT AND PRIVATE TRANSFORMER INFERENCE WITH MPC",
      "authorids": [
        "~Dacheng_Li1",
        "~Hongyi_Wang1",
        "~Rulin_Shao1",
        "~Han_Guo1",
        "~Eric_Xing1",
        "~Hao_Zhang2"
      ],
      "authors": [
        "Dacheng Li",
        "Hongyi Wang",
        "Rulin Shao",
        "Han Guo",
        "Eric Xing",
        "Hao Zhang"
      ],
      "keywords": [
        "Secure Multiparty Computation",
        "Privacy",
        "Machine Learning",
        "Transformer model"
      ],
      "TL;DR": "We develop a framework that allows fast, performant, and private inference with MPC for Transformer models.",
      "abstract": "Enabling private inference is crucial for many cloud inference services that are based on Transformer models. However, existing private inference solutions can increase the inference latency by more than 60$\\times$ or significantly compromise the inference quality. In this paper, we design the framework MPCFORMER as a practical solution, using Secure Multi-Party Computation (MPC) and Knowledge Distillation (KD). Through extensive evaluations, we show that MPCFORMER significantly speeds up Transformer inference in MPC settings while achieving similar ML performance to the input model. On the IMDb dataset, it achieves similar performance to $\\text{BERT}_\\text{BASE}$, while being 5.3$\\times$ faster. On the GLUE benchmark, it achieves 97% performance of $\\text{BERT}_\\text{BASE}$ with a 2.2$\\times$ speedup. MPCFORMER remains effective with different trained Transformer weights such as $\\text{ROBERTA}_\\text{BASE}$ and larger models including $\\text{BERT}_\\text{LARGE}$. Code is available at https://github.com/MccRee177/MPCFormer.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Social Aspects of Machine Learning (eg, AI safety, fairness, privacy, interpretability, human-AI interaction, ethics)",
      "paperhash": "li|mpcformer_fast_performant_and_private_transformer_inference_with_mpc",
      "pdf": "/pdf/f2f107f5dbed42ef3523a9abb2677e2c00c61c31.pdf",
      "supplementary_material": "/attachment/8505517d4650b76652fe471fdd6e536b4160ddec.zip",
      "_bibtex": "@inproceedings{\nli2023mpcformer,\ntitle={{MPCFORMER}: {FAST}, {PERFORMANT} {AND} {PRIVATE} {TRANSFORMER} {INFERENCE} {WITH} {MPC}},\nauthor={Dacheng Li and Hongyi Wang and Rulin Shao and Han Guo and Eric Xing and Hao Zhang},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=CWmvjOEhgH-}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/mpcformer-fast-performant-and-private/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279120902,
    "odate": 1664468100000,
    "details": {
      "replyCount": 17,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "KwmPfARgOTD",
    "original": "6XtdxFzzeB",
    "number": 734,
    "cdate": 1663849879249,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849879249,
    "tmdate": 1760256818919,
    "tddate": null,
    "forum": "KwmPfARgOTD",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs",
      "authorids": [
        "~Yi-Lun_Liao1",
        "~Tess_Smidt1"
      ],
      "authors": [
        "Yi-Lun Liao",
        "Tess Smidt"
      ],
      "keywords": [
        "equivariant neural networks",
        "graph neural networks",
        "computational physics",
        "transformer networks"
      ],
      "TL;DR": "We propose an equivariant graph neural network based on Transformer networks and propose a novel attention mechanism, which improves upon self-attention in typical Transformers.",
      "abstract": "Despite their widespread success in various domains, Transformer networks have yet to perform well across datasets in the domain of 3D atomistic graphs such as molecules even when 3D-related inductive biases like translational invariance and rotational equivariance are considered. In this paper, we demonstrate that Transformers can generalize well to 3D atomistic graphs and present Equiformer, a graph neural network leveraging the strength of Transformer architectures and incorporating SE(3)/E(3)-equivariant features based on irreducible representations (irreps). First, we propose a simple and effective architecture by only replacing original operations in Transformers with their equivariant counterparts and including tensor products. Using equivariant operations enables encoding equivariant information in channels of irreps features without complicating graph structures. With minimal modifications to Transformers, this architecture has already achieved strong empirical results. Second, we propose a novel attention mechanism called equivariant graph attention, which improves upon typical attention in Transformers through replacing dot product attention with multi-layer perceptron attention and including non-linear message passing. With these two innovations, Equiformer achieves competitive results to previous models on QM9, MD17 and OC20 datasets.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Machine Learning for Sciences (eg biology, physics, health sciences, social sciences, climate/sustainability )",
      "paperhash": "liao|equiformer_equivariant_graph_attention_transformer_for_3d_atomistic_graphs",
      "pdf": "/pdf/adc86be91e22b350b3f22fb21d5124250509a935.pdf",
      "_bibtex": "@inproceedings{\nliao2023equiformer,\ntitle={Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs},\nauthor={Yi-Lun Liao and Tess Smidt},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=KwmPfARgOTD}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/equiformer-equivariant-graph-attention/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279124256,
    "odate": 1664468100000,
    "details": {
      "replyCount": 34,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "B_pCIsX8KL_",
    "original": "gQ8JUJfT2l3",
    "number": 743,
    "cdate": 1663849880362,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849880362,
    "tmdate": 1677632347807,
    "tddate": null,
    "forum": "B_pCIsX8KL_",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "GRACE-C: Generalized Rate Agnostic Causal Estimation via Constraints",
      "authorids": [
        "~Mohammadsajad_Abavisani1",
        "~David_Danks1",
        "~Sergey_Plis1"
      ],
      "authors": [
        "Mohammadsajad Abavisani",
        "David Danks",
        "Sergey Plis"
      ],
      "keywords": [
        "Causal structure learning",
        "causal learning",
        "graph theory",
        "brain imaging",
        "fMRI"
      ],
      "TL;DR": "A novel method for causal structure discovery in undersampled time-series with three orders of magnitude speedup under the same theoretical guarantees.",
      "abstract": "Graphical structures estimated by causal learning algorithms from time series data can provide highly misleading causal information if the causal timescale of the generating process fails to match the measurement timescale of the data. Existing algorithms provide limited resources to respond to this challenge, and so researchers must either use models that they know are likely misleading, or else forego causal learning entirely. Existing methods face up-to-four distinct shortfalls, as they might a) require that the difference between causal and measurement timescales is known; b) only handle very small number of random variables when the timescale difference is unknown; c) only apply to pairs of variables (albeit with fewer assumptions about prior knowledge); or d) be unable to find a solution given statistical noise in the data. This paper aims to address these challenges. We present an approach that combines constraint programming with both theoretical insights into the problem structure and prior information about admissible causal interactions to achieve speed up of multiple orders of magnitude. The resulting system scales to significantly larger sets of random variables ($>100$) without knowledge of the timescale difference while maintaining  theoretical guarantees. This method is also robust to edge misidentification and can use parametric connection strengths, while optionally finding the optimal among many possible solutions.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "paperhash": "abavisani|gracec_generalized_rate_agnostic_causal_estimation_via_constraints",
      "pdf": "/pdf/c2afd6ed2baef8ba0d051c56c6a88ec4fdfd0cd9.pdf",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Probabilistic Methods (eg, variational inference, causal inference, Gaussian processes)",
      "_bibtex": "@inproceedings{\nabavisani2023gracec,\ntitle={{GRACE}-C: Generalized Rate Agnostic Causal Estimation via Constraints},\nauthor={Mohammadsajad Abavisani and David Danks and Sergey Plis},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=B_pCIsX8KL_}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279125259,
    "odate": 1664468100000,
    "details": {
      "replyCount": 16,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "Bo7eeXm6An8",
    "original": "grVbSRF4POL",
    "number": 744,
    "cdate": 1663849880481,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849880481,
    "tmdate": 1760256818355,
    "tddate": null,
    "forum": "Bo7eeXm6An8",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Multi-lingual Evaluation of Code Generation Models",
      "authorids": [
        "~Ben_Athiwaratkun1",
        "~Sanjay_Krishna_Gouda1",
        "~Zijian_Wang1",
        "~Xiaopeng_Li1",
        "tiayuche@amazon.com",
        "~Ming_Tan2",
        "~Wasi_Uddin_Ahmad1",
        "~Shiqi_Wang2",
        "~Qing_Sun2",
        "~Mingyue_Shang1",
        "~Sujan_Kumar_Gonugondla1",
        "~Hantian_Ding1",
        "~Varun_Kumar3",
        "~Nathan_Fulton2",
        "~Arash_Farahani1",
        "~Siddhartha_Jain1",
        "~Robert_Giaquinto1",
        "~Haifeng_Qian1",
        "~Murali_Krishna_Ramanathan1",
        "~Ramesh_Nallapati1",
        "~Baishakhi_Ray2",
        "~Parminder_Bhatia1",
        "sudipta@amazon.com",
        "~Dan_Roth3",
        "~Bing_Xiang2"
      ],
      "authors": [
        "Ben Athiwaratkun",
        "Sanjay Krishna Gouda",
        "Zijian Wang",
        "Xiaopeng Li",
        "Yuchen Tian",
        "Ming Tan",
        "Wasi Uddin Ahmad",
        "Shiqi Wang",
        "Qing Sun",
        "Mingyue Shang",
        "Sujan Kumar Gonugondla",
        "Hantian Ding",
        "Varun Kumar",
        "Nathan Fulton",
        "Arash Farahani",
        "Siddhartha Jain",
        "Robert Giaquinto",
        "Haifeng Qian",
        "Murali Krishna Ramanathan",
        "Ramesh Nallapati",
        "Baishakhi Ray",
        "Parminder Bhatia",
        "Sudipta Sengupta",
        "Dan Roth",
        "Bing Xiang"
      ],
      "keywords": [
        "code generation",
        "execution-based evaluation",
        "test-based evaluation",
        "language models",
        "multi-lingual code generation benchmark",
        "code insertion",
        "code summarization",
        "robustness for code",
        "code translation",
        "zero-shot code translation",
        "multi-lingual",
        "mono-lingual",
        "language models."
      ],
      "abstract": "We present two new benchmarks, MBXP and Multilingual HumanEval, designed to evaluate code completion models in over 10 programming languages. These datasets are generated using a conversion framework that transpiles prompts and test cases from the original MBPP and HumanEval datasets into the corresponding data in the target language. By using these benchmarks, we are able to assess the performance of code generation models in a multi-lingual fashion, and discovered generalization ability of language models on out-of-domain languages, advantages of multi-lingual models over mono-lingual, the ability of  few-shot prompting to teach the model new languages, and zero-shot translation abilities. In addition, we use our code generation model to perform large-scale bootstrapping to obtain synthetic canonical solutions in several languages, which can be used for other code-related evaluations such as code insertion, robustness, or summarization tasks.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "athiwaratkun|multilingual_evaluation_of_code_generation_models",
      "pdf": "/pdf/c2ba4659e44c45ec67969ec9a74097a37184ad62.pdf",
      "_bibtex": "@inproceedings{\nathiwaratkun2023multilingual,\ntitle={Multi-lingual Evaluation of Code Generation Models},\nauthor={Ben Athiwaratkun and Sanjay Krishna Gouda and Zijian Wang and Xiaopeng Li and Yuchen Tian and Ming Tan and Wasi Uddin Ahmad and Shiqi Wang and Qing Sun and Mingyue Shang and Sujan Kumar Gonugondla and Hantian Ding and Varun Kumar and Nathan Fulton and Arash Farahani and Siddhartha Jain and Robert Giaquinto and Haifeng Qian and Murali Krishna Ramanathan and Ramesh Nallapati and Baishakhi Ray and Parminder Bhatia and Sudipta Sengupta and Dan Roth and Bing Xiang},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=Bo7eeXm6An8}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 6 code implementations](https://www.catalyzex.com/paper/multi-lingual-evaluation-of-code-generation/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279125573,
    "odate": 1664468100000,
    "details": {
      "replyCount": 11,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "G2Q2Mh3avow",
    "original": "VGOh5fWm8X",
    "number": 748,
    "cdate": 1663849880965,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849880965,
    "tmdate": 1760256818235,
    "tddate": null,
    "forum": "G2Q2Mh3avow",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
      "authorids": [
        "~Andy_Zeng3",
        "~Maria_Attarian1",
        "~brian_ichter1",
        "~Krzysztof_Marcin_Choromanski1",
        "~Adrian_Wong1",
        "swelker@google.com",
        "~Federico_Tombari1",
        "aveek@google.com",
        "~Michael_S_Ryoo1",
        "~Vikas_Sindhwani1",
        "johnnylee@google.com",
        "~Vincent_Vanhoucke1",
        "~Pete_Florence1"
      ],
      "authors": [
        "Andy Zeng",
        "Maria Attarian",
        "brian ichter",
        "Krzysztof Marcin Choromanski",
        "Adrian Wong",
        "Stefan Welker",
        "Federico Tombari",
        "Aveek Purohit",
        "Michael S Ryoo",
        "Vikas Sindhwani",
        "Johnny Lee",
        "Vincent Vanhoucke",
        "Pete Florence"
      ],
      "keywords": [
        "prompt engineering",
        "multimodal applications",
        "visual language models",
        "large language models",
        "commonsense reasoning"
      ],
      "TL;DR": "We present a modular class of systems in which multiple pretrained models may be composed zero-shot via multimodal-informed prompt engineering to capture new multimodal capabilities, without additional finetuning.",
      "abstract": "We investigate how multimodal prompt engineering can use language as the intermediate representation to combine complementary knowledge from different pretrained (potentially multimodal) language models for a variety of tasks. This approach is both distinct from and complementary to the dominant paradigm of joint multimodal training. It also recalls a traditional systems-building view as in classical NLP pipelines, but with prompting large pretrained multimodal models. We refer to these as Socratic Models (SMs): a modular class of systems in which multiple pretrained models may be composed zero-shot via multimodal-informed prompting to capture new multimodal capabilities, without additional finetuning. We show that these systems provide competitive state-of-the-art performance for zero-shot image captioning and video-to-text retrieval, and also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes), and (iii) robot perception and planning. We hope this work provides (a) results for stronger zero-shot baseline performance with analysis also highlighting their limitations, (b) new perspectives for building multimodal systems powered by large pretrained models, and (c) practical application advantages in certain regimes limited by data scarcity, training compute, or model access.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "paperhash": "zeng|socratic_models_composing_zeroshot_multimodal_reasoning_with_language",
      "pdf": "/pdf/92b6e024f8a9e971e8041aa14e06de2802245730.pdf",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Applications (eg, speech processing, computer vision, NLP)",
      "_bibtex": "@inproceedings{\nzeng2023socratic,\ntitle={Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language},\nauthor={Andy Zeng and Maria Attarian and brian ichter and Krzysztof Marcin Choromanski and Adrian Wong and Stefan Welker and Federico Tombari and Aveek Purohit and Michael S Ryoo and Vikas Sindhwani and Johnny Lee and Vincent Vanhoucke and Pete Florence},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=G2Q2Mh3avow}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/socratic-models-composing-zero-shot/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279125732,
    "odate": 1664468100000,
    "details": {
      "replyCount": 13,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "FUORz1tG8Og",
    "original": "RDHxXMF3vMy",
    "number": 761,
    "cdate": 1663849882534,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849882534,
    "tmdate": 1760256817609,
    "tddate": null,
    "forum": "FUORz1tG8Og",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "CROM: Continuous Reduced-Order Modeling of PDEs Using Implicit Neural Representations",
      "authorids": [
        "~Peter_Yichen_Chen1",
        "~Jinxu_Xiang1",
        "~Dong_Heon_Cho2",
        "~Yue_Chang1",
        "~G_A_Pershing1",
        "~Henrique_Teles_Maia1",
        "~Maurizio_M_Chiaramonte1",
        "~Kevin_Thomas_Carlberg1",
        "~Eitan_Grinspun3"
      ],
      "authors": [
        "Peter Yichen Chen",
        "Jinxu Xiang",
        "Dong Heon Cho",
        "Yue Chang",
        "G A Pershing",
        "Henrique Teles Maia",
        "Maurizio M Chiaramonte",
        "Kevin Thomas Carlberg",
        "Eitan Grinspun"
      ],
      "keywords": [
        "PDE",
        "implicit neural representation",
        "neural field",
        "latent space traversal",
        "reduced-order modeling",
        "numerical methods"
      ],
      "TL;DR": "We accelerate PDE solvers via rapid latent space traversal of continuous vector fields leveraging implicit neural representations.",
      "abstract": "The long runtime of high-fidelity partial differential equation (PDE) solvers makes them unsuitable for time-critical applications. We propose to accelerate PDE solvers using reduced-order modeling (ROM). Whereas prior ROM approaches reduce the dimensionality of discretized vector fields, our continuous reduced-order modeling (CROM) approach builds a low-dimensional embedding of the continuous vector fields themselves, not their discretization. We represent this reduced manifold using continuously differentiable neural fields, which may train on any and all available numerical solutions of the continuous system, even when they are obtained using diverse methods or discretizations. We validate our approach on an extensive range of PDEs with training data from voxel grids, meshes, and point clouds. Compared to prior discretization-dependent ROM methods, such as linear subspace proper orthogonal decomposition (POD) and nonlinear manifold neural-network-based autoencoders, CROM features higher accuracy, lower memory consumption, dynamically adaptive resolutions, and applicability to any discretization. For equal latent space dimension, CROM exhibits 79$\\times$ and 49$\\times$ better accuracy, and 39$\\times$ and 132$\\times$ smaller memory footprint, than POD and autoencoder methods, respectively. Experiments demonstrate 109$\\times$ and 89$\\times$ wall-clock speedups over unreduced models on CPUs and GPUs, respectively. Videos and codes are available on the project page: https://crom-pde.github.io",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "paperhash": "chen|crom_continuous_reducedorder_modeling_of_pdes_using_implicit_neural_representations",
      "pdf": "/pdf/0b88dbf1d08790a512a587e20d57a19dea822933.pdf",
      "supplementary_material": "/attachment/41475a8caf3c6970b181a03a01741218f62703b3.zip",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Machine Learning for Sciences (eg biology, physics, health sciences, social sciences, climate/sustainability )",
      "_bibtex": "@inproceedings{\nchen2023crom,\ntitle={{CROM}: Continuous Reduced-Order Modeling of {PDE}s Using Implicit Neural Representations},\nauthor={Peter Yichen Chen and Jinxu Xiang and Dong Heon Cho and Yue Chang and G A Pershing and Henrique Teles Maia and Maurizio M Chiaramonte and Kevin Thomas Carlberg and Eitan Grinspun},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=FUORz1tG8Og}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/crom-continuous-reduced-order-modeling-of/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279126565,
    "odate": 1664468100000,
    "details": {
      "replyCount": 14,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "rvsbw2YthH_",
    "original": "HqQPLKJPW2",
    "number": 788,
    "cdate": 1663849885581,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849885581,
    "tmdate": 1760256816742,
    "tddate": null,
    "forum": "rvsbw2YthH_",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "The Trade-off between Universality and Label Efficiency of Representations from Contrastive Learning",
      "authorids": [
        "~Zhenmei_Shi1",
        "~Jiefeng_Chen2",
        "~Kunyang_Li1",
        "~Jayaram_Raghuram1",
        "~Xi_Wu1",
        "~Yingyu_Liang1",
        "~Somesh_Jha1"
      ],
      "authors": [
        "Zhenmei Shi",
        "Jiefeng Chen",
        "Kunyang Li",
        "Jayaram Raghuram",
        "Xi Wu",
        "Yingyu Liang",
        "Somesh Jha"
      ],
      "keywords": [
        "Contrastive Learning",
        "Self-Supervised Learning",
        "Foundation Model",
        "Complexity"
      ],
      "TL;DR": "We focus on contrastive learning and systematically study a trade-off between label efficiency and universality both empirically and theoretically.",
      "abstract": "Pre-training representations (a.k.a. foundation models) has recently become a prevalent learning paradigm, where one first pre-trains a representation using large-scale unlabeled data, and then learns simple predictors on top of the representation using small labeled data from the downstream tasks. There are two key desiderata for the representation: label efficiency (the ability to learn an accurate classifier on top of the representation with a small amount of labeled data) and universality (usefulness across a wide range of downstream tasks). In this paper, we focus on one of the most popular instantiations of this paradigm: contrastive learning with linear probing, i.e., learning a linear predictor on the representation pre-trained by contrastive learning. We show that there exists a trade-off between the two desiderata so that one may not be able to achieve both simultaneously. \nSpecifically, we provide analysis using a theoretical data model and show that,  while more diverse pre-training data result in more diverse features for different tasks (improving universality), it puts less emphasis on task-specific features, giving rise to larger sample complexity for down-stream supervised tasks, and thus worse prediction performance. Guided by this analysis, we propose a contrastive regularization method to improve the trade-off. We validate our analysis and method empirically with systematic experiments using real-world datasets and foundation models.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "paperhash": "shi|the_tradeoff_between_universality_and_label_efficiency_of_representations_from_contrastive_learning",
      "pdf": "/pdf/043603199adc5b3a50a0bd4a9a36f0faea6f3b13.pdf",
      "supplementary_material": "/attachment/2f581784186d9fa1b9570393ac7665f9443bc51b.zip",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Unsupervised and Self-supervised learning",
      "_bibtex": "@inproceedings{\nshi2023the,\ntitle={The Trade-off between Universality and Label Efficiency of Representations from Contrastive Learning},\nauthor={Zhenmei Shi and Jiefeng Chen and Kunyang Li and Jayaram Raghuram and Xi Wu and Yingyu Liang and Somesh Jha},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=rvsbw2YthH_}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/the-trade-off-between-universality-and-label/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279127986,
    "odate": 1664468100000,
    "details": {
      "replyCount": 16,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "urF_CBK5XC0",
    "original": "WAkQaTktOGZ",
    "number": 811,
    "cdate": 1663849888573,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849888573,
    "tmdate": 1760256815833,
    "tddate": null,
    "forum": "urF_CBK5XC0",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Generative Augmented Flow Networks",
      "authorids": [
        "~Ling_Pan1",
        "~Dinghuai_Zhang1",
        "~Aaron_Courville3",
        "~Longbo_Huang2",
        "~Yoshua_Bengio1"
      ],
      "authors": [
        "Ling Pan",
        "Dinghuai Zhang",
        "Aaron Courville",
        "Longbo Huang",
        "Yoshua Bengio"
      ],
      "keywords": [
        "Generative Flow Networks (GFlowNets)",
        "Exploration"
      ],
      "TL;DR": "We propose a novel GFlowNet learning framework to incorporate intermediate rewards represented by intrinsic motivation to improve exploration.",
      "abstract": "The Generative Flow Network is a probabilistic framework where an agent learns a stochastic policy for object generation, such that the probability of generating an object is proportional to a given reward function. Its effectiveness has been shown in discovering high-quality and diverse solutions, compared to reward-maximizing reinforcement learning-based methods. Nonetheless, GFlowNets only learn from rewards of the terminal states, which can limit its applicability. Indeed, intermediate rewards play a critical role in learning, for example from intrinsic motivation to provide intermediate feedback even in particularly challenging sparse reward tasks. Inspired by this, we propose Generative Augmented Flow Networks (GAFlowNets), a novel learning framework to incorporate intermediate rewards into GFlowNets. We specify intermediate rewards by intrinsic motivation to tackle the exploration problem in sparse reward environments. GAFlowNets can leverage edge-based and state-based intrinsic rewards in a joint way to improve exploration. Based on extensive experiments on the GridWorld task, we demonstrate the effectiveness and efficiency of GAFlowNet in terms of convergence, performance, and diversity of solutions. We further show that GAFlowNet is scalable to a more complex and large-scale molecule generation domain, where it achieves consistent and significant performance improvement.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Probabilistic Methods (eg, variational inference, causal inference, Gaussian processes)",
      "paperhash": "pan|generative_augmented_flow_networks",
      "pdf": "/pdf/6f7969e92eef7ad5bb4561e7dbd141decf138128.pdf",
      "_bibtex": "@inproceedings{\npan2023generative,\ntitle={Generative Augmented Flow Networks},\nauthor={Ling Pan and Dinghuai Zhang and Aaron Courville and Longbo Huang and Yoshua Bengio},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=urF_CBK5XC0}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/generative-augmented-flow-networks/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279129389,
    "odate": 1664468100000,
    "details": {
      "replyCount": 18,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "iOc57X9KM54",
    "original": "Y5Jg46KcU8",
    "number": 822,
    "cdate": 1663849889788,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849889788,
    "tmdate": 1677741643056,
    "tddate": null,
    "forum": "iOc57X9KM54",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Neuro-Symbolic Procedural Planning with Commonsense Prompting",
      "authorids": [
        "~Yujie_Lu1",
        "~Weixi_Feng2",
        "~Wanrong_Zhu1",
        "~Wenda_Xu1",
        "~Xin_Eric_Wang2",
        "~Miguel_Eckstein1",
        "~William_Yang_Wang2"
      ],
      "authors": [
        "Yujie Lu",
        "Weixi Feng",
        "Wanrong Zhu",
        "Wenda Xu",
        "Xin Eric Wang",
        "Miguel Eckstein",
        "William Yang Wang"
      ],
      "keywords": [
        "Procedural Planning",
        "Commonsense Knowledge",
        "Prompting",
        "Neuro-Symbolic"
      ],
      "TL;DR": "We propose a neuro-symbolic procedural planner that elicits procedural planning knowledge from the large language models with commonsense-infused prompting. We achieve state-of-the-art performance on WikiHow and RobotHow.",
      "abstract": "Procedural planning aims to implement complex high-level goals by decomposition into simpler low-level steps. Although procedural planning is a basic skill set for humans in daily life, it remains a challenge for large language models (LLMs) that lack a deep understanding of the cause-effect relations in procedures. Previous methods require manual exemplars to acquire procedural planning knowledge from LLMs in the zero-shot setting. However, such elicited pre-trained knowledge in LLMs induces spurious correlations between goals and steps, which impair the model generalization to unseen tasks. In contrast, this paper proposes a neuro-symbolic procedural PLANner (PLAN) that elicits procedural planning knowledge from the LLMs with commonsense-infused prompting. To mitigate spurious goal-step correlations, we use symbolic program executors on the latent procedural representations to formalize prompts from commonsense knowledge bases as a causal intervention toward the Structural Causal Model. Both automatic and human evaluations on WikiHow and RobotHow show the superiority of PLAN on procedural planning without further training or manual exemplars.",
      "pdf": "/pdf/3af66a16e02e6ec05187d765b1d2da8cabae2719.pdf",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Applications (eg, speech processing, computer vision, NLP)",
      "paperhash": "lu|neurosymbolic_procedural_planning_with_commonsense_prompting",
      "supplementary_material": "/attachment/503b1c818fb6638d3adc2c7745afbc1fe7f29592.zip",
      "_bibtex": "@inproceedings{\nlu2023neurosymbolic,\ntitle={Neuro-Symbolic Procedural Planning with Commonsense Prompting},\nauthor={Yujie Lu and Weixi Feng and Wanrong Zhu and Wenda Xu and Xin Eric Wang and Miguel Eckstein and William Yang Wang},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=iOc57X9KM54}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279129962,
    "odate": 1664468100000,
    "details": {
      "replyCount": 34,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "2RwXVje1rAh",
    "original": "J7kgohxuWga",
    "number": 831,
    "cdate": 1663849890858,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849890858,
    "tmdate": 1760256815353,
    "tddate": null,
    "forum": "2RwXVje1rAh",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Exploring Active 3D Object Detection from a Generalization Perspective",
      "authorids": [
        "~Yadan_Luo1",
        "~Zhuoxiao_Chen1",
        "~Zijian_Wang2",
        "~Xin_Yu1",
        "~Zi_Huang1",
        "~Mahsa_Baktashmotlagh1"
      ],
      "authors": [
        "Yadan Luo",
        "Zhuoxiao Chen",
        "Zijian Wang",
        "Xin Yu",
        "Zi Huang",
        "Mahsa Baktashmotlagh"
      ],
      "keywords": [
        "Active Learning",
        "3D Object Detection",
        "Lidar Point Clouds"
      ],
      "abstract": "To alleviate the high annotation cost in LiDAR-based 3D object detection, active learning is a promising solution that learns to select only a small portion of unlabeled data to annotate, without compromising model performance. Our empirical study, however, suggests that mainstream uncertainty-based and diversity-based active learning policies are not effective when applied in the 3D detection task, as they fail to balance the trade-off between point cloud informativeness and box-level annotation costs. To overcome this limitation, we jointly investigate three novel criteria in our framework CRB for point cloud acquisition - label conciseness, feature representativeness and \ngeometric balance, which hierarchically filters out the point clouds of redundant 3D bounding box labels, latent features and geometric characteristics (e.g., point cloud density) from the unlabeled sample pool and greedily selects informative ones with fewer objects to annotate. Our theoretical analysis demonstrates that the proposed criteria aligns the marginal distributions of the selected subset and the prior distributions of the unseen test set, and minimizes the upper bound of the generalization error. To validate the effectiveness and applicability of CRB, we conduct extensive experiments on the two benchmark 3D object detection datasets of KITTI and Waymo and examine both one-stage (i.e., Second) and two-stage 3D detector (i.e., PV-RCNN). Experiments evidence that the proposed approach outperforms existing active learning strategies and achieves fully supervised performance requiring $1\\%$ and $8\\%$ annotations of bounding boxes and point clouds, respectively. ",
      "pdf": "/pdf/cbdf54e075523d503dc1b31538bc70e029256b15.pdf",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "General Machine Learning (ie none of the above)",
      "paperhash": "luo|exploring_active_3d_object_detection_from_a_generalization_perspective",
      "supplementary_material": "/attachment/f51cb80f98f84e37de4a788ea173a0b9dd425ebf.zip",
      "_bibtex": "@inproceedings{\nluo2023exploring,\ntitle={Exploring Active 3D Object Detection from a Generalization Perspective},\nauthor={Yadan Luo and Zhuoxiao Chen and Zijian Wang and Xin Yu and Zi Huang and Mahsa Baktashmotlagh},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=2RwXVje1rAh}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/exploring-active-3d-object-detection-from-a/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279130318,
    "odate": 1664468100000,
    "details": {
      "replyCount": 19,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "rLwC0_MG-4w",
    "original": "gU1tuuNAXbZ",
    "number": 841,
    "cdate": 1663849892094,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849892094,
    "tmdate": 1677503518703,
    "tddate": null,
    "forum": "rLwC0_MG-4w",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Denoising Diffusion Error Correction Codes",
      "authorids": [
        "~Yoni_Choukroun1",
        "~Lior_Wolf1"
      ],
      "authors": [
        "Yoni Choukroun",
        "Lior Wolf"
      ],
      "keywords": [
        "ECC",
        "Deep Learning",
        "Diffusion Models"
      ],
      "TL;DR": "We propose a novel SOTA Neural error correction decoder based on a new diffusion model.",
      "abstract": "Error correction code (ECC) is an integral part of the physical communication layer, ensuring reliable data transfer over noisy channels. \nRecently, neural decoders have demonstrated their advantage over classical decoding techniques. \nHowever, recent state-of-the-art neural decoders suffer from high complexity and lack the important iterative scheme characteristic of many legacy decoders. \nIn this work, we propose to employ denoising diffusion models for the soft decoding of linear codes at arbitrary block lengths. \nOur framework models the forward channel corruption as a series of diffusion steps that can be reversed iteratively. \nThree contributions are made: (i) a diffusion process suitable for the decoding setting is introduced, (ii) the neural diffusion decoder is conditioned on the number of parity errors, which indicates the level of corruption at a given step, (iii) a line search procedure based on the code's syndrome obtains the optimal reverse diffusion step size. \nThe proposed approach demonstrates the power of diffusion models for ECC and is able to achieve state-of-the-art accuracy, outperforming the other neural decoders by sizable margins, even for a single reverse diffusion step. ",
      "pdf": "/pdf/1ba7d8f5e235d93b8db4a40b633bb42c9494223e.pdf",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "supplementary_material": "/attachment/260b97939babf582c7fc3633869663f03bf8c3da.zip",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "choukroun|denoising_diffusion_error_correction_codes",
      "_bibtex": "@inproceedings{\nchoukroun2023denoising,\ntitle={Denoising Diffusion Error Correction Codes},\nauthor={Yoni Choukroun and Lior Wolf},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=rLwC0_MG-4w}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279130741,
    "odate": 1664468100000,
    "details": {
      "replyCount": 17,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "SEh5SfEQtqB",
    "original": "PxkX_rlwmlp",
    "number": 851,
    "cdate": 1663849893184,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849893184,
    "tmdate": 1760256814680,
    "tddate": null,
    "forum": "SEh5SfEQtqB",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Meta-prediction Model for Distillation-Aware NAS on Unseen Datasets",
      "authorids": [
        "~Hayeon_Lee1",
        "~Sohyun_An1",
        "~Minseon_Kim1",
        "~Sung_Ju_Hwang1"
      ],
      "authors": [
        "Hayeon Lee",
        "Sohyun An",
        "Minseon Kim",
        "Sung Ju Hwang"
      ],
      "keywords": [
        "Neural Architecture Search",
        "Meta Learning"
      ],
      "TL;DR": "We propose a one-shot meta accuracy prediction model which can predict a given architecture's final performances on a dataset when performing KD with a given teacher, without having to actually train it on the target task. ",
      "abstract": "Distillation-aware Neural Architecture Search (DaNAS) aims to search for an optimal student architecture that obtains the best performance and/or efficiency when distilling the knowledge from a given teacher model. Previous DaNAS methods have mostly tackled the search for the neural architecture for fixed datasets and the teacher, which are not generalized well on a new task consisting of an unseen dataset and an unseen teacher, thus need to perform a costly search for any new combination of the datasets and the teachers. For standard NAS tasks without KD, meta-learning-based computationally efficient NAS methods have been proposed, which learn the generalized search process over multiple tasks (datasets) and transfer the knowledge obtained over those tasks to a new task. However, since they assume learning from scratch without KD from a teacher, they might not be ideal for DaNAS scenarios. To eliminate the excessive computational cost of DaNAS methods and the sub-optimality of rapid NAS methods, we propose a distillation-aware meta-accuracy prediction model, DaSS (Distillation-aware Student Search), which can predict a given architecture's final performances on a dataset when performing KD with a given teacher, without having actually to train it on the target task. The experimental results demonstrate that our proposed meta-prediction model successfully generalizes to multiple unseen datasets for DaNAS tasks, largely outperforming existing meta-NAS methods and rapid NAS baselines. Code is available at https://github.com/CownowAn/DaSS.",
      "pdf": "/pdf/80703f68458650e155bc0dd7dd6c988a91fbc1be.pdf",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "lee|metaprediction_model_for_distillationaware_nas_on_unseen_datasets",
      "supplementary_material": "/attachment/de76a4c3147835acec72f50759fee58ce808d98c.zip",
      "_bibtex": "@inproceedings{\nlee2023metaprediction,\ntitle={Meta-prediction Model for Distillation-Aware {NAS} on Unseen Datasets},\nauthor={Hayeon Lee and Sohyun An and Minseon Kim and Sung Ju Hwang},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=SEh5SfEQtqB}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/meta-prediction-model-for-distillation-aware/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279131010,
    "odate": 1664468100000,
    "details": {
      "replyCount": 32,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "JJuP86nBl4q",
    "original": "Or-_zfLE84S",
    "number": 857,
    "cdate": 1663849893887,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849893887,
    "tmdate": 1760256814290,
    "tddate": null,
    "forum": "JJuP86nBl4q",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "LAVA: Data Valuation without Pre-Specified Learning Algorithms",
      "authorids": [
        "~Hoang_Anh_Just1",
        "~Feiyang_Kang1",
        "~Tianhao_Wang2",
        "~Yi_Zeng3",
        "~Myeongseob_Ko1",
        "~Ming_Jin2",
        "~Ruoxi_Jia1"
      ],
      "authors": [
        "Hoang Anh Just",
        "Feiyang Kang",
        "Tianhao Wang",
        "Yi Zeng",
        "Myeongseob Ko",
        "Ming Jin",
        "Ruoxi Jia"
      ],
      "keywords": [
        "data valuation",
        "optimal transport",
        "model agnostic",
        "data-driven"
      ],
      "TL;DR": "We propose LAVA: a novel model-agnostic approach to data valuation using a non-conventional, class-wise Wasserstein discrepancy.",
      "abstract": "Traditionally, data valuation is posed as a problem of equitably splitting the validation performance of a learning algorithm among the training data. As a result, the calculated data values depend on many design choices of the underlying learning algorithm. However, this dependence is undesirable for many use cases of data valuation, such as setting priorities over different data sources in a data acquisition process and informing pricing mechanisms in a data marketplace. In these scenarios, data needs to be valued before the actual analysis and the choice of the learning algorithm is still undetermined then. Another side-effect of the dependence is that to assess the value of individual points, one needs to re-run the learning algorithm with and without a point, which incurs a large computation burden. \n\nThis work leapfrogs over the current limits of data valuation methods by introducing a new framework that can value training data in a way that is oblivious to the downstream learning algorithm. Our main results are as follows. $\\textbf{(1)}$ We develop a proxy for the validation performance associated with a training set based on a non-conventional $\\textit{class-wise}$ $\\textit{Wasserstein distance}$ between the training and the validation set. We show that the distance characterizes the upper bound of the validation performance for any given model under certain Lipschitz conditions. $\\textbf{(2)}$ We develop a novel method to value individual data based on the sensitivity analysis of the $\\textit{class-wise}$ Wasserstein distance. Importantly, these values can be directly obtained $\\textit{for free}$ from the output of off-the-shelf optimization solvers once the Wasserstein distance is computed. $\\textbf{(3) }$We evaluate our new data valuation framework over various use cases related to detecting low-quality data\nand show that, surprisingly, the learning-agnostic feature of our framework enables a $\\textit{significant improvement}$ over the state-of-the-art performance while being $\\textit{orders of magnitude faster.}$ ",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "General Machine Learning (ie none of the above)",
      "paperhash": "just|lava_data_valuation_without_prespecified_learning_algorithms",
      "pdf": "/pdf/8a4a49f404d172df902842781f95ef52ed70433e.pdf",
      "supplementary_material": "",
      "_bibtex": "@inproceedings{\njust2023lava,\ntitle={{LAVA}: Data Valuation without Pre-Specified Learning Algorithms},\nauthor={Hoang Anh Just and Feiyang Kang and Tianhao Wang and Yi Zeng and Myeongseob Ko and Ming Jin and Ruoxi Jia},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=JJuP86nBl4q}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/lava-data-valuation-without-pre-specified/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279131324,
    "odate": 1664468100000,
    "details": {
      "replyCount": 48,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "VA1YpcNr7ul",
    "original": "RQ8wV0gx_MA",
    "number": 886,
    "cdate": 1663849897269,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849897269,
    "tmdate": 1760256813029,
    "tddate": null,
    "forum": "VA1YpcNr7ul",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "DASHA: Distributed Nonconvex Optimization with Communication Compression and Optimal Oracle Complexity",
      "authorids": [
        "~Alexander_Tyurin1",
        "~Peter_Richtárik1"
      ],
      "authors": [
        "Alexander Tyurin",
        "Peter Richtárik"
      ],
      "keywords": [
        "Nonconvex Optimization",
        "Variance Reduction",
        "Compressed Communication",
        "Distributed Optimization"
      ],
      "TL;DR": "We provide a new method that improves the state-of-the-art theoretical complexity of distributed optimization methods with compressed communication in the nonconvex regime.",
      "abstract": "We develop and analyze  DASHA: a new family of methods for nonconvex distributed optimization problems. When the local functions at the nodes have a finite-sum or an expectation form, our new methods, DASHA-PAGE, DASHA-MVR and DASHA-SYNC-MVR, improve the theoretical oracle and communication complexity of the previous state-of-the-art method MARINA by Gorbunov et al. (2020). In particular, to achieve an $\\varepsilon$-stationary point, and considering the random sparsifier Rand$K$ as an example, our methods compute the optimal number of gradients $\\mathcal{O}\\left(\\frac{\\sqrt{m}}{\\varepsilon\\sqrt{n}}\\right)$ and $\\mathcal{O}\\left(\\frac{\\sigma}{\\varepsilon^{3/2}n}\\right)$ in finite-sum and expectation form cases, respectively, while maintaining the SOTA communication complexity $\\mathcal{O}\\left(\\frac{d}{\\varepsilon \\sqrt{n}}\\right)$. Furthermore, unlike MARINA, the new methods DASHA, DASHA-PAGE and DASHA-MVR send compressed vectors only, which makes them more practical for federated learning. We extend our results to the case when the functions satisfy the Polyak-Lojasiewicz condition. Finally, our theory is corroborated in practice: we see a significant improvement in experiments with nonconvex classification and training of deep learning models.",
      "pdf": "/pdf/b48aa1d65ec14a3d0d8248dd7e332ea750bdee69.pdf",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Optimization (eg, convex and non-convex optimization)",
      "paperhash": "tyurin|dasha_distributed_nonconvex_optimization_with_communication_compression_and_optimal_oracle_complexity",
      "_bibtex": "@inproceedings{\ntyurin2023dasha,\ntitle={{DASHA}: Distributed Nonconvex Optimization with Communication Compression and Optimal Oracle Complexity},\nauthor={Alexander Tyurin and Peter Richt{\\'a}rik},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=VA1YpcNr7ul}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/dasha-distributed-nonconvex-optimization-with/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279132679,
    "odate": 1664468100000,
    "details": {
      "replyCount": 11,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "zqwryBoXYnh",
    "original": "7Ig7hrzuvh6",
    "number": 898,
    "cdate": 1663849898560,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849898560,
    "tmdate": 1677620361027,
    "tddate": null,
    "forum": "zqwryBoXYnh",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "PLOT: Prompt Learning with Optimal Transport for Vision-Language Models",
      "authorids": [
        "~Guangyi_Chen1",
        "~Weiran_Yao1",
        "~Xiangchen_Song1",
        "~Xinyue_Li3",
        "~Yongming_Rao1",
        "~Kun_Zhang1"
      ],
      "authors": [
        "Guangyi Chen",
        "Weiran Yao",
        "Xiangchen Song",
        "Xinyue Li",
        "Yongming Rao",
        "Kun Zhang"
      ],
      "keywords": [],
      "abstract": "With the increasing attention to large vision-language models such as CLIP, there has been a significant amount of effort dedicated to building efficient prompts. Unlike conventional methods of only learning one single prompt, we propose to learn multiple comprehensive prompts to describe diverse characteristics of categories such as intrinsic attributes or extrinsic contexts. However, directly matching each prompt to the same visual feature is problematic, as it pushes the prompts to converge to one point. To solve this problem, we propose to apply optimal transport to match the vision and text modalities. Specifically, we first model images and the categories with visual and textual feature sets. Then, we apply a two-stage optimization strategy to learn the prompts. In the inner loop, we optimize the optimal transport distance to align visual features and prompts by the Sinkhorn algorithm, while in the outer loop, we learn the prompts by this distance from the supervised data. Extensive experiments are conducted on the few-shot recognition task and the improvement demonstrates the superiority of our method. The code is available at https://github.com/CHENGY12/PLOT.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "chen|plot_prompt_learning_with_optimal_transport_for_visionlanguage_models",
      "pdf": "/pdf/ddf150416bd1ce46f5512042c2aaa162c8ad10b7.pdf",
      "supplementary_material": "/attachment/b6b86d575bfaf4304159426709ae9861059451c7.zip",
      "_bibtex": "@inproceedings{\nchen2023plot,\ntitle={{PLOT}: Prompt Learning with Optimal Transport for Vision-Language Models},\nauthor={Guangyi Chen and Weiran Yao and Xiangchen Song and Xinyue Li and Yongming Rao and Kun Zhang},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=zqwryBoXYnh}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279133034,
    "odate": 1664468100000,
    "details": {
      "replyCount": 15,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "hVrXUps3LFA",
    "original": "uaq3ga1aEl",
    "number": 912,
    "cdate": 1663849899983,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849899983,
    "tmdate": 1760256812065,
    "tddate": null,
    "forum": "hVrXUps3LFA",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Divide to Adapt: Mitigating Confirmation Bias for Domain Adaptation of Black-Box Predictors",
      "authorids": [
        "~Jianfei_Yang4",
        "~Xiangyu_Peng2",
        "~Kai_Wang8",
        "~Zheng_Zhu1",
        "~Jiashi_Feng1",
        "~Lihua_Xie2",
        "~Yang_You1"
      ],
      "authors": [
        "Jianfei Yang",
        "Xiangyu Peng",
        "Kai Wang",
        "Zheng Zhu",
        "Jiashi Feng",
        "Lihua Xie",
        "Yang You"
      ],
      "keywords": [
        "model adaptation",
        "black-box predictors",
        "transfer learning"
      ],
      "TL;DR": "A black-box model adaptation approach that purifies the pseudo labels for knowledge distillation.",
      "abstract": "Domain Adaptation of Black-box Predictors (DABP) aims to learn a model on an unlabeled target domain supervised by a black-box predictor trained on a source domain. It does not require access to both the source-domain data and the predictor parameters, thus addressing the data privacy and portability issues of standard domain adaptation methods. Existing DABP approaches mostly rely on knowledge distillation (KD) from the black-box predictor, i.e., training the model with its noisy target-domain predictions, which however inevitably introduces the confirmation bias accumulated from the prediction noises and leads to degrading performance. To mitigate such bias, we propose a new strategy, \\textit{divide-to-adapt}, that purifies cross-domain knowledge distillation by proper domain division. This is inspired by an observation we make for the first time in domain adaptation: the target domain usually contains easy-to-adapt and hard-to-adapt samples that have different levels of domain discrepancy w.r.t. the source domain, and deep models tend to fit easy-to-adapt samples first. Leveraging easy-to-adapt samples with less noise can help KD alleviate the negative effect of prediction noises from black-box predictors. In this sense, the target domain can be divided into an easy-to-adapt subdomain with less noise and a hard-to-adapt subdomain at the early stage of training. Then the adaptation is achieved by semi-supervised learning. We further reduce distribution discrepancy between subdomains and develop weak-strong augmentation strategy to filter the predictor errors progressively. As such, our method is a simple yet effective solution to reduce error accumulation in cross-domain knowledge distillation for DABP. Moreover, we prove that the target error of DABP is bounded by the noise ratio of two subdomains, i.e., the confirmation bias, which provides the theoretical justifications for our method. Extensive experiments demonstrate our method achieves state of the art on all DABP benchmarks, outperforming the existing best approach by 7.0\\% on VisDA-17, and is even comparable with the standard domain adaptation methods that use the source-domain data.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "paperhash": "yang|divide_to_adapt_mitigating_confirmation_bias_for_domain_adaptation_of_blackbox_predictors",
      "pdf": "/pdf/f6acdba3a448c8c49b38089c9fcca2175f862634.pdf",
      "supplementary_material": "/attachment/5e2c58a42b7cd96818cde67ffea5a10b8cbac49a.zip",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "_bibtex": "@inproceedings{\nyang2023divide,\ntitle={Divide to Adapt: Mitigating Confirmation Bias for Domain Adaptation of Black-Box Predictors},\nauthor={Jianfei Yang and Xiangyu Peng and Kai Wang and Zheng Zhu and Jiashi Feng and Lihua Xie and Yang You},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=hVrXUps3LFA}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/divide-to-adapt-mitigating-confirmation-bias/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279133888,
    "odate": 1664468100000,
    "details": {
      "replyCount": 22,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "plKu2GByCNW",
    "original": "07on_mgdwQ5",
    "number": 924,
    "cdate": 1663849901422,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849901422,
    "tmdate": 1760256811443,
    "tddate": null,
    "forum": "plKu2GByCNW",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Vision Transformer Adapter for Dense Predictions",
      "authorids": [
        "~Zhe_Chen10",
        "~Yuchen_Duan1",
        "~Wenhai_Wang2",
        "~Junjun_He2",
        "~Tong_Lu1",
        "~Jifeng_Dai1",
        "~Yu_Qiao1"
      ],
      "authors": [
        "Zhe Chen",
        "Yuchen Duan",
        "Wenhai Wang",
        "Junjun He",
        "Tong Lu",
        "Jifeng Dai",
        "Yu Qiao"
      ],
      "keywords": [
        "Plain Vision Transformer",
        "Adapter",
        "Dense Prediction"
      ],
      "abstract": "This work investigates a simple yet powerful dense prediction task adapter for Vision Transformer (ViT). Unlike recently advanced variants that incorporate vision-specific inductive biases into their architectures, the plain ViT suffers inferior performance on dense predictions due to weak prior assumptions. To address this issue, we propose the ViT-Adapter, which allows plain ViT to achieve comparable performance to vision-specific transformers. Specifically, the backbone in our framework is a plain ViT that can learn powerful representations from large-scale multi-modal data. When transferring to downstream tasks, a pre-training-free adapter is used to introduce the image-related inductive biases into the model, making it suitable for these tasks. We verify ViT-Adapter on multiple dense prediction tasks, including object detection, instance segmentation, and semantic segmentation. Notably, without using extra detection data, our ViT-Adapter-L yields state-of-the-art 60.9 box AP and 53.0 mask AP on COCO test-dev. We hope that the ViT-Adapter could serve as an alternative for vision-specific transformers and facilitate future research. Code and models will be released at https://github.com/czczup/ViT-Adapter.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Applications (eg, speech processing, computer vision, NLP)",
      "paperhash": "chen|vision_transformer_adapter_for_dense_predictions",
      "TL;DR": "This work investigates a simple yet powerful dense prediction task adapter for Vision Transformer (ViT).",
      "pdf": "/pdf/a1a7cac48a3e0fa0d2a12b5a46c5b2463fe22a38.pdf",
      "_bibtex": "@inproceedings{\nchen2023vision,\ntitle={Vision Transformer Adapter for Dense Predictions},\nauthor={Zhe Chen and Yuchen Duan and Wenhai Wang and Junjun He and Tong Lu and Jifeng Dai and Yu Qiao},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=plKu2GByCNW}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/vision-transformer-adapter-for-dense/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279134513,
    "odate": 1664468100000,
    "details": {
      "replyCount": 15,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "RWtGreRpovS",
    "original": "WEzcz-Xx5FN",
    "number": 926,
    "cdate": 1663849901667,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849901667,
    "tmdate": 1760256811249,
    "tddate": null,
    "forum": "RWtGreRpovS",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Simplicial Embeddings in Self-Supervised Learning and Downstream Classification",
      "authorids": [
        "~Samuel_Lavoie1",
        "~Christos_Tsirigotis1",
        "~Max_Schwarzer1",
        "~Ankit_Vani1",
        "~Michael_Noukhovitch1",
        "~Kenji_Kawaguchi1",
        "~Aaron_Courville3"
      ],
      "authors": [
        "Samuel Lavoie",
        "Christos Tsirigotis",
        "Max Schwarzer",
        "Ankit Vani",
        "Michael Noukhovitch",
        "Kenji Kawaguchi",
        "Aaron Courville"
      ],
      "keywords": [
        "Self-Supervised learning",
        "Representation learning",
        "Pre-training"
      ],
      "TL;DR": "We use softmax to embed representations in a collection of simplices in SSL models, which offers improved generalization properties for downstream classification.",
      "abstract": "Simplicial Embeddings (SEM) are representations learned through self-supervised learning (SSL), wherein a representation is projected into $L$ simplices of $V$ dimensions each using a \\texttt{softmax} operation. This procedure conditions the representation onto a constrained space during pretraining and imparts an inductive bias for group sparsity. For downstream classification, we formally prove that the SEM representation leads to better generalization than an unnormalized representation.\nFurthermore, we empirically demonstrate that SSL methods trained with SEMs have improved generalization on natural image datasets such as CIFAR-100 and ImageNet. Finally, when used in a downstream classification task, we show that SEM features exhibit emergent semantic coherence where small groups of learned features are distinctly predictive of semantically-relevant classes.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Unsupervised and Self-supervised learning",
      "paperhash": "lavoie|simplicial_embeddings_in_selfsupervised_learning_and_downstream_classification",
      "pdf": "/pdf/fadb3c6b6bc3ec01fc5bf271cf08a4eb1e0f6fc1.pdf",
      "supplementary_material": "/attachment/4fd5f9371de7715e85d215337cea506dcb997227.zip",
      "_bibtex": "@inproceedings{\nlavoie2023simplicial,\ntitle={Simplicial Embeddings in Self-Supervised Learning and Downstream Classification},\nauthor={Samuel Lavoie and Christos Tsirigotis and Max Schwarzer and Ankit Vani and Michael Noukhovitch and Kenji Kawaguchi and Aaron Courville},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=RWtGreRpovS}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/simplicial-embeddings-in-self-supervised/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279134686,
    "odate": 1664468100000,
    "details": {
      "replyCount": 18,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "3RhuF8foyPW",
    "original": "Q-_MJS_Cvxv",
    "number": 935,
    "cdate": 1663849902765,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849902765,
    "tmdate": 1677524834451,
    "tddate": null,
    "forum": "3RhuF8foyPW",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Single-shot General Hyper-parameter Optimization for Federated Learning",
      "authorids": [
        "~Yi_Zhou13",
        "~Parikshit_Ram1",
        "~Theodoros_Salonidis1",
        "~Nathalie_Baracaldo1",
        "~Horst_Samulowitz1",
        "~Heiko_Ludwig1"
      ],
      "authors": [
        "Yi Zhou",
        "Parikshit Ram",
        "Theodoros Salonidis",
        "Nathalie Baracaldo",
        "Horst Samulowitz",
        "Heiko Ludwig"
      ],
      "keywords": [
        "Federated Learning",
        "Hyperparameter Optimization",
        "Optimality Gap Analysis"
      ],
      "TL;DR": "We propose a single-shot hyperparameter optimization scheme for Federated Learning systems with theoretical performance guarantees and strong empirical performance against baselines.",
      "abstract": "We address the problem of hyper-parameter optimization (HPO) for federated learning (FL-HPO). We introduce Federated Loss SuRface Aggregation (FLoRA), a general FL-HPO solution framework that can address use cases of tabular data and any Machine Learning (ML) model including gradient boosting training algorithms, SVMs, neural networks, among others and thereby further expands the scope of FL-HPO. FLoRA enables single-shot FL-HPO: identifying a single set of good hyper-parameters that are subsequently used in a single FL training. Thus, it enables FL-HPO solutions with minimal additional communication overhead compared to FL training without HPO. Utilizing standard smoothness assumptions, we theoretically characterize the optimality gap of FLoRA for any convex and non-convex loss functions, which explicitly accounts for the heterogeneous nature of the parties' local data distributions, a dominant characteristic of FL systems. Our empirical evaluation of FLoRA for multiple FL algorithms on seven OpenML datasets demonstrates significant model accuracy improvements over the baselines, and robustness to increasing number of parties involved in FL-HPO training.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Optimization (eg, convex and non-convex optimization)",
      "paperhash": "zhou|singleshot_general_hyperparameter_optimization_for_federated_learning",
      "pdf": "/pdf/05e0c917ee8caab80ea9a21a831a3c9589442e99.pdf",
      "supplementary_material": "/attachment/e75b7dd85264fb7266bd4c321a2ba2763a681f59.zip",
      "_bibtex": "@inproceedings{\nzhou2023singleshot,\ntitle={Single-shot General Hyper-parameter Optimization for Federated Learning},\nauthor={Yi Zhou and Parikshit Ram and Theodoros Salonidis and Nathalie Baracaldo and Horst Samulowitz and Heiko Ludwig},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=3RhuF8foyPW}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279135449,
    "odate": 1664468100000,
    "details": {
      "replyCount": 13,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "Z3IClM_bzvP",
    "original": "tAEllYARIG",
    "number": 940,
    "cdate": 1663849903374,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849903374,
    "tmdate": 1760256810771,
    "tddate": null,
    "forum": "Z3IClM_bzvP",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Multi-skill Mobile Manipulation for Object Rearrangement",
      "authorids": [
        "~Jiayuan_Gu1",
        "~Devendra_Singh_Chaplot2",
        "~Hao_Su1",
        "~Jitendra_Malik2"
      ],
      "authors": [
        "Jiayuan Gu",
        "Devendra Singh Chaplot",
        "Hao Su",
        "Jitendra Malik"
      ],
      "keywords": [
        "mobile manipulation",
        "reinforcement learning"
      ],
      "abstract": "We study a modular approach to tackle long-horizon mobile manipulation tasks for object rearrangement, which decomposes a full task into a sequence of subtasks. To tackle the entire task, prior work chains multiple stationary manipulation skills with a point-goal navigation skill, which are learned individually on subtasks. Although more effective than monolithic end-to-end RL policies, this framework suffers from compounding errors in skill chaining, e.g., navigating to a bad location where a stationary manipulation skill can not reach its target to manipulate. To this end, we propose that the manipulation skills should include mobility to have flexibility in interacting with the target object from multiple locations and at the same time the navigation skill could have multiple end points which lead to successful manipulation. We operationalize these ideas by implementing mobile manipulation skills rather than stationary ones and training a navigation skill trained with region goal instead of point goal. We evaluate our multi-skill mobile manipulation method M3 on 3 challenging long-horizon mobile manipulation tasks in the Home Assistant Benchmark (HAB), and show superior performance as compared to the baselines.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Reinforcement Learning (eg, decision and control, planning, hierarchical RL, robotics)",
      "paperhash": "gu|multiskill_mobile_manipulation_for_object_rearrangement",
      "pdf": "/pdf/826efb580419b89e9ce1db3a7c676c7010ebe04b.pdf",
      "_bibtex": "@inproceedings{\ngu2023multiskill,\ntitle={Multi-skill Mobile Manipulation for Object Rearrangement},\nauthor={Jiayuan Gu and Devendra Singh Chaplot and Hao Su and Jitendra Malik},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=Z3IClM_bzvP}\n}",
      "supplementary_material": "/attachment/e84ece0b5844f41a84a6060f27810f198b678e5d.zip",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/multi-skill-mobile-manipulation-for-object/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279135314,
    "odate": 1664468100000,
    "details": {
      "replyCount": 11,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "k60XE_b0Ix6",
    "original": "CTOqXJH-nN",
    "number": 952,
    "cdate": 1663849904831,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849904831,
    "tmdate": 1760256809928,
    "tddate": null,
    "forum": "k60XE_b0Ix6",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Learning Label Encodings for Deep Regression",
      "authorids": [
        "~Deval_Shah1",
        "~Tor_M._Aamodt1"
      ],
      "authors": [
        "Deval Shah",
        "Tor M. Aamodt"
      ],
      "keywords": [
        "Regression",
        "Label Encoding"
      ],
      "TL;DR": "We propose an end-to-end automated approach to learn label encodings for deep regression.",
      "abstract": "Deep regression networks are widely used to tackle the problem of predicting a continuous value for a given input. Task-specialized approaches for training regression networks have shown significant improvement over generic approaches, such as direct regression. More recently, a generic approach based on regression by binary classification using binary-encoded labels has shown significant improvement over direct regression. The space of label encodings for regression is large. Lacking heretofore have been automated approaches to find a good label encoding for a given application. This paper introduces Regularized Label Encoding Learning (RLEL) for end-to-end training of an entire network and its label encoding. RLEL provides a generic approach for tackling regression. Underlying RLEL is our observation that the search space of label encodings can be constrained and efficiently explored by using a continuous search space of real-valued label encodings combined with a regularization function designed to encourage encodings with certain properties. These properties balance the probability of classification error in individual bits against error correction capability. Label encodings found by RLEL result in lower or comparable errors to manually designed label encodings. Applying RLEL results in $10.9\\%$ and $12.4\\%$ improvement in Mean Absolute Error (MAE) over direct regression and multiclass classification, respectively. Our evaluation demonstrates that RLEL can be combined with off-the-shelf feature extractors and is suitable across different architectures, datasets, and tasks. Code is available at \\url{https://github.com/ubc-aamodt-group/RLEL_regression}. ",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "shah|learning_label_encodings_for_deep_regression",
      "pdf": "/pdf/3af8d03ffcf4536fc86a2416e751d3d4282af4d0.pdf",
      "supplementary_material": "/attachment/f4a8a40bcac831cf270f18d3ca41399c79ad652c.zip",
      "_bibtex": "@inproceedings{\nshah2023learning,\ntitle={Learning Label Encodings for Deep Regression},\nauthor={Deval Shah and Tor M. Aamodt},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=k60XE_b0Ix6}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/learning-label-encodings-for-deep-regression/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279136291,
    "odate": 1664468100000,
    "details": {
      "replyCount": 15,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "LV_MeMS38Q9",
    "original": "fu5URJcHPV4",
    "number": 963,
    "cdate": 1663849906080,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849906080,
    "tmdate": 1760256808971,
    "tddate": null,
    "forum": "LV_MeMS38Q9",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Betty: An Automatic Differentiation Library for Multilevel Optimization",
      "authorids": [
        "~Sang_Keun_Choe1",
        "~Willie_Neiswanger2",
        "~Pengtao_Xie3",
        "~Eric_Xing1"
      ],
      "authors": [
        "Sang Keun Choe",
        "Willie Neiswanger",
        "Pengtao Xie",
        "Eric Xing"
      ],
      "keywords": [
        "Multilevel Optimization",
        "Automatic Differentiation",
        "Bilevel Optimization",
        "Meta Learning",
        "Software Library"
      ],
      "TL;DR": "We develop a scalable, user-friendly, and modular automatic differentiation library for multilevel optimization based on a novel interpretation of multilevel optimization as a dataflow graph.",
      "abstract": "Gradient-based multilevel optimization (MLO) has gained attention as a framework for studying numerous problems, ranging from hyperparameter optimization and meta-learning to neural architecture search and reinforcement learning. However, gradients in MLO, which are obtained by composing best-response Jacobians via the chain rule, are notoriously difficult to implement and memory/compute intensive. We take an initial step towards closing this gap by introducing Betty, a software library for large-scale MLO. At its core, we devise a novel dataflow graph for MLO, which allows us to (1) develop efficient automatic differentiation for MLO that reduces the computational complexity from $\\mathcal{O}(d^3)$ to $\\mathcal{O}(d^2)$, (2) incorporate systems support such as mixed-precision and data-parallel training for scalability, and (3) facilitate implementation of MLO programs of arbitrary complexity while allowing a modular interface for diverse algorithmic and systems design choices. We empirically demonstrate that Betty can be used to implement an array of MLO programs, while also observing up to 11% increase in test accuracy, 14% decrease in GPU memory usage, and 20% decrease in training wall time over existing implementations on multiple benchmarks. We also showcase that Betty enables scaling MLO to models with hundreds of millions of parameters. We open-source the code at https://github.com/leopard-ai/betty.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Infrastructure (eg, datasets, competitions, implementations, libraries)",
      "paperhash": "choe|betty_an_automatic_differentiation_library_for_multilevel_optimization",
      "pdf": "/pdf/e92379cd67840d63d8a85743600bfe396bcdf7fb.pdf",
      "supplementary_material": "/attachment/50604f54232b54885bc61c52e0d6de2f1634a8d4.zip",
      "_bibtex": "@inproceedings{\nchoe2023betty,\ntitle={Betty: An Automatic Differentiation Library for Multilevel Optimization},\nauthor={Sang Keun Choe and Willie Neiswanger and Pengtao Xie and Eric Xing},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=LV_MeMS38Q9}\n}",
      "venue": "ICLR 2023 notable top 5%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/betty-an-automatic-differentiation-library/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279137102,
    "odate": 1664468100000,
    "details": {
      "replyCount": 12,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "CrfhZAsJDsZ",
    "original": "N5Tc3cC5Kc",
    "number": 969,
    "cdate": 1663849906812,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849906812,
    "tmdate": 1677616196442,
    "tddate": null,
    "forum": "CrfhZAsJDsZ",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Nonlinear Reconstruction for Operator Learning of PDEs with Discontinuities",
      "authorids": [
        "~Samuel_Lanthaler1",
        "~Roberto_Molinaro1",
        "~Patrik_Hadorn1",
        "~Siddhartha_Mishra1"
      ],
      "authors": [
        "Samuel Lanthaler",
        "Roberto Molinaro",
        "Patrik Hadorn",
        "Siddhartha Mishra"
      ],
      "keywords": [],
      "TL;DR": "Operator learning based on non-linear reconstruction (FNOs, shift-DeepONets) outperform methods based on linear reconstruction (DeepONets, PCA-Net) for PDEs with discontinuities.",
      "abstract": "Discontinuous solutions arise in a large class of hyperbolic and advection-dominated PDEs. This paper investigates, both theoretically and empirically, the operator learning of PDEs with discontinuous solutions. We rigorously prove, in terms of lower approximation bounds, that methods which entail a linear reconstruction step (e.g. DeepONets or PCA-Nets) fail to efficiently approximate the solution operator of such PDEs. In contrast, we show that certain methods employing a non-linear reconstruction mechanism can overcome these fundamental lower bounds and approximate the underlying operator efficiently. The latter class includes Fourier Neural Operators and a novel extension of DeepONets termed shift-DeepONets. Our theoretical findings are confirmed by empirical results for advection equations, inviscid Burgers’ equation and the compressible Euler equations of gas dynamics.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Theory (eg, control theory, learning theory, algorithmic game theory)",
      "paperhash": "lanthaler|nonlinear_reconstruction_for_operator_learning_of_pdes_with_discontinuities",
      "pdf": "/pdf/995dfba9492244e0ce9642782af6ec9a55816279.pdf",
      "supplementary_material": "/attachment/80383641b8f1818bc2f501b8ac3c27c6add6c1da.zip",
      "_bibtex": "@inproceedings{\nlanthaler2023nonlinear,\ntitle={Nonlinear Reconstruction for Operator Learning of {PDE}s with Discontinuities},\nauthor={Samuel Lanthaler and Roberto Molinaro and Patrik Hadorn and Siddhartha Mishra},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=CrfhZAsJDsZ}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279137435,
    "odate": 1664468100000,
    "details": {
      "replyCount": 20,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "vSVLM2j9eie",
    "original": "cqThAxZQ57f",
    "number": 992,
    "cdate": 1663849909422,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849909422,
    "tmdate": 1677728596066,
    "tddate": null,
    "forum": "vSVLM2j9eie",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting",
      "authorids": [
        "~Yunhao_Zhang1",
        "~Junchi_Yan2"
      ],
      "authors": [
        "Yunhao Zhang",
        "Junchi Yan"
      ],
      "keywords": [
        "Transformer",
        "multivariate time series forecasting",
        "deep learning"
      ],
      "abstract": "Recently many deep models have been proposed for multivariate time series (MTS) forecasting. In particular, Transformer-based models have shown great potential because they can capture long-term dependency. However, existing Transformer-based models mainly focus on modeling the temporal dependency (cross-time dependency) yet often omit the dependency among different variables (cross-dimension dependency), which is critical for MTS forecasting. To fill the gap, we propose Crossformer, a Transformer-based model utilizing cross-dimension dependency for MTS forecasting. In Crossformer, the input MTS is embedded into a 2D vector array through the Dimension-Segment-Wise (DSW) embedding to preserve time and dimension information. Then the Two-Stage Attention (TSA) layer is proposed to efficiently capture the cross-time and cross-dimension dependency. Utilizing DSW embedding and TSA layer, Crossformer establishes a Hierarchical Encoder-Decoder (HED) to use the information at different scales for the final forecasting. Extensive experimental results on six real-world datasets show the effectiveness of Crossformer against previous state-of-the-arts.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Applications (eg, speech processing, computer vision, NLP)",
      "paperhash": "zhang|crossformer_transformer_utilizing_crossdimension_dependency_for_multivariate_time_series_forecasting",
      "TL;DR": "We propose Crossformer, a Transformer-based model that explicitly utilizes cross-dimension dependency for multivariate time series forecasting.",
      "pdf": "/pdf/1d793d6ba7c00ecfe98128614d58e2493255bd89.pdf",
      "_bibtex": "@inproceedings{\nzhang2023crossformer,\ntitle={Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting},\nauthor={Yunhao Zhang and Junchi Yan},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=vSVLM2j9eie}\n}",
      "venue": "ICLR 2023 notable top 5%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279138361,
    "odate": 1664468100000,
    "details": {
      "replyCount": 21,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "mRieQgMtNTQ",
    "original": "IadhHxexONB",
    "number": 1026,
    "cdate": 1663849913437,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849913437,
    "tmdate": 1760256807166,
    "tddate": null,
    "forum": "mRieQgMtNTQ",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model",
      "authorids": [
        "~Yinhuai_Wang1",
        "~Jiwen_Yu1",
        "~Jian_Zhang22"
      ],
      "authors": [
        "Yinhuai Wang",
        "Jiwen Yu",
        "Jian Zhang"
      ],
      "keywords": [
        "Zero-Shot",
        "Inverse Problems",
        "Super-Resolution",
        "Diffusion Models",
        "Range-Null Space",
        "Image Restoration",
        "Colorization",
        "Compressed Sensing",
        "Inpainting",
        "Deblur",
        "Old Photo Restoration",
        "Blind Restoration"
      ],
      "abstract": "Most existing Image Restoration (IR) models are task-specific, which can not be generalized to different degradation operators. In this work, we propose the Denoising Diffusion Null-Space Model (DDNM), a novel zero-shot framework for arbitrary linear IR problems, including but not limited to image super-resolution, colorization, inpainting, compressed sensing, and deblurring. DDNM only needs a pre-trained off-the-shelf diffusion model as the generative prior, without any extra training or network modifications. By refining only the null-space contents during the reverse diffusion process, we can yield diverse results satisfying both data consistency and realness. We further propose an enhanced and robust version, dubbed DDNM+, to support noisy restoration and improve restoration quality for hard tasks. Our experiments on several IR tasks reveal that DDNM outperforms other state-of-the-art zero-shot IR methods. We also demonstrate that DDNM+ can solve complex real-world applications, e.g., old photo restoration. ",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Applications (eg, speech processing, computer vision, NLP)",
      "paperhash": "wang|zeroshot_image_restoration_using_denoising_diffusion_nullspace_model",
      "TL;DR": "We present a novel zero-shot image restoration framework, achieving state-of-the-art performance.",
      "pdf": "/pdf/e31de23cacc50c8cddef5c6e559520cdd3a62b0c.pdf",
      "_bibtex": "@inproceedings{\nwang2023zeroshot,\ntitle={Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model},\nauthor={Yinhuai Wang and Jiwen Yu and Jian Zhang},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=mRieQgMtNTQ}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/zero-shot-image-restoration-using-denoising/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279139998,
    "odate": 1664468100000,
    "details": {
      "replyCount": 18,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "pd1P2eUBVfq",
    "original": "9JiBBeby9s",
    "number": 1031,
    "cdate": 1663849914030,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849914030,
    "tmdate": 1760256806719,
    "tddate": null,
    "forum": "pd1P2eUBVfq",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Diffusion Models Already Have A Semantic Latent Space",
      "authorids": [
        "~Mingi_Kwon1",
        "~Jaeseok_Jeong2",
        "~Youngjung_Uh2"
      ],
      "authors": [
        "Mingi Kwon",
        "Jaeseok Jeong",
        "Youngjung Uh"
      ],
      "keywords": [
        "diffusion models",
        "semantic image editing"
      ],
      "abstract": "Diffusion models achieve outstanding generative performance in various domains. Despite their great success, they lack semantic latent space which is essential for controlling the generative process. To address the problem, we propose asymmetric reverse process (Asyrp) which discovers the semantic latent space in frozen pretrained diffusion models. Our semantic latent space, named h-space, has nice properties for accommodating semantic image manipulation: homogeneity, linearity, robustness, and consistency across timesteps. In addition, we measure editing strength and quality deficiency of a generative process at timesteps to provide a principled design of the process for versatility and quality improvements. Our method is applicable to various architectures (DDPM++, iDDPM, and ADM) and datasets (CelebA-HQ, AFHQ-dog, LSUN-church, LSUN-bedroom, and METFACES).",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Generative models",
      "paperhash": "kwon|diffusion_models_already_have_a_semantic_latent_space",
      "TL;DR": "We discover the semantic latent space of pretrained diffusion models by introducing asymmetric reverse process.",
      "pdf": "/pdf/0d48a82a332a9c1fbc68f65e41a9b16eb9efa537.pdf",
      "supplementary_material": "/attachment/62942df08ed6f6aaf1899bf56970b1e3ddefc9ae.zip",
      "_bibtex": "@inproceedings{\nkwon2023diffusion,\ntitle={Diffusion Models Already Have A Semantic Latent Space},\nauthor={Mingi Kwon and Jaeseok Jeong and Youngjung Uh},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=pd1P2eUBVfq}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/arxiv:2210.10960/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279140425,
    "odate": 1664468100000,
    "details": {
      "replyCount": 22,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "4oXTQ6m_ws8",
    "original": "6ZOQRwGi4CL",
    "number": 1033,
    "cdate": 1663849914270,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849914270,
    "tmdate": 1760256806674,
    "tddate": null,
    "forum": "4oXTQ6m_ws8",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "The Role of ImageNet Classes in Fréchet Inception Distance",
      "authorids": [
        "~Tuomas_Kynkäänniemi1",
        "~Tero_Karras1",
        "~Miika_Aittala2",
        "~Timo_Aila1",
        "~Jaakko_Lehtinen1"
      ],
      "authors": [
        "Tuomas Kynkäänniemi",
        "Tero Karras",
        "Miika Aittala",
        "Timo Aila",
        "Jaakko Lehtinen"
      ],
      "keywords": [
        "generative models",
        "evaluation",
        "Fréchet Inception Distance"
      ],
      "TL;DR": "We elucidate why using ImageNet pre-trained Inception features in FID can cause discrepancies with human judgement.",
      "abstract": "Fréchet Inception Distance (FID) is the primary metric for ranking models in data-driven generative modeling. While remarkably successful, the metric is known to sometimes disagree with human judgement. We investigate a root cause of these discrepancies, and visualize what FID \"looks at\" in generated images. We show that the feature space that FID is (typically) computed in is so close to the ImageNet classifications that aligning the histograms of Top-$N$ classifications between sets of generated and real images can reduce FID substantially — without actually improving the quality of results. Thus, we conclude that FID is prone to intentional or accidental distortions. As a practical example of an accidental distortion, we discuss a case where an ImageNet pre-trained FastGAN achieves a FID comparable to StyleGAN2, while being worse in terms of human evaluation.",
      "pdf": "/pdf/0e0f4c80c56d0d57f3f758fa07e6f2226ddefea8.pdf",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Generative models",
      "paperhash": "kynkäänniemi|the_role_of_imagenet_classes_in_fréchet_inception_distance",
      "_bibtex": "@inproceedings{\nkynk{\\\"a}{\\\"a}nniemi2023the,\ntitle={The Role of ImageNet Classes in Fr\\'echet Inception Distance},\nauthor={Tuomas Kynk{\\\"a}{\\\"a}nniemi and Tero Karras and Miika Aittala and Timo Aila and Jaakko Lehtinen},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=4oXTQ6m_ws8}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/the-role-of-imagenet-classes-in-frechet/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279140580,
    "odate": 1664468100000,
    "details": {
      "replyCount": 10,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "C2fsSj3ZGiU",
    "original": "mafnXpVl39",
    "number": 1059,
    "cdate": 1663849917003,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849917003,
    "tmdate": 1676880373708,
    "tddate": null,
    "forum": "C2fsSj3ZGiU",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Neural Episodic Control with State Abstraction",
      "authorids": [
        "~Zhuo_Li6",
        "~Derui_Zhu2",
        "~Yujing_Hu2",
        "~Xiaofei_Xie2",
        "~Lei_Ma1",
        "~YAN_ZHENG1",
        "~Yan_Song4",
        "~Yingfeng_Chen2",
        "~Jianjun_Zhao1"
      ],
      "authors": [
        "Zhuo Li",
        "Derui Zhu",
        "Yujing Hu",
        "Xiaofei Xie",
        "Lei Ma",
        "YAN ZHENG",
        "Yan Song",
        "Yingfeng Chen",
        "Jianjun Zhao"
      ],
      "keywords": [
        "Deep reinforcement learning",
        "episodic control",
        "sample efficiency",
        "state abstraction"
      ],
      "TL;DR": "We propose NECSA, a simple and effective state abstraction-based episodic control containing a more comprehensive episodic memory, a novel state measurement, and a multi-step state analysis.",
      "abstract": "Existing Deep Reinforcement Learning (DRL) algorithms suffer from sample inefficiency. Generally, episodic control-based approaches are solutions that leverage highly rewarded past experiences to improve sample efficiency of DRL algorithms. However, previous episodic control-based approaches fail to utilize the latent information from the historical behaviors (\\eg, state transitions, topological similarities, \\etc) and lack scalability during DRL training. This work introduces Neural Episodic Control with State Abstraction (NECSA), a simple but effective state abstraction-based episodic control containing a more comprehensive episodic memory, a novel state evaluation, and a multi-step state analysis. We evaluate our approach to the MuJoCo and Atari tasks in OpenAI gym domains. The experimental results indicate that NECSA achieves higher sample efficiency than the state-of-the-art episodic control-based approaches. Our data and code are available at the project website\\footnote{\\url{https://sites.google.com/view/drl-necsa}}. ",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Reinforcement Learning (eg, decision and control, planning, hierarchical RL, robotics)",
      "paperhash": "li|neural_episodic_control_with_state_abstraction",
      "pdf": "/pdf/48c93f23de2f99bd3c38419a3f4bf1aba384c134.pdf",
      "supplementary_material": "/attachment/8c7ba7297579f446b825beb84765b7950c5a6a4b.zip",
      "_bibtex": "@inproceedings{\nli2023neural,\ntitle={Neural Episodic Control with State Abstraction},\nauthor={Zhuo Li and Derui Zhu and Yujing Hu and Xiaofei Xie and Lei Ma and YAN ZHENG and Yan Song and Yingfeng Chen and Jianjun Zhao},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=C2fsSj3ZGiU}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279142201,
    "odate": 1664468100000,
    "details": {
      "replyCount": 25,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "kIAx30hYi_p",
    "original": "BTXLRk_rL5M",
    "number": 1061,
    "cdate": 1663849917239,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849917239,
    "tmdate": 1677424674345,
    "tddate": null,
    "forum": "kIAx30hYi_p",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Self-Supervised Set Representation Learning for Unsupervised Meta-Learning",
      "authorids": [
        "~Dong_Bok_Lee1",
        "~Seanie_Lee1",
        "~Kenji_Kawaguchi1",
        "~Yunji_Kim1",
        "~Jihwan_Bang1",
        "~Jung-Woo_Ha1",
        "~Sung_Ju_Hwang1"
      ],
      "authors": [
        "Dong Bok Lee",
        "Seanie Lee",
        "Kenji Kawaguchi",
        "Yunji Kim",
        "Jihwan Bang",
        "Jung-Woo Ha",
        "Sung Ju Hwang"
      ],
      "keywords": [
        "Unsupervised Meta-Learning",
        "Set Representation Learning",
        "Self-Supervised Learning"
      ],
      "TL;DR": "We propose self-supervised set representation learning for unsupervised meta-learning.",
      "abstract": "Unsupervised meta-learning (UML) essentially shares the spirit of self-supervised learning (SSL) in that their goal aims at learning models without any human supervision so that the models can be adapted to downstream tasks. Further, the learning objective of self-supervised learning, which pulls positive pairs closer and repels negative pairs, also resembles metric-based meta-learning. Metric-based meta-learning is one of the most successful meta-learning methods, which learns to minimize the distance between representations from the same class. \nOne notable aspect of metric-based meta-learning, however, is that it is widely interpreted as a set-level problem since the inference of discriminative class prototypes (or set representations) from few examples is crucial for the performance of downstream tasks. Motivated by this, we propose Set-SimCLR, a novel self-supervised set representation learning framework for targeting UML problem. Specifically, our Set-SimCLR learns a set encoder on top of instance representations to maximize the agreement between two sets of augmented samples, which are generated by applying stochastic augmentations to a given image. We theoretically analyze how our proposed set representation learning can potentially improve the generalization performance at the meta-test. We also empirically validate its effectiveness on various benchmark datasets, showing that Set-SimCLR largely outperforms both UML and instance-level self-supervised learning baselines.",
      "pdf": "/pdf/9ce55edaecb2d3fc88d5f57116eae4648e79ecf4.pdf",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Unsupervised and Self-supervised learning",
      "paperhash": "lee|selfsupervised_set_representation_learning_for_unsupervised_metalearning",
      "supplementary_material": "/attachment/f2625a5d390bce721fa0f9c48b20aad1934dc39a.zip",
      "_bibtex": "@inproceedings{\nlee2023selfsupervised,\ntitle={Self-Supervised Set Representation Learning for Unsupervised Meta-Learning},\nauthor={Dong Bok Lee and Seanie Lee and Kenji Kawaguchi and Yunji Kim and Jihwan Bang and Jung-Woo Ha and Sung Ju Hwang},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=kIAx30hYi_p}\n}",
      "venue": "ICLR 2023 poster",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279142234,
    "odate": 1664468100000,
    "details": {
      "replyCount": 7,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "IloMJ5rqfnt",
    "original": "VIUjKncihJ1",
    "number": 1067,
    "cdate": 1663849918023,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849918023,
    "tmdate": 1760256804939,
    "tddate": null,
    "forum": "IloMJ5rqfnt",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Accurate Image Restoration with Attention Retractable Transformer",
      "authorids": [
        "~Jiale_Zhang3",
        "~Yulun_Zhang1",
        "~Jinjin_Gu1",
        "~Yongbing_Zhang1",
        "~Linghe_Kong1",
        "~Xin_Yuan4"
      ],
      "authors": [
        "Jiale Zhang",
        "Yulun Zhang",
        "Jinjin Gu",
        "Yongbing Zhang",
        "Linghe Kong",
        "Xin Yuan"
      ],
      "keywords": [
        "Image restoration",
        "Dense and sparse attention"
      ],
      "TL;DR": "A new SOTA image restoration method attention retractable Transformer.",
      "abstract": "Recently, Transformer-based image restoration networks have achieved promising improvements over convolutional neural networks due to parameter-independent global interactions. To lower computational cost, existing works generally limit self-attention computation within non-overlapping windows. However, each group of tokens are always from a dense area of the image. This is considered as a dense attention strategy since the interactions of tokens are restrained in dense regions. Obviously, this strategy could result in restricted receptive fields. To address this issue, we propose \\textbf{A}ttention \\textbf{R}etractable \\textbf{T}ransformer (ART) for image restoration, which presents both dense and sparse attention modules in the network. The sparse attention module allows tokens from sparse areas to interact and thus provides a wider receptive field. Furthermore, the alternating application of dense and sparse attention modules greatly enhances representation ability of Transformer while providing retractable attention on the input image.We conduct extensive experiments on image super-resolution, denoising, and JPEG compression artifact reduction tasks. Experimental results validate that our proposed ART outperforms state-of-the-art methods on various benchmark datasets both quantitatively and visually. We also provide code and models at~\\url{https://github.com/gladzhang/ART}.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Applications (eg, speech processing, computer vision, NLP)",
      "paperhash": "zhang|accurate_image_restoration_with_attention_retractable_transformer",
      "pdf": "/pdf/aa567c400b76e1249b3186bd548cfc118ad0f339.pdf",
      "supplementary_material": "/attachment/77fe90ca1af98ee50e60942c3068310e2c0a3ee8.zip",
      "_bibtex": "@inproceedings{\nzhang2023accurate,\ntitle={Accurate Image Restoration with Attention Retractable Transformer},\nauthor={Jiale Zhang and Yulun Zhang and Jinjin Gu and Yongbing Zhang and Linghe Kong and Xin Yuan},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=IloMJ5rqfnt}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/accurate-image-restoration-with-attention/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279142511,
    "odate": 1664468100000,
    "details": {
      "replyCount": 12,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "4WM4cy42B81",
    "original": "evxixEar4h7",
    "number": 1068,
    "cdate": 1663849918179,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849918179,
    "tmdate": 1760256804872,
    "tddate": null,
    "forum": "4WM4cy42B81",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Dirichlet-based Uncertainty Calibration for Active Domain Adaptation",
      "authorids": [
        "~Mixue_Xie2",
        "~Shuang_Li6",
        "~Rui_Zhang20",
        "~Chi_Harold_Liu1"
      ],
      "authors": [
        "Mixue Xie",
        "Shuang Li",
        "Rui Zhang",
        "Chi Harold Liu"
      ],
      "keywords": [
        "domain adaptation",
        "active learning",
        "uncertainty",
        "Dirichlet"
      ],
      "abstract": "Active domain adaptation (DA) aims to maximally boost the model adaptation on a new target domain by actively selecting limited target data to annotate, whereas traditional active learning methods may be less effective since they do not consider the domain shift issue. Despite active DA methods address this by further proposing targetness to measure the representativeness of target domain characteristics, their predictive uncertainty is usually based on the prediction of deterministic models, which can easily be miscalibrated on data with distribution shift. Considering this, we propose a Dirichlet-based Uncertainty Calibration (DUC) approach for active DA, which simultaneously achieves the mitigation of miscalibration and the selection of informative target samples. Specifically, we place a Dirichlet prior on the prediction and interpret the prediction as a distribution on the probability simplex, rather than a point estimate like deterministic models. This manner enables us to consider all possible predictions, mitigating the miscalibration of unilateral prediction. Then a two-round selection strategy based on different uncertainty origins is designed to select target samples that are both representative of target domain and conducive to discriminability. Extensive experiments on cross-domain image classification and semantic segmentation validate the superiority of DUC.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "General Machine Learning (ie none of the above)",
      "paperhash": "xie|dirichletbased_uncertainty_calibration_for_active_domain_adaptation",
      "pdf": "/pdf/d263b9c4283973a09247e2e1effd05f8d9bd7652.pdf",
      "supplementary_material": "/attachment/3ad1fa04da718b44a4e6b2f80fdd578220f9854c.zip",
      "_bibtex": "@inproceedings{\nxie2023dirichletbased,\ntitle={Dirichlet-based Uncertainty Calibration for Active Domain Adaptation},\nauthor={Mixue Xie and Shuang Li and Rui Zhang and Chi Harold Liu},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=4WM4cy42B81}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/dirichlet-based-uncertainty-calibration-for/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279142577,
    "odate": 1664468100000,
    "details": {
      "replyCount": 4,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "d8CBRlWNkqH",
    "original": "qkE1m1aotAB",
    "number": 1111,
    "cdate": 1663849923456,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849923456,
    "tmdate": 1760256802657,
    "tddate": null,
    "forum": "d8CBRlWNkqH",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Neural Optimal Transport",
      "authorids": [
        "~Alexander_Korotin2",
        "~Daniil_Selikhanovych1",
        "~Evgeny_Burnaev1"
      ],
      "authors": [
        "Alexander Korotin",
        "Daniil Selikhanovych",
        "Evgeny Burnaev"
      ],
      "keywords": [
        "weak optimal transport",
        "neural networks"
      ],
      "TL;DR": "We present a novel neural-networks-based algorithm to compute optimal transport maps and plans for strong and weak transport costs.",
      "abstract": "We present a novel neural-networks-based algorithm to compute optimal transport maps and plans for strong and weak transport costs. To justify the usage of neural networks, we prove that they are universal approximators of transport plans between probability distributions. We evaluate the performance of our optimal transport algorithm on toy examples and on the unpaired image-to-image translation.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Generative models",
      "paperhash": "korotin|neural_optimal_transport",
      "pdf": "/pdf/b137a1ea32ff2b3c00faafef118b83c3223bc3eb.pdf",
      "supplementary_material": "",
      "_bibtex": "@inproceedings{\nkorotin2023neural,\ntitle={Neural Optimal Transport},\nauthor={Alexander Korotin and Daniil Selikhanovych and Evgeny Burnaev},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=d8CBRlWNkqH}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 5 code implementations](https://www.catalyzex.com/paper/neural-optimal-transport/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279145368,
    "odate": 1664468100000,
    "details": {
      "replyCount": 9,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "IowKt5rYWsK",
    "original": "lPdhke-_sdE",
    "number": 1136,
    "cdate": 1663849926314,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849926314,
    "tmdate": 1760256801330,
    "tddate": null,
    "forum": "IowKt5rYWsK",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "GPViT: A High Resolution Non-Hierarchical Vision Transformer with Group Propagation",
      "authorids": [
        "~Chenhongyi_Yang3",
        "~Jiarui_Xu1",
        "~Shalini_De_Mello1",
        "~Elliot_J._Crowley1",
        "~Xiaolong_Wang3"
      ],
      "authors": [
        "Chenhongyi Yang",
        "Jiarui Xu",
        "Shalini De Mello",
        "Elliot J. Crowley",
        "Xiaolong Wang"
      ],
      "keywords": [
        "Visual Recognition",
        "Vision transformer architecture"
      ],
      "TL;DR": "A high-resolution vision transformer architecture based on a new efficient global information exchange mechanism for general visual recognition.",
      "abstract": "We present the Group Propagation Vision Transformer (GPViT): a novel non- hierarchical (i.e. non-pyramidal) transformer model designed for general visual recognition with high-resolution features. High-resolution features (or tokens) are a natural fit for tasks that involve perceiving fine-grained details such as detection and segmentation, but exchanging global information between these features is expensive in memory and computation because of the way self-attention scales. We provide a highly efficient alternative Group Propagation Block (GP Block) to exchange global information. In each GP Block, features are first grouped to- gether by a fixed number of learnable group tokens; we then perform Group Propagation where global information is exchanged between the grouped fea- tures; finally, global information in the updated grouped features is returned back to the image features through a transformer decoder. We evaluate GPViT on a variety of visual recognition tasks including image classification, semantic seg- mentation, object detection, and instance segmentation. Our method achieves significant performance gains over previous works across all tasks, especially on tasks that require high-resolution outputs, for example, our GPViT-L3 out- performs Swin Transformer-B by 2.0 mIoU on ADE20K semantic segmentation with only half as many parameters. Code and pre-trained models are available at https://github.com/ChenhongyiYang/GPViT.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Applications (eg, speech processing, computer vision, NLP)",
      "paperhash": "yang|gpvit_a_high_resolution_nonhierarchical_vision_transformer_with_group_propagation",
      "pdf": "/pdf/9542365fc4380de76797ec856ed324fe9acf8f79.pdf",
      "_bibtex": "@inproceedings{\nyang2023gpvit,\ntitle={{GPV}iT: A High Resolution Non-Hierarchical Vision Transformer with Group Propagation},\nauthor={Chenhongyi Yang and Jiarui Xu and Shalini De Mello and Elliot J. Crowley and Xiaolong Wang},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=IowKt5rYWsK}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/gpvit-a-high-resolution-non-hierarchical/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279147400,
    "odate": 1664468100000,
    "details": {
      "replyCount": 11,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "i9ogGQHYbkY",
    "original": "qU6u28Nr5FA",
    "number": 1152,
    "cdate": 1663849928268,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849928268,
    "tmdate": 1677449010078,
    "tddate": null,
    "forum": "i9ogGQHYbkY",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Near-Optimal Adversarial Reinforcement Learning with Switching Costs",
      "authorids": [
        "~Ming_Shi1",
        "~Yingbin_Liang1",
        "~Ness_Shroff1"
      ],
      "authors": [
        "Ming Shi",
        "Yingbin Liang",
        "Ness Shroff"
      ],
      "keywords": [
        "adversarial reinforcement learning",
        "switching costs",
        "regret analysis",
        "lower bound"
      ],
      "TL;DR": "This paper provides the first algorithms with near-optimal regrets for adversarial reinforcement learning with switching costs, and a matching lower bound on the regret.",
      "abstract": "Switching costs, which capture the costs for changing policies, are regarded as a critical metric in reinforcement learning (RL), in addition to the standard metric of losses (or rewards). However, existing studies on switching costs (with a coefficient that is strictly positive and is independent of the time horizon) have mainly focused on static RL, where the loss distribution is assumed to be fixed during the learning process, and thus practical scenarios where the loss distribution could be non-stationary or even adversarial are not considered. While adversarial RL better models this type of practical scenarios, an open problem remains: how to develop a provably efficient algorithm for adversarial RL with switching costs? This paper makes the first effort towards solving this problem. First, we provide a regret lower-bound that shows that the regret of any algorithm must be larger than $\\tilde{\\Omega}( ( H S A )^{1/3} T^{2/3} )$, where $T$, $S$, $A$ and $H$ are the number of episodes, states, actions and layers in each episode, respectively. Our lower bound indicates that, due to the fundamental challenge of switching costs in adversarial RL, the best achieved regret (whose dependency on $T$ is $\\tilde{O}(\\sqrt{T})$) in static RL with switching costs (as well as adversarial RL without switching costs) is no longer achievable. Moreover, we propose two novel switching-reduced algorithms with regrets that match our lower bound when the transition function is known, and match our lower bound within a small factor of $\\tilde{O}( H^{1/3} )$ when the transition function is unknown. Our regret analysis demonstrates the near-optimal performance of them.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Theory (eg, control theory, learning theory, algorithmic game theory)",
      "paperhash": "shi|nearoptimal_adversarial_reinforcement_learning_with_switching_costs",
      "pdf": "/pdf/c49c1d1fb9288fba31814bc7cccd62fe483bf469.pdf",
      "_bibtex": "@inproceedings{\nshi2023nearoptimal,\ntitle={Near-Optimal Adversarial Reinforcement Learning with Switching Costs},\nauthor={Ming Shi and Yingbin Liang and Ness Shroff},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=i9ogGQHYbkY}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279148332,
    "odate": 1664468100000,
    "details": {
      "replyCount": 20,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "ZTMuZ68B1g",
    "original": "34-cHcBKsS",
    "number": 1153,
    "cdate": 1663849928388,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849928388,
    "tmdate": 1677571983416,
    "tddate": null,
    "forum": "ZTMuZ68B1g",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle",
      "authorids": [
        "~Jae_Oh_Woo1"
      ],
      "authors": [
        "Jae Oh Woo"
      ],
      "keywords": [
        "bayesian neural network",
        "bayesian active learning",
        "balanced entropy learning",
        "uncertainty quantification"
      ],
      "TL;DR": "We propose a new bayesian active learning principle.",
      "abstract": "Acquiring labeled data is challenging in many machine learning applications with limited budgets. Active learning gives a procedure to select the most informative data points and improve data efficiency by reducing the cost of labeling. The info-max learning principle maximizing mutual information such as BALD has been successful and widely adapted in various active learning applications. However, this pool-based specific objective inherently introduces a redundant selection and further requires a high computational cost for batch selection. In this paper, we design and propose a new uncertainty measure, Balanced Entropy Acquisition (BalEntAcq), which captures the information balance between the uncertainty of underlying softmax probability and the label variable. To do this, we approximate each marginal distribution by Beta distribution. Beta approximation enables us to formulate BalEntAcq as a ratio between an augmented entropy and the marginalized joint entropy. The closed-form expression of BalEntAcq facilitates parallelization by estimating two parameters in each marginal Beta distribution. BalEntAcq is a purely standalone measure without requiring any relational computations with other data points. Nevertheless, BalEntAcq captures a well-diversified selection near the decision boundary with a margin, unlike other existing uncertainty measures such as BALD, Entropy, or Mean Standard Deviation (MeanSD). Finally, we demonstrate that our balanced entropy learning principle with BalEntAcq consistently outperforms well-known linearly scalable active learning methods, including a recently proposed PowerBALD, a simple but diversified version of BALD, by showing experimental results obtained from MNIST, CIFAR-100, SVHN, and TinyImageNet datasets.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Probabilistic Methods (eg, variational inference, causal inference, Gaussian processes)",
      "paperhash": "woo|active_learning_in_bayesian_neural_networks_with_balanced_entropy_learning_principle",
      "pdf": "/pdf/4919425fc00a999aa99ff64ba1e275ca945e9f6f.pdf",
      "supplementary_material": "/attachment/1ade9c8d935be2c53f27740c0e3c579a5b953a99.zip",
      "_bibtex": "@inproceedings{\nwoo2023active,\ntitle={Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle},\nauthor={Jae Oh Woo},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=ZTMuZ68B1g}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279148355,
    "odate": 1664468100000,
    "details": {
      "replyCount": 14,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "aFzaXRImWE",
    "original": "lZ17YKzBwN",
    "number": 1154,
    "cdate": 1663849928508,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849928508,
    "tmdate": 1677543810223,
    "tddate": null,
    "forum": "aFzaXRImWE",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "A Holistic View of Label Noise Transition Matrix in Deep Learning and Beyond",
      "authorids": [
        "~LIN_Yong1",
        "~Renjie_Pi1",
        "~WEIZHONG_ZHANG2",
        "~Xiaobo_Xia1",
        "~Jiahui_Gao2",
        "~Xiao_Zhou4",
        "~Tongliang_Liu1",
        "~Bo_Han1"
      ],
      "authors": [
        "LIN Yong",
        "Renjie Pi",
        "WEIZHONG ZHANG",
        "Xiaobo Xia",
        "Jiahui Gao",
        "Xiao Zhou",
        "Tongliang Liu",
        "Bo Han"
      ],
      "keywords": [],
      "abstract": "In this paper, we explore learning statistically consistent classifiers under label noise by estimating the noise transition matrix T. We first provide a holistic view of existing T-estimation methods including those with or without anchor point assumptions.  We unified them into the Minimum Geometric Envelope Operator (MGEO) framework, which tries to find the smallest T (in terms of a certain metric) that elicits a convex hull to enclose the posteriors of all the training data. Although MGEO methods show appealing theoretical properties and empirical results, we find them prone to failing when the noisy posterior estimation is imperfect, which is inevitable in practice. Specifically, we show that MGEO methods are in-consistent even with infinite samples if the noisy posterior is not estimated accurately. In view of this, we make the first effort to address this issue by proposing a novel T-estimation framework via the lens of bilevel optimization, and term it RObust Bilevel OpTimzation (ROBOT). ROBOT paves a new road beyond MGEO framework, which enjoys strong theoretical properties: identifibility, consistency and finite-sample generalization guarantees. Notably, ROBOT neither requires the perfect posterior estimation nor assumes the existence of anchor points. We further theoretically demonstrate that ROBOT is more robust in the case where MGEO methods fail. Experimentally, our framework also shows superior performance across multiple benchmarks.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "General Machine Learning (ie none of the above)",
      "paperhash": "yong|a_holistic_view_of_label_noise_transition_matrix_in_deep_learning_and_beyond",
      "pdf": "/pdf/405fab9c74f731a957a6e9ee24c23a06a6809b77.pdf",
      "supplementary_material": "/attachment/429434df65546c46f62904b772e5fa83c55ce987.zip",
      "_bibtex": "@inproceedings{\nyong2023a,\ntitle={A Holistic View of Label Noise Transition Matrix in Deep Learning and Beyond},\nauthor={LIN Yong and Renjie Pi and WEIZHONG ZHANG and Xiaobo Xia and Jiahui Gao and Xiao Zhou and Tongliang Liu and Bo Han},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=aFzaXRImWE}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279148397,
    "odate": 1664468100000,
    "details": {
      "replyCount": 19,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "GcM7qfl5zY",
    "original": "f109AF8VQaf",
    "number": 1210,
    "cdate": 1663849934659,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849934659,
    "tmdate": 1678006207465,
    "tddate": null,
    "forum": "GcM7qfl5zY",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "AutoGT: Automated Graph Transformer Architecture Search",
      "authorids": [
        "~Zizhao_Zhang4",
        "~Xin_Wang17",
        "~Chaoyu_Guan1",
        "~Ziwei_Zhang1",
        "~Haoyang_Li1",
        "~Wenwu_Zhu1"
      ],
      "authors": [
        "Zizhao Zhang",
        "Xin Wang",
        "Chaoyu Guan",
        "Ziwei Zhang",
        "Haoyang Li",
        "Wenwu Zhu"
      ],
      "keywords": [],
      "abstract": "Although Transformer architectures have been successfully applied to graph data with the advent of Graph Transformer, current design of Graph Transformer still heavily relies on human labor and expertise knowledge to decide proper neural architectures and suitable graph encoding strategies at each Transformer layer. In literature, there have been some works on automated design of Transformers focusing on non-graph data such as texts and images without considering graph encoding strategies, which fail to handle the non-euclidean graph data. In this paper, we study the problem of automated graph Transformer, for the first time. However, solving these problems poses the following challenges: i) how can we design a unified search space for graph Transformer, and ii) how to deal with the coupling relations between Transformer architectures and the graph encodings of each Transformer layer. To address these challenges, we propose Automated Graph Transformer (AutoGT), a neural architecture search framework that can automatically discover the optimal graph Transformer architectures by joint optimization of Transformer architecture and graph encoding strategies. Specifically, we first propose a unified graph Transformer formulation that can represent most of state-of-the-art graph Transformer architectures. Based upon the unified formulation, we further design the graph Transformer search space that includes both candidate architectures and various graph encodings. To handle the coupling relations, we propose a novel encoding-aware performance estimation strategy by gradually training and splitting the supernets according to the correlations between graph encodings and architectures. The proposed strategy can provide a more consistent and fine-grained performance prediction when evaluating the jointly optimized graph encodings and architectures. Extensive experiments and ablation studies show that our proposed AutoGT gains sufficient improvement over state-of-the-art hand-crafted baselines on all datasets, demonstrating its effectiveness and wide applicability.\n",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "zhang|autogt_automated_graph_transformer_architecture_search",
      "pdf": "/pdf/ea1ae3473367dc3011d3f2b84c2b2192c39aee04.pdf",
      "_bibtex": "@inproceedings{\nzhang2023autogt,\ntitle={Auto{GT}: Automated Graph Transformer Architecture Search},\nauthor={Zizhao Zhang and Xin Wang and Chaoyu Guan and Ziwei Zhang and Haoyang Li and Wenwu Zhu},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=GcM7qfl5zY}\n}",
      "venue": "ICLR 2023 notable top 5%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279150912,
    "odate": 1664468100000,
    "details": {
      "replyCount": 9,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "g8wBdhnstYz",
    "original": "mzDGLW1pWEB",
    "number": 1227,
    "cdate": 1663849936758,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849936758,
    "tmdate": 1677764255842,
    "tddate": null,
    "forum": "g8wBdhnstYz",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Deterministic training of generative autoencoders using invertible layers",
      "authorids": [
        "~Gianluigi_Silvestri1",
        "~Daan_Roos1",
        "~Luca_Ambrogioni1"
      ],
      "authors": [
        "Gianluigi Silvestri",
        "Daan Roos",
        "Luca Ambrogioni"
      ],
      "keywords": [],
      "abstract": "In this work, we provide a deterministic alternative to the stochastic variational training of generative autoencoders. We refer to these new generative autoencoders as AutoEncoders within Flows (AEF), since the encoder and decoder are defined as affine layers of an overall invertible architecture. This results in a deterministic encoding of the data, as opposed to the stochastic encoding of VAEs. The paper introduces two related families of AEFs. The first family relies on a partition of the ambient space and is trained by exact maximum-likelihood. The second family exploits a deterministic expansion of the ambient space and is trained by maximizing the log-probability in this extended space. This latter case leaves complete freedom in the choice of encoder, decoder and prior architectures, making it a drop-in replacement for the training of existing VAEs and VAE-style models. We show that these AEFs can have strikingly higher performance than architecturally identical VAEs in terms of log-likelihood and sample quality, especially for low dimensional latent spaces. Importantly, we show that AEF samples are substantially sharper than VAE samples. ",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Generative models",
      "paperhash": "silvestri|deterministic_training_of_generative_autoencoders_using_invertible_layers",
      "pdf": "/pdf/78c7fb939078a784f02006f7272c92a758e1e9c7.pdf",
      "_bibtex": "@inproceedings{\nsilvestri2023deterministic,\ntitle={Deterministic training of generative autoencoders using invertible layers},\nauthor={Gianluigi Silvestri and Daan Roos and Luca Ambrogioni},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=g8wBdhnstYz}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279152514,
    "odate": 1664468100000,
    "details": {
      "replyCount": 19,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "p7G8t5FVn2h",
    "original": "qsbQUn8THL",
    "number": 1277,
    "cdate": 1663849942645,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849942645,
    "tmdate": 1760256795594,
    "tddate": null,
    "forum": "p7G8t5FVn2h",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "One-Pixel Shortcut: On the Learning Preference of Deep Neural Networks",
      "authorids": [
        "~Shutong_Wu1",
        "~Sizhe_Chen1",
        "~Cihang_Xie3",
        "~Xiaolin_Huang1"
      ],
      "authors": [
        "Shutong Wu",
        "Sizhe Chen",
        "Cihang Xie",
        "Xiaolin Huang"
      ],
      "keywords": [
        "unlearnable examples",
        "shortcut learning",
        "one-pixel feature",
        "deep neural network"
      ],
      "TL;DR": "We propose a model-free method to craft unlearnable example by perturbing only one pixel, and construct a benchmark containing images that are unlearnable by various existing methods to avoid shortcut learning.",
      "abstract": "Unlearnable examples (ULEs) aim to protect data from unauthorized usage for training DNNs. Existing work adds $\\ell_\\infty$-bounded perturbations to the original sample so that the trained model generalizes poorly. Such perturbations, however, are easy to eliminate by adversarial training and data augmentations. In this paper, we resolve this problem from a novel perspective by perturbing only one pixel in each image. Interestingly, such a small modification could effectively degrade model accuracy to almost an untrained counterpart. Moreover, our produced \\emph{One-Pixel Shortcut (OPS)} could not be erased by adversarial training and strong augmentations. To generate OPS, we perturb in-class images at the same position to the same target value that could mostly and stably deviate from all the original images. Since such generation is only based on images, OPS needs significantly less computation cost than the previous methods using DNN generators. Based on OPS, we introduce an unlearnable dataset called CIFAR-10-S, which is indistinguishable from CIFAR-10 by humans but induces the trained model to extremely low accuracy. Even under adversarial training, a ResNet-18 trained on CIFAR-10-S has only 10.61% accuracy, compared to 83.02% by the existing error-minimizing method.",
      "pdf": "/pdf/b69561625d5ce4388db999c205fdb5a8b988725e.pdf",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "supplementary_material": "/attachment/da6c321999c823b721cfa616459f879e63ee4d82.zip",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "wu|onepixel_shortcut_on_the_learning_preference_of_deep_neural_networks",
      "_bibtex": "@inproceedings{\nwu2023onepixel,\ntitle={One-Pixel Shortcut: On the Learning Preference of Deep Neural Networks},\nauthor={Shutong Wu and Sizhe Chen and Cihang Xie and Xiaolin Huang},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=p7G8t5FVn2h}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/one-pixel-shortcut-on-the-learning-preference/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279155225,
    "odate": 1664468100000,
    "details": {
      "replyCount": 16,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "0ypGZvm0er0",
    "original": "a2tE7mGVFa",
    "number": 1283,
    "cdate": 1663849943246,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849943246,
    "tmdate": 1760256795444,
    "tddate": null,
    "forum": "0ypGZvm0er0",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "View Synthesis with Sculpted Neural Points",
      "authorids": [
        "~Yiming_Zuo2",
        "~Jia_Deng1"
      ],
      "authors": [
        "Yiming Zuo",
        "Jia Deng"
      ],
      "keywords": [],
      "abstract": "We address the task of view synthesis, generating novel views of a scene given a set of images as input. In many recent works such as NeRF (Mildenhall et al., 2020), the scene geometry is parameterized using neural implicit representations (i.e., MLPs). Implicit neural representations have achieved impressive visual quality but have drawbacks in computational efficiency. In this work, we propose a new approach that performs view synthesis using point clouds. It is the first point-based method that achieves better visual quality than NeRF while being 100× faster in rendering speed. Our approach builds on existing works on differentiable point-based rendering but introduces a novel technique we call “Sculpted Neural Points (SNP)”, which significantly improves the robustness to errors and holes in the reconstructed point cloud. We further propose to use view-dependent point features based on spherical harmonics to capture non-Lambertian surfaces, and new designs in the point-based rendering pipeline that further boost the performance. Finally, we show that our system supports fine-grained scene editing. Code is available at https://github.com/princeton-vl/SNP.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Applications (eg, speech processing, computer vision, NLP)",
      "paperhash": "zuo|view_synthesis_with_sculpted_neural_points",
      "pdf": "/pdf/a844600e54c069b827ba8e0013a60b4a1193f97f.pdf",
      "supplementary_material": "/attachment/48cb4d26c510c2629c1b3743c42bc78bc250c882.zip",
      "_bibtex": "@inproceedings{\nzuo2023view,\ntitle={View Synthesis with Sculpted Neural Points},\nauthor={Yiming Zuo and Jia Deng},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=0ypGZvm0er0}\n}",
      "venue": "ICLR 2023 notable top 5%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/view-synthesis-with-sculpted-neural-points/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279155763,
    "odate": 1664468100000,
    "details": {
      "replyCount": 7,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "pxStyaf2oJ5",
    "original": "3rEm76j1Ru",
    "number": 1287,
    "cdate": 1663849943710,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849943710,
    "tmdate": 1760256795100,
    "tddate": null,
    "forum": "pxStyaf2oJ5",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Domain-Indexing Variational Bayes: Interpretable Domain Index for Domain Adaptation",
      "authorids": [
        "~Zihao_Xu2",
        "~Guang-Yuan_Hao1",
        "~Hao_He1",
        "~Hao_Wang3"
      ],
      "authors": [
        "Zihao Xu",
        "Guang-Yuan Hao",
        "Hao He",
        "Hao Wang"
      ],
      "keywords": [],
      "abstract": "Previous studies have shown that leveraging \"domain index\" can significantly boost domain adaptation performance (Wang et al., 2020; Xu et al., 2022). However, such domain indices are not always available. To address this challenge, we first provide a formal definition of domain index from the probabilistic perspective, and then propose an adversarial variational Bayesian framework that infers domain indices from multi-domain data, thereby providing additional insight on domain relations and improving domain adaptation performance. Our theoretical analysis shows that our adversarial variational Bayesian framework finds the optimal domain index at equilibrium. Empirical results on both synthetic and real data verify that our model can produce interpretable domain indices which enable us to achieve superior performance compared to state-of-the-art domain adaptation methods. Code is available at https://github.com/Wang-ML-Lab/VDI.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Probabilistic Methods (eg, variational inference, causal inference, Gaussian processes)",
      "paperhash": "xu|domainindexing_variational_bayes_interpretable_domain_index_for_domain_adaptation",
      "pdf": "/pdf/4340ecd2eb1d6cddf23d0257a4ab36cd01fba41e.pdf",
      "_bibtex": "@inproceedings{\nxu2023domainindexing,\ntitle={Domain-Indexing Variational Bayes: Interpretable Domain Index for Domain Adaptation},\nauthor={Zihao Xu and Guang-Yuan Hao and Hao He and Hao Wang},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=pxStyaf2oJ5}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/domain-indexing-variational-bayes/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279156015,
    "odate": 1664468100000,
    "details": {
      "replyCount": 19,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "Zy350cRstc6",
    "original": "d_Bstd5nR_",
    "number": 1293,
    "cdate": 1663849944424,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849944424,
    "tmdate": 1760256794827,
    "tddate": null,
    "forum": "Zy350cRstc6",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Continual evaluation for lifelong learning: Identifying the stability gap",
      "authorids": [
        "~Matthias_De_Lange1",
        "~Gido_M_van_de_Ven1",
        "~Tinne_Tuytelaars1"
      ],
      "authors": [
        "Matthias De Lange",
        "Gido M van de Ven",
        "Tinne Tuytelaars"
      ],
      "keywords": [
        "Continual learning",
        "lifelong learning",
        "incremental learning",
        "evaluation metrics"
      ],
      "TL;DR": "Proposing an iteration-based continual evaluation framework for CL, we discover, quantify, and analyse the \"stability gap\", a phenomenon where upon learning new tasks, past tasks exhibit substantial but transient performance loss for SOTA CL methods.",
      "abstract": "Time-dependent data-generating distributions have proven to be difficult for gradient-based training of neural networks, as the greedy updates result in catastrophic forgetting of previously learned knowledge. Despite the progress in the field of continual learning to overcome this forgetting, we show that a set of common state-of-the-art methods still suffers from substantial forgetting upon starting to learn new tasks, except that this forgetting is temporary and followed by a phase of performance recovery. We refer to this intriguing but potentially problematic phenomenon as the stability gap. The stability gap had likely remained under the radar due to standard practice in the field of evaluating continual learning models only after each task. Instead, we establish a framework for continual evaluation that uses per-iteration evaluation and we define a new set of metrics to quantify worst-case performance. Empirically we show that experience replay, constraint-based replay, knowledge-distillation, and parameter regularization methods are all prone to the stability gap; and that the stability gap can be observed in class-, task-, and domain-incremental learning benchmarks. Additionally, a controlled experiment shows that the stability gap increases when tasks are more dissimilar. Finally, by disentangling gradients into plasticity and stability components, we propose a conceptual explanation for the stability gap.",
      "pdf": "/pdf/913d2396e313fdd690c50b875b3c31efaa2e05a5.pdf",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "lange|continual_evaluation_for_lifelong_learning_identifying_the_stability_gap",
      "_bibtex": "@inproceedings{\nlange2023continual,\ntitle={Continual evaluation for lifelong learning: Identifying the stability gap},\nauthor={Matthias De Lange and Gido M van de Ven and Tinne Tuytelaars},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=Zy350cRstc6}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/continual-evaluation-for-lifelong-learning/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279156754,
    "odate": 1664468100000,
    "details": {
      "replyCount": 19,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "U2g8OGONA_V",
    "original": "tcuVrRZshTT",
    "number": 1305,
    "cdate": 1663849945737,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849945737,
    "tmdate": 1677731596522,
    "tddate": null,
    "forum": "U2g8OGONA_V",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Multi-domain image generation and translation with identifiability guarantees",
      "authorids": [
        "~Shaoan_Xie4",
        "~Lingjing_Kong1",
        "~Mingming_Gong1",
        "~Kun_Zhang1"
      ],
      "authors": [
        "Shaoan Xie",
        "Lingjing Kong",
        "Mingming Gong",
        "Kun Zhang"
      ],
      "keywords": [
        "multi-domain image generation",
        "image translation",
        "identifiability",
        "Nonlinear ICA"
      ],
      "abstract": "Multi-domain image generation and unpaired image-to-to-image translation are two important and related computer vision problems. The common technique for the two tasks is the learning of a joint distribution from multiple marginal distributions. However, it is well known that there can be infinitely many joint distributions that can derive the same marginals. Hence, it is necessary to formulate suitable constraints to address this highly ill-posed problem. Inspired by the recent advances in nonlinear Independent Component Analysis (ICA) theory, we propose a new method to learn the joint distribution from the marginals by enforcing a specific type of minimal change across domains. We report one of the first results connecting multi-domain generative models to identifiability and shows why identifiability is essential and how to achieve it theoretically and practically. We apply our method to five multi-domain image generation and six image-to-image translation tasks. The superior performance of our model supports our theory and demonstrates the effectiveness of our method. The training code are available at https://github.com/Mid-Push/i-stylegan.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Generative models",
      "paperhash": "xie|multidomain_image_generation_and_translation_with_identifiability_guarantees",
      "TL;DR": "We propose a way to learn the pairing information from unpaired data with theoretial guarantees, with direct applications in learning tasks such as image-to-image translation",
      "pdf": "/pdf/51f8278f376fd961504ae802f4d2f35deeb936d7.pdf",
      "supplementary_material": "/attachment/627c2a1e20901494783c437e41e440c81b7dbde3.zip",
      "_bibtex": "@inproceedings{\nxie2023multidomain,\ntitle={Multi-domain image generation and translation with identifiability guarantees},\nauthor={Shaoan Xie and Lingjing Kong and Mingming Gong and Kun Zhang},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=U2g8OGONA_V}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279157742,
    "odate": 1664468100000,
    "details": {
      "replyCount": 18,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "9y0HFvaAYD6",
    "original": "SIn27Ds_mYc",
    "number": 1335,
    "cdate": 1663849949266,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849949266,
    "tmdate": 1760256793504,
    "tddate": null,
    "forum": "9y0HFvaAYD6",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Hidden Markov Transformer for Simultaneous Machine Translation",
      "authorids": [
        "~Shaolei_Zhang1",
        "~Yang_Feng4"
      ],
      "authors": [
        "Shaolei Zhang",
        "Yang Feng"
      ],
      "keywords": [
        "Simultaneous machine translation",
        "Machine translation",
        "Natural language processing",
        "Transformer"
      ],
      "abstract": "Simultaneous machine translation (SiMT) outputs the target sequence while receiving the source sequence, and hence learning when to start translating each target token is the core challenge for SiMT task. However, it is non-trivial to learn the optimal moment among many possible moments of starting translating, as the moments of starting translating always hide inside the model and can only be supervised with the observed target sequence. In this paper, we propose a Hidden Markov Transformer (HMT), which treats the moments of starting translating as hidden events and the target sequence as the corresponding observed events, thereby organizing them as a hidden Markov model. HMT explicitly models multiple moments of starting translating as the candidate hidden events, and then selects one to generate the target token. During training, by maximizing the marginal likelihood of the target sequence over multiple moments of starting translating, HMT learns to start translating at the moments that target tokens can be generated more accurately. Experiments on multiple SiMT benchmarks show that HMT outperforms strong baselines and achieves state-of-the-art performance.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Applications (eg, speech processing, computer vision, NLP)",
      "paperhash": "zhang|hidden_markov_transformer_for_simultaneous_machine_translation",
      "pdf": "/pdf/fcf9747a3df24a2f10acd861765126ce790b5424.pdf",
      "_bibtex": "@inproceedings{\nzhang2023hidden,\ntitle={Hidden Markov Transformer for Simultaneous Machine Translation},\nauthor={Shaolei Zhang and Yang Feng},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=9y0HFvaAYD6}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/hidden-markov-transformer-for-simultaneous/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279159208,
    "odate": 1664468100000,
    "details": {
      "replyCount": 12,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "XXTyv1zD9zD",
    "original": "kSA7z89kCx",
    "number": 1362,
    "cdate": 1663849952926,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849952926,
    "tmdate": 1760256792473,
    "tddate": null,
    "forum": "XXTyv1zD9zD",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Packed Ensembles for efficient uncertainty estimation",
      "authorids": [
        "~Olivier_Laurent1",
        "~Adrien_Lafage1",
        "~Enzo_Tartaglione1",
        "~Geoffrey_Daniel1",
        "~Jean-marc_Martinez1",
        "~Andrei_Bursuc1",
        "~Gianni_Franchi1"
      ],
      "authors": [
        "Olivier Laurent",
        "Adrien Lafage",
        "Enzo Tartaglione",
        "Geoffrey Daniel",
        "Jean-marc Martinez",
        "Andrei Bursuc",
        "Gianni Franchi"
      ],
      "keywords": [
        "Efficient Ensembling",
        "Uncertainty Quantification",
        "OOD Detection"
      ],
      "abstract": "Deep Ensembles (DE) are a prominent approach for achieving excellent performance on key metrics such as accuracy, calibration, uncertainty estimation, and out-of-distribution detection. However, hardware limitations of real-world systems constrain to smaller ensembles and lower-capacity networks, significantly deteriorating their performance and properties. We introduce Packed-Ensembles (PE), a strategy to design and train lightweight structured ensembles by carefully modulating the dimension of their encoding space. We leverage grouped convolutions to parallelize the ensemble into a single shared backbone and forward pass to improve training and inference speeds. PE is designed to operate within the memory limits of a standard neural network. Our extensive research indicates that PE accurately preserves the properties of DE, such as diversity, and performs equally well in terms of accuracy, calibration, out-of-distribution detection, and robustness to distribution shift. We make our code available at https://github.com/ENSTA-U2IS/torch-uncertainty.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "General Machine Learning (ie none of the above)",
      "paperhash": "laurent|packed_ensembles_for_efficient_uncertainty_estimation",
      "TL;DR": "Packed-Ensembles leverage the width of DNNs and grouped convolutions to train subnetworks in parallel and form an efficient ensemble.",
      "pdf": "/pdf/ca8af472cc5062b34c4e52f4fcb5b8591d4474c9.pdf",
      "supplementary_material": "/attachment/2fd42b3fc4774c1f8ab80bab361e1c560c55e2ac.zip",
      "_bibtex": "@inproceedings{\nlaurent2023packed,\ntitle={Packed Ensembles for efficient uncertainty estimation},\nauthor={Olivier Laurent and Adrien Lafage and Enzo Tartaglione and Geoffrey Daniel and Jean-marc Martinez and Andrei Bursuc and Gianni Franchi},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=XXTyv1zD9zD}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/packed-ensembles-for-efficient-uncertainty/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279160605,
    "odate": 1664468100000,
    "details": {
      "replyCount": 12,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "6fuPIe9tbnC",
    "original": "8cP4Pn6KD3",
    "number": 1367,
    "cdate": 1663849953518,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849953518,
    "tmdate": 1760256791913,
    "tddate": null,
    "forum": "6fuPIe9tbnC",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Multifactor Sequential Disentanglement via Structured Koopman Autoencoders",
      "authorids": [
        "~Nimrod_Berman1",
        "~Ilan_Naiman1",
        "~Omri_Azencot1"
      ],
      "authors": [
        "Nimrod Berman",
        "Ilan Naiman",
        "Omri Azencot"
      ],
      "keywords": [
        "Koopman methods",
        "Sequential Disentanglement"
      ],
      "TL;DR": "A new method for learning multifactor disentangled representations of sequential data",
      "abstract": "Disentangling complex data to its latent factors of variation is a fundamental task in representation learning. Existing work on sequential disentanglement mostly provides two factor representations, i.e., it separates the data to time-varying and time-invariant factors. In contrast, we consider multifactor disentanglement in which multiple (more than two) semantic disentangled components are generated. Key to our approach is a strong inductive bias where we assume that the underlying dynamics can be represented linearly in the latent space. Under this assumption, it becomes natural to exploit the recently introduced Koopman autoencoder models. However, disentangled representations are not guaranteed in Koopman approaches, and thus we propose a novel spectral loss term which leads to structured Koopman matrices and disentanglement. Overall, we propose a simple and easy to code new deep model that is fully unsupervised and it supports multifactor disentanglement. We showcase new disentangling abilities such as swapping of individual static factors between characters, and an incremental swap of disentangled factors from the source to the target. Moreover, we evaluate our method extensively on two factor standard benchmark tasks where we significantly improve over competing unsupervised approaches, and we perform competitively in comparison to weakly- and self-supervised state-of-the-art approaches. The code is available at https://github.com/azencot-group/SKD.",
      "pdf": "/pdf/80996ea72234008065b9f90cd4275bc159fa8565.pdf",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "supplementary_material": "/attachment/bfff9d538e012f5bab63814d7173609342bcb3fe.zip",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "berman|multifactor_sequential_disentanglement_via_structured_koopman_autoencoders",
      "_bibtex": "@inproceedings{\nberman2023multifactor,\ntitle={Multifactor Sequential Disentanglement via Structured Koopman Autoencoders},\nauthor={Nimrod Berman and Ilan Naiman and Omri Azencot},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=6fuPIe9tbnC}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/multifactor-sequential-disentanglement-via/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279161117,
    "odate": 1664468100000,
    "details": {
      "replyCount": 11,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "-P7G-8dmSh4",
    "original": "ytGLntB8NU",
    "number": 1405,
    "cdate": 1663849957851,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849957851,
    "tmdate": 1760256790482,
    "tddate": null,
    "forum": "-P7G-8dmSh4",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Formal Mathematics Statement Curriculum Learning",
      "authorids": [
        "spolu@protonmail.com",
        "~Jesse_Michael_Han1",
        "~Kunhao_Zheng1",
        "~Mantas_Baksys1",
        "~Igor_Babuschkin1",
        "~Ilya_Sutskever1"
      ],
      "authors": [
        "Stanislas Polu",
        "Jesse Michael Han",
        "Kunhao Zheng",
        "Mantas Baksys",
        "Igor Babuschkin",
        "Ilya Sutskever"
      ],
      "keywords": [
        "neural theorem proving",
        "formal mathematics",
        "language modeling",
        "expert iteration"
      ],
      "abstract": "We explore the use of expert iteration in the context of language modeling applied to formal mathematics. We show that at same compute budget, expert iteration, by which we mean proof search interleaved with learning, dramatically outperforms proof search only. We also observe that when applied to a collection of formal statements of sufficiently varied difficulty, expert iteration is capable of finding and solving a curriculum of increasingly difficult problems, without the need for associated ground-truth proofs. Finally, by applying this expert iteration to a manually curated set of problem statements, we surpass previous state-of-the-art on the miniF2F benchmark, automatically solving multiple challenging problems drawn from high school olympiads.",
      "pdf": "/pdf/f13db9db8ab1cc1e6f7db9c1754c276c7c7601ed.pdf",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Machine Learning for Sciences (eg biology, physics, health sciences, social sciences, climate/sustainability )",
      "paperhash": "polu|formal_mathematics_statement_curriculum_learning",
      "_bibtex": "@inproceedings{\npolu2023formal,\ntitle={Formal Mathematics Statement Curriculum Learning},\nauthor={Stanislas Polu and Jesse Michael Han and Kunhao Zheng and Mantas Baksys and Igor Babuschkin and Ilya Sutskever},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=-P7G-8dmSh4}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/formal-mathematics-statement-curriculum/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279162955,
    "odate": 1664468100000,
    "details": {
      "replyCount": 8,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "9gfir3fSy3J",
    "original": "fjgRKCPSx_b",
    "number": 1421,
    "cdate": 1663849959556,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849959556,
    "tmdate": 1760256789853,
    "tddate": null,
    "forum": "9gfir3fSy3J",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "NeRN: Learning Neural Representations for Neural Networks",
      "authorids": [
        "~Maor_Ashkenazi1",
        "~Zohar_Rimon1",
        "~Ron_Vainshtein1",
        "~Shir_Levi1",
        "~Elad_Richardson2",
        "~Pinchas_Mintz1",
        "~Eran_Treister1"
      ],
      "authors": [
        "Maor Ashkenazi",
        "Zohar Rimon",
        "Ron Vainshtein",
        "Shir Levi",
        "Elad Richardson",
        "Pinchas Mintz",
        "Eran Treister"
      ],
      "keywords": [
        "Convolutional Neural Networks",
        "Neural Representations",
        "Implicit Representations"
      ],
      "TL;DR": "In this paper we present NerN: a neural representation for the weights of a pretrained neural network, which is obtained by applying smoothness over the reconstructed weights and various knowledge distillation techniques",
      "abstract": "Neural Representations have recently been shown to effectively reconstruct a wide range of signals from 3D meshes and shapes to images and videos. We show that, when adapted correctly, neural representations can be used to directly represent the weights of a pre-trained convolutional neural network, resulting in a Neural Representation for Neural Networks (NeRN). Inspired by coordinate inputs of previous neural representation methods, we assign a coordinate to each convolutional kernel in our network based on its position in the architecture, and optimize a predictor network to map coordinates to their corresponding weights. Similarly to the spatial smoothness of visual scenes, we show that incorporating a smoothness constraint over the original network's weights aids NeRN towards a better reconstruction. In addition, since slight perturbations in pre-trained model weights can result in a considerable accuracy loss, we employ techniques from the field of knowledge distillation to stabilize the learning process. We demonstrate the effectiveness of NeRN in reconstructing widely used architectures on CIFAR-10, CIFAR-100, and ImageNet. Finally, we present two applications using NeRN, demonstrating the capabilities of the learned representations.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "ashkenazi|nern_learning_neural_representations_for_neural_networks",
      "pdf": "/pdf/7a5e2aeccac1ea354d122a24d739db89c51f2599.pdf",
      "supplementary_material": "/attachment/bcf5ec7864fde8d72756ed6f516572a3a518615e.zip",
      "_bibtex": "@inproceedings{\nashkenazi2023nern,\ntitle={Ne{RN}: Learning Neural Representations for Neural Networks},\nauthor={Maor Ashkenazi and Zohar Rimon and Ron Vainshtein and Shir Levi and Elad Richardson and Pinchas Mintz and Eran Treister},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=9gfir3fSy3J}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/nern-learning-neural-representations-for/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279163861,
    "odate": 1664468100000,
    "details": {
      "replyCount": 10,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "vtVDI3w_BLL",
    "original": "IdHiCciuM7",
    "number": 1430,
    "cdate": 1663849960506,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849960506,
    "tmdate": 1760256789361,
    "tddate": null,
    "forum": "vtVDI3w_BLL",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "AANG : Automating Auxiliary Learning",
      "authorids": [
        "~Lucio_M._Dery1",
        "~Paul_Michel1",
        "~Mikhail_Khodak1",
        "~Graham_Neubig1",
        "~Ameet_Talwalkar1"
      ],
      "authors": [
        "Lucio M. Dery",
        "Paul Michel",
        "Mikhail Khodak",
        "Graham Neubig",
        "Ameet Talwalkar"
      ],
      "keywords": [
        "auxiliary learning",
        "automl",
        "natural language processing",
        "meta-learning",
        "algorithmic stability",
        "multitask learning"
      ],
      "TL;DR": "We automatically generate a suite of auxiliary objectives and give a theoretically informed, efficient algorithm for searching the space of generated objectives to find those most useful to a specified end-task.",
      "abstract": "Auxiliary objectives, supplementary learning signals that are introduced to help aid learning on data-starved or highly complex end-tasks, are commonplace in machine learning. Whilst much work has been done to formulate useful auxiliary objectives, their construction is still an art which proceeds by slow and tedious hand-design. Intuition for how and when these objectives improve end-task performance has also had limited theoretical backing. In this work, we present an approach for automatically generating a suite of auxiliary objectives. We achieve this by deconstructing existing objectives within a novel unified taxonomy, identifying connections between them, and generating new ones based on the uncovered structure. Next, we theoretically formalize widely-held intuitions about how auxiliary learning improves generalization on the end-task. This leads us to a principled and efficient algorithm for searching the space of generated objectives to find those most useful to a specified end-task.\nWith natural language processing (NLP) as our domain of study, we demonstrate that our automated auxiliary learning pipeline leads to strong improvements over competitive baselines across continued training experiments on a pre-trained model on 5 NLP end-tasks.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Unsupervised and Self-supervised learning",
      "paperhash": "dery|aang_automating_auxiliary_learning",
      "pdf": "/pdf/89801dac56ce056d438ce9105f85d897747fa081.pdf",
      "supplementary_material": "/attachment/63c1d8036f53ddb644881cc07ab785dbb7cdbd59.zip",
      "_bibtex": "@inproceedings{\ndery2023aang,\ntitle={{AANG} : Automating Auxiliary Learning},\nauthor={Lucio M. Dery and Paul Michel and Mikhail Khodak and Graham Neubig and Ameet Talwalkar},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=vtVDI3w_BLL}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/aang-automating-auxiliary-learning/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279164610,
    "odate": 1664468100000,
    "details": {
      "replyCount": 19,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "gOZ_pKANaPW",
    "original": "EZCg7ngdhM",
    "number": 1478,
    "cdate": 1663849966126,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849966126,
    "tmdate": 1677639284530,
    "tddate": null,
    "forum": "gOZ_pKANaPW",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Unsupervised Model Selection for Time Series Anomaly Detection",
      "authorids": [
        "~Mononito_Goswami1",
        "~Cristian_Ignacio_Challu1",
        "~Laurent_Callot1",
        "~Lenon_Minorics1",
        "~Andrey_Kan1"
      ],
      "authors": [
        "Mononito Goswami",
        "Cristian Ignacio Challu",
        "Laurent Callot",
        "Lenon Minorics",
        "Andrey Kan"
      ],
      "keywords": [
        "Time Series",
        "Anomaly Detection",
        "Model Selection",
        "Unsupervised Learning",
        "Rank Aggregation"
      ],
      "TL;DR": "This paper answers the question-- Given an unlabeled dataset and a set of candidate time series anomaly detectors, how can we select the most accurate model?",
      "abstract": "Anomaly detection in time-series has a wide range of practical applications. While numerous anomaly detection methods have been proposed in the literature, a recent survey concluded that no single method is the most accurate across various datasets. To make matters worse, anomaly labels are scarce and rarely available in practice. The practical problem of selecting the most accurate model for a given dataset without labels has received little attention in the literature. This paper answers this question \\textit{i.e.} Given an unlabeled dataset and a set of candidate anomaly detectors, how can we select the most accurate model? To this end, we identify three classes of surrogate (unsupervised) metrics, namely, \\textit{prediction error}, \\textit{model centrality}, and \\textit{performance on injected synthetic anomalies}, and show that some metrics are highly correlated with standard supervised anomaly detection performance metrics such as the $F_1$ score, but to varying degrees. We formulate metric combination with multiple imperfect surrogate metrics as a robust rank aggregation problem. We then provide theoretical justification behind the proposed approach. Large-scale experiments on multiple real-world datasets demonstrate that our proposed unsupervised approach is as effective as selecting the most accurate model based on partially labeled data.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "General Machine Learning (ie none of the above)",
      "paperhash": "goswami|unsupervised_model_selection_for_time_series_anomaly_detection",
      "pdf": "/pdf/b9338f8e0cd4d78c188aa60e26ced6737232b2a8.pdf",
      "_bibtex": "@inproceedings{\ngoswami2023unsupervised,\ntitle={Unsupervised Model Selection for Time Series Anomaly Detection},\nauthor={Mononito Goswami and Cristian Ignacio Challu and Laurent Callot and Lenon Minorics and Andrey Kan},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=gOZ_pKANaPW}\n}",
      "supplementary_material": "",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279167194,
    "odate": 1664468100000,
    "details": {
      "replyCount": 20,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "kIPyTuEZuAK",
    "original": "fJYUUsypGi",
    "number": 1496,
    "cdate": 1663849968020,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849968020,
    "tmdate": 1677552947007,
    "tddate": null,
    "forum": "kIPyTuEZuAK",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics",
      "authorids": [
        "~Qing_Li1",
        "~Siyuan_Huang2",
        "~Yining_Hong1",
        "~Yixin_Zhu1",
        "~Ying_Nian_Wu1",
        "~Song-Chun_Zhu1"
      ],
      "authors": [
        "Qing Li",
        "Siyuan Huang",
        "Yining Hong",
        "Yixin Zhu",
        "Ying Nian Wu",
        "Song-Chun Zhu"
      ],
      "keywords": [
        "Systematic Generalization",
        "Concept Learning"
      ],
      "abstract": "Inspired by humans' exceptional ability to master arithmetic and generalize to new problems, we present a new dataset, HINT, to examine machines' capability of learning generalizable concepts at three levels: perception, syntax, and semantics. In HINT, machines are tasked with learning how concepts are perceived from raw signals such as images (i.e., perception), how multiple concepts are structurally combined to form a valid expression (i.e., syntax), and how concepts are realized to afford various reasoning tasks (i.e., semantics), all in a weakly supervised manner. Focusing on systematic generalization, we carefully design a five-fold test set to evaluate both the interpolation and the extrapolation of learned concepts w.r.t the three levels. Further, we design a few-shot learning split to determine whether or not models can rapidly learn new concepts and generalize them to more complex scenarios. To comprehend existing models' limitations, we undertake extensive experiments with various sequence-to-sequence models, including RNNs, Transformers, and GPT-3 (with the chain of thought prompting). The results indicate that current models struggle to extrapolate to long-range syntactic dependency and semantics. Models exhibit a considerable gap toward human-level generalization when evaluated with new concepts in a few-shot setting. Moreover, we discover that it is infeasible to solve HINT by merely scaling up the dataset and the model size; this strategy contributes little to the extrapolation of syntax and semantics. Finally, in zero-shot GPT-3 experiments, the chain of thought prompting exhibits impressive results and significantly boosts the test accuracy. We believe the HINT dataset and the experimental findings are of great interest to the learning community on systematic generalization.%",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Infrastructure (eg, datasets, competitions, implementations, libraries)",
      "paperhash": "li|a_minimalist_dataset_for_systematic_generalization_of_perception_syntax_and_semantics",
      "TL;DR": "We take inspiration from arithmetic and present a new benchmark for studying systematic generalization of perception, syntax, and semantics.",
      "pdf": "/pdf/09b973c9c84fd934195e0c087cb7af065e9c6829.pdf",
      "_bibtex": "@inproceedings{\nli2023a,\ntitle={A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics},\nauthor={Qing Li and Siyuan Huang and Yining Hong and Yixin Zhu and Ying Nian Wu and Song-Chun Zhu},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=kIPyTuEZuAK}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279167923,
    "odate": 1664468100000,
    "details": {
      "replyCount": 10,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "3F6I-0-57SC",
    "original": "vOSA1haxs3y",
    "number": 1520,
    "cdate": 1663849970677,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849970677,
    "tmdate": 1677664533625,
    "tddate": null,
    "forum": "3F6I-0-57SC",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "HiViT: A Simpler and More Efficient Design of Hierarchical Vision Transformer",
      "authorids": [
        "~Xiaosong_Zhang2",
        "~Yunjie_Tian1",
        "~Lingxi_Xie1",
        "~Wei_Huang11",
        "~Qi_Dai2",
        "~Qixiang_Ye1",
        "~Qi_Tian3"
      ],
      "authors": [
        "Xiaosong Zhang",
        "Yunjie Tian",
        "Lingxi Xie",
        "Wei Huang",
        "Qi Dai",
        "Qixiang Ye",
        "Qi Tian"
      ],
      "keywords": [
        "Hierarchical vision transformers",
        "self-supervised learning",
        "masked image modeling"
      ],
      "TL;DR": "A novel hierarchical vision transformer that is stronger and faster when applied to masked image modeling",
      "abstract": "There has been a debate on the choice of plain vs. hierarchical vision transformers, where researchers often believe that the former (e.g., ViT) has a simpler design but the latter (e.g., Swin) enjoys higher recognition accuracy. Recently, the emerge of masked image modeling (MIM), a self-supervised visual pre-training method, raised a new challenge to vision transformers in terms of flexibility, i.e., part of image patches or tokens are to be discarded, which seems to claim the advantages of plain vision transformers. In this paper, we delve deep into the comparison between ViT and Swin, revealing that (i) the performance gain of Swin is mainly brought by a deepened backbone and relative positional encoding, (ii) the hierarchical design of Swin can be simplified into hierarchical patch embedding (proposed in this work), and (iii) other designs such as shifted-window attentions can be removed. By removing the unnecessary operations, we come up with a new architecture named HiViT (short for hierarchical ViT), which is simpler and more efficient than Swin yet further improves its performance on fully-supervised and self-supervised visual representation learning. In particular, after pre-trained using masked autoencoder (MAE) on ImageNet-1K, HiViT-B reports a 84.6% accuracy on ImageNet-1K classification, a 53.3% box AP on COCO detection, and a 52.8% mIoU on ADE20K segmentation, significantly surpassing the baseline. Code is available at https://github.com/zhangxiaosong18/hivit.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Unsupervised and Self-supervised learning",
      "paperhash": "zhang|hivit_a_simpler_and_more_efficient_design_of_hierarchical_vision_transformer",
      "pdf": "/pdf/7835ef364a3e5f77397911e7f2f90b3aa3630f8b.pdf",
      "supplementary_material": "/attachment/1f35accd3be082244c68fb391909ec922537ad4a.zip",
      "_bibtex": "@inproceedings{\nzhang2023hivit,\ntitle={HiViT: A Simpler and More Efficient Design of Hierarchical Vision Transformer},\nauthor={Xiaosong Zhang and Yunjie Tian and Lingxi Xie and Wei Huang and Qi Dai and Qixiang Ye and Qi Tian},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=3F6I-0-57SC}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279169262,
    "odate": 1664468100000,
    "details": {
      "replyCount": 12,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "Mvetq8DO05O",
    "original": "MDxIbhXgKCw",
    "number": 1532,
    "cdate": 1663849972249,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849972249,
    "tmdate": 1760256786255,
    "tddate": null,
    "forum": "Mvetq8DO05O",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "A Laplace-inspired Distribution on SO(3) for Probabilistic Rotation Estimation",
      "authorids": [
        "~Yingda_Yin1",
        "~Yang_Wang34",
        "~He_Wang5",
        "~Baoquan_Chen1"
      ],
      "authors": [
        "Yingda Yin",
        "Yang Wang",
        "He Wang",
        "Baoquan Chen"
      ],
      "keywords": [],
      "abstract": "Estimating the 3DoF rotation from a single RGB image is an important yet challenging problem. Probabilistic rotation regression has raised more and more attention with the benefit of expressing uncertainty information along with the prediction. Though modeling noise using Gaussian-resembling Bingham distribution and matrix Fisher distribution is natural, they are shown to be sensitive to outliers for the nature of quadratic punishment to deviations. In this paper, we draw inspiration from multivariate Laplace distribution and propose a novel Rotation Laplace distribution on SO(3). Rotation Laplace distribution is robust to the disturbance of outliers and enforces much gradient to the low-error region, resulting in a better convergence. Our extensive experiments show that our proposed distribution achieves state-of-the-art performance for rotation regression tasks over both probabilistic and non-probabilistic baselines. Our project page is at pku-epic.github.io/RotationLaplace.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Probabilistic Methods (eg, variational inference, causal inference, Gaussian processes)",
      "paperhash": "yin|a_laplaceinspired_distribution_on_so3_for_probabilistic_rotation_estimation",
      "pdf": "/pdf/40fe2e0a65fc37b47b6e6110a4872759e47e20f0.pdf",
      "supplementary_material": "",
      "_bibtex": "@inproceedings{\nyin2023a,\ntitle={A Laplace-inspired Distribution on {SO}(3) for Probabilistic Rotation Estimation},\nauthor={Yingda Yin and Yang Wang and He Wang and Baoquan Chen},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=Mvetq8DO05O}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/a-laplace-inspired-distribution-on-so-for/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279169657,
    "odate": 1664468100000,
    "details": {
      "replyCount": 15,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "v69itrHLEu",
    "original": "KwqcuZAnBM",
    "number": 1563,
    "cdate": 1663849975927,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849975927,
    "tmdate": 1760256785117,
    "tddate": null,
    "forum": "v69itrHLEu",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Outcome-directed Reinforcement Learning by Uncertainty \\& Temporal Distance-Aware Curriculum Goal Generation",
      "authorids": [
        "~Daesol_Cho1",
        "~Seungjae_Lee2",
        "~H._Jin_Kim1"
      ],
      "authors": [
        "Daesol Cho",
        "Seungjae Lee",
        "H. Jin Kim"
      ],
      "keywords": [
        "Curriculum Learning",
        "Outcome-directed RL",
        "Goal-conditioned RL"
      ],
      "abstract": "Current reinforcement learning (RL) often suffers when solving a challenging exploration problem where the desired outcomes or high rewards are rarely observed. Even though curriculum RL, a framework that solves complex tasks by proposing a sequence of surrogate tasks, shows reasonable results, most of the previous works still have difficulty in proposing curriculum due to the absence of a mechanism for obtaining calibrated guidance to the desired outcome state without any prior domain knowledge. To alleviate it, we propose an uncertainty \\& temporal distance-aware curriculum goal generation method for the outcome-directed RL via solving a bipartite matching problem. It could not only provide precisely calibrated guidance of the curriculum to the desired outcome states but also bring much better sample efficiency and geometry-agnostic curriculum goal proposal capability compared to previous curriculum RL methods. We demonstrate that our algorithm significantly outperforms these prior methods in a variety of challenging navigation tasks and robotic manipulation tasks in a quantitative and qualitative way.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Reinforcement Learning (eg, decision and control, planning, hierarchical RL, robotics)",
      "paperhash": "cho|outcomedirected_reinforcement_learning_by_uncertainty_\\_temporal_distanceaware_curriculum_goal_generation",
      "pdf": "/pdf/2c6db8a4ff69ca681456fe5e686ec86f942d3e12.pdf",
      "supplementary_material": "/attachment/623faec2c3c315c0e2a00bb104f6674515b2fd74.zip",
      "_bibtex": "@inproceedings{\ncho2023outcomedirected,\ntitle={Outcome-directed Reinforcement Learning by Uncertainty {\\textbackslash}\\& Temporal Distance-Aware Curriculum Goal Generation},\nauthor={Daesol Cho and Seungjae Lee and H. Jin Kim},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=v69itrHLEu}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/outcome-directed-reinforcement-learning-by/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279171782,
    "odate": 1664468100000,
    "details": {
      "replyCount": 13,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "TD7AnQjNzR6",
    "original": "8IyLafgSxf",
    "number": 1578,
    "cdate": 1663849977610,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849977610,
    "tmdate": 1678055248234,
    "tddate": null,
    "forum": "TD7AnQjNzR6",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Statistical Efficiency of Score Matching: The View from Isoperimetry",
      "authorids": [
        "~Frederic_Koehler1",
        "aheckett@andrew.cmu.edu",
        "~Andrej_Risteski2"
      ],
      "authors": [
        "Frederic Koehler",
        "Alexander Heckett",
        "Andrej Risteski"
      ],
      "keywords": [
        "score matching",
        "log-Sobolev inequality",
        "isoperimetry",
        "relative efficiency",
        "sample complexity"
      ],
      "TL;DR": "We show a tight connection between the statistical efficiency of score matching and the isoperimetric properties (e.g. log-Sobolev constant) of the distribution being estimated",
      "abstract": "  Deep generative models parametrized up to a normalizing constant (e.g. energy-based models) are difficult to train by maximizing the likelihood of the data because the likelihood and/or gradients thereof cannot be explicitly or efficiently written down. Score matching is a training method, whereby instead of fitting the likelihood $\\log p(x)$ for the training data, we instead fit the score function $\\nabla_x \\log p(x)$ --- obviating the need to evaluate the partition function. Though this estimator is known to be consistent, its unclear whether (and when) its statistical efficiency is comparable to that of maximum likelihood --- which is known to be (asymptotically) optimal. We initiate this line of inquiry in this paper, and show a tight connection between statistical efficiency of score matching and the isoperimetric properties of the distribution being estimated --- i.e. the Poincar\\'e, log-Sobolev and isoperimetric constant --- quantities which govern the mixing time of Markov processes like Langevin dynamics. Roughly, we show that the score matching estimator is statistically comparable to the maximum likelihood when the  distribution has a small isoperimetric constant. Conversely, if the distribution has a large isoperimetric constant --- even for simple families of distributions like exponential families with rich enough sufficient statistics --- score matching will be substantially less efficient than maximum likelihood. We suitably formalize these results both in the finite sample regime, and in the asymptotic regime. Finally, we identify a direct parallel in the discrete setting, where we connect the statistical properties of pseudolikelihood estimation with approximate tensorization of entropy and the Glauber dynamics.\n",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Theory (eg, control theory, learning theory, algorithmic game theory)",
      "paperhash": "koehler|statistical_efficiency_of_score_matching_the_view_from_isoperimetry",
      "pdf": "/pdf/650e8b5c38872cf721fff2c0b10c3e5fa039579b.pdf",
      "_bibtex": "@inproceedings{\nkoehler2023statistical,\ntitle={Statistical Efficiency of Score Matching: The View from Isoperimetry},\nauthor={Frederic Koehler and Alexander Heckett and Andrej Risteski},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=TD7AnQjNzR6}\n}",
      "venue": "ICLR 2023 notable top 5%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279172308,
    "odate": 1664468100000,
    "details": {
      "replyCount": 9,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "iV9Cs8s8keU",
    "original": "GSoWegHc0C7",
    "number": 1599,
    "cdate": 1663849980160,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849980160,
    "tmdate": 1681537396719,
    "tddate": null,
    "forum": "iV9Cs8s8keU",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Learning the Positions in CountSketch",
      "authorids": [
        "~Yi_Li8",
        "~Honghao_Lin1",
        "~Simin_Liu1",
        "~Ali_Vakilian1",
        "~David_Woodruff1"
      ],
      "authors": [
        "Yi Li",
        "Honghao Lin",
        "Simin Liu",
        "Ali Vakilian",
        "David Woodruff"
      ],
      "keywords": [
        "learning-augmented sketches",
        "count-sketch",
        "low-rank approximation",
        "iterative Hessian sketch"
      ],
      "abstract": "We consider sketching algorithms which first compress data by multiplication with a random sketch matrix, and then apply the sketch to quickly solve an optimization problem, e.g., low-rank approximation and regression. In the learning-based sketching paradigm proposed by Indyk et al., the sketch matrix is found by choosing a random sparse matrix, e.g., CountSketch, and then the values of its non-zero entries are updated by running gradient descent on a training data set. Despite the growing body of work on this paradigm, a noticeable omission is that the locations of the non-zero entries of previous algorithms were fixed, and only their values were learned.\nIn this work, we propose the first learning-based algorithms that also optimize the locations of the non-zero entries. Our first proposed algorithm is based on a greedy algorithm. However, one drawback of the greedy algorithm is its slower training time. We fix this issue and propose approaches for learning a sketching matrix for both low-rank approximation and Hessian approximation for second-order optimization. The latter is helpful for a range of constrained optimization problems, such as LASSO and matrix estimation with a nuclear norm constraint. Both approaches achieve good accuracy with a fast running time.  Moreover, our experiments suggest that our algorithm can still reduce the error significantly even if we only have a very limited number of training matrices.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Optimization (eg, convex and non-convex optimization)",
      "paperhash": "li|learning_the_positions_in_countsketch",
      "TL;DR": "We propose the first learning-based algorithms that also optimize the locations of the non-zero entries of CountSketch matrix.",
      "pdf": "/pdf/cc22b9e7a5b739383f6f17c1c3f51a0cf1f79b66.pdf",
      "supplementary_material": "/attachment/054d7b89f7670546fb466c2b0237b83d4349eaae.zip",
      "_bibtex": "@inproceedings{\nli2023learning,\ntitle={Learning the Positions in CountSketch},\nauthor={Yi Li and Honghao Lin and Simin Liu and Ali Vakilian and David Woodruff},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=iV9Cs8s8keU}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279173874,
    "odate": 1664468100000,
    "details": {
      "replyCount": 10,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "MkbcAHIYgyS",
    "original": "0yFdYtru4h6",
    "number": 1606,
    "cdate": 1663849981048,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849981048,
    "tmdate": 1760256783328,
    "tddate": null,
    "forum": "MkbcAHIYgyS",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Mass-Editing Memory in a Transformer",
      "authorids": [
        "~Kevin_Meng1",
        "~Arnab_Sen_Sharma1",
        "~Alex_J_Andonian1",
        "~Yonatan_Belinkov1",
        "~David_Bau1"
      ],
      "authors": [
        "Kevin Meng",
        "Arnab Sen Sharma",
        "Alex J Andonian",
        "Yonatan Belinkov",
        "David Bau"
      ],
      "keywords": [
        "language models",
        "GPT",
        "transformers",
        "model editing",
        "factual associations",
        "memory"
      ],
      "TL;DR": "An algorithm that can make tens of thousands of edits to an autoregressive transformer's memory.",
      "abstract": "Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by an order of magnitude. Our code and data will be open-sourced upon publication.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Applications (eg, speech processing, computer vision, NLP)",
      "paperhash": "meng|massediting_memory_in_a_transformer",
      "pdf": "/pdf/5d2ff18d2f074c0f0b7bda40d118bb08e13bcd43.pdf",
      "supplementary_material": "/attachment/7163bba87b77b7773eaac5f74f70dcb09ed7097d.zip",
      "_bibtex": "@inproceedings{\nmeng2023massediting,\ntitle={Mass-Editing Memory in a Transformer},\nauthor={Kevin Meng and Arnab Sen Sharma and Alex J Andonian and Yonatan Belinkov and David Bau},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=MkbcAHIYgyS}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/mass-editing-memory-in-a-transformer/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279174094,
    "odate": 1664468100000,
    "details": {
      "replyCount": 11,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "OnD9zGAGT0k",
    "original": "GSfD7XLZNy",
    "number": 1619,
    "cdate": 1663849982650,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849982650,
    "tmdate": 1760256782841,
    "tddate": null,
    "forum": "OnD9zGAGT0k",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Diffusion Posterior Sampling for General Noisy Inverse Problems",
      "authorids": [
        "~Hyungjin_Chung1",
        "~Jeongsol_Kim1",
        "mccann@lanl.gov",
        "mklasky@lanl.gov",
        "~Jong_Chul_Ye1"
      ],
      "authors": [
        "Hyungjin Chung",
        "Jeongsol Kim",
        "Michael Thompson Mccann",
        "Marc Louis Klasky",
        "Jong Chul Ye"
      ],
      "keywords": [
        "Diffusion model",
        "Inverse problem",
        "Posterior sampling"
      ],
      "TL;DR": "We propose a diffusion model-based general inverse problem solver that scales to nonlinear problems and different noise statistics.",
      "abstract": "Diffusion models have been recently studied as powerful generative inverse problem solvers, owing to their high quality reconstructions and the ease of combining existing iterative solvers. However, most works focus on solving simple linear inverse problems in noiseless settings, which significantly under-represents the complexity of real-world problems. In this work, we extend diffusion solvers to efficiently handle general noisy (non)linear inverse problems via the Laplace approximation of the posterior sampling. Interestingly, the resulting posterior sampling scheme is a blended version of diffusion sampling with the manifold constrained gradient without a strict measurement consistency projection step, yielding a more desirable generative path in noisy settings compared to the previous studies. Our method demonstrates that diffusion models can incorporate various measurement noise statistics such as Gaussian and Poisson, and also efficiently handle noisy nonlinear inverse problems such as Fourier phase retrieval and non-uniform deblurring.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Generative models",
      "paperhash": "chung|diffusion_posterior_sampling_for_general_noisy_inverse_problems",
      "pdf": "/pdf/dd7f2e1f5581d91eb4c1ff34fec78b93d3dfa599.pdf",
      "_bibtex": "@inproceedings{\nchung2023diffusion,\ntitle={Diffusion Posterior Sampling for General Noisy Inverse Problems},\nauthor={Hyungjin Chung and Jeongsol Kim and Michael Thompson Mccann and Marc Louis Klasky and Jong Chul Ye},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=OnD9zGAGT0k}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/diffusion-posterior-sampling-for-general/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279175050,
    "odate": 1664468100000,
    "details": {
      "replyCount": 18,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "uqg3FhRZaq",
    "original": "nWTT9d7BsJq",
    "number": 1640,
    "cdate": 1663849985181,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849985181,
    "tmdate": 1676716570412,
    "tddate": null,
    "forum": "uqg3FhRZaq",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "On the complexity of nonsmooth automatic differentiation",
      "authorids": [
        "~Jerome_Bolte1",
        "~Ryan_Boustany1",
        "~Edouard_Pauwels1",
        "~Béatrice_Pesquet-Popescu1"
      ],
      "authors": [
        "Jerome Bolte",
        "Ryan Boustany",
        "Edouard Pauwels",
        "Béatrice Pesquet-Popescu"
      ],
      "keywords": [
        "Automatic differentiation",
        "nonsmooth derivatives",
        "computational complexity",
        "cheap derivatives",
        "conservative gradients"
      ],
      "TL;DR": "Backpropagation of nonsmooth  gradients is proved to be a fast/cheap process for the vast class of locally Lipschitz semi-algebraic functions.",
      "abstract": "Using the notion of conservative gradient, we provide a simple model to estimate the computational costs of the backward and forward modes of algorithmic differentiation for a wide class of nonsmooth programs. The complexity overhead of the backward mode turns out to be independent of the dimension when using programs with locally Lipschitz semi-algebraic or definable elementary functions. This extends considerably the Baur-Strassen's smooth cheap gradient principle. We illustrate our results by establishing fast backpropagation results of conservative gradients through feedforward neural networks with standard activation and  loss functions. Nonsmooth backpropagation's cheapness contrasts with concurrent forward approaches, which have, to this day, dimensional-dependent worst case  overhead estimates. We provide further results suggesting the superiority of backward propagation of conservative gradients. Indeed, we relate the complexity of computing a large number of directional derivatives to that of matrix multiplication, and we show that finding two subgradients in the Clarke subdifferential of a function is a NP-hard problem.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Optimization (eg, convex and non-convex optimization)",
      "paperhash": "bolte|on_the_complexity_of_nonsmooth_automatic_differentiation",
      "pdf": "/pdf/81b5f1858aeb447b1d248391272b9e30a7ceb511.pdf",
      "supplementary_material": "",
      "_bibtex": "@inproceedings{\nbolte2023on,\ntitle={On the complexity of nonsmooth automatic differentiation},\nauthor={Jerome Bolte and Ryan Boustany and Edouard Pauwels and B{\\'e}atrice Pesquet-Popescu},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=uqg3FhRZaq}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279176463,
    "odate": 1664468100000,
    "details": {
      "replyCount": 11,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "q3F0UBAruO",
    "original": "DkDf19WLnBt",
    "number": 1657,
    "cdate": 1663849987191,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849987191,
    "tmdate": 1677603044714,
    "tddate": null,
    "forum": "q3F0UBAruO",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Towards Effective and Interpretable Human-Agent Collaboration in MOBA Games: A Communication Perspective",
      "authorids": [
        "~Yiming_Gao4",
        "~Feiyu_Liu1",
        "~Liang_Wang10",
        "~Zhenjie_Lian1",
        "~Weixuan_Wang1",
        "~Siqin_Li1",
        "~Xianliang_Wang1",
        "~Xianhan_Zeng1",
        "~Rundong_Wang1",
        "~jiawei_wang2",
        "~QIANG_FU8",
        "~Yang_Wei2",
        "~Lanxiao_Huang1",
        "~Wei_Liu3"
      ],
      "authors": [
        "Yiming Gao",
        "Feiyu Liu",
        "Liang Wang",
        "Zhenjie Lian",
        "Weixuan Wang",
        "Siqin Li",
        "Xianliang Wang",
        "Xianhan Zeng",
        "Rundong Wang",
        "jiawei wang",
        "QIANG FU",
        "Yang Wei",
        "Lanxiao Huang",
        "Wei Liu"
      ],
      "keywords": [
        "game playing",
        "multi-agent",
        "human-ai communication",
        "human-ai collaboration",
        "deep reinforcement learning"
      ],
      "TL;DR": "We propose an efficient and interpretable Meta-Command Communication-based (MCC) framework for accomplishing effective human-AI collaboration in MOBA games. ",
      "abstract": "MOBA games, e.g., Dota2 and Honor of Kings, have been actively used as the testbed for the recent AI research on games, and various AI systems have been developed at the human level so far. However, these AI systems mainly focus on how to compete with humans, less on exploring how to collaborate with humans. To this end, this paper makes the first attempt to investigate human-agent collaboration in MOBA games. In this paper, we propose to enable humans and agents to collaborate through explicit communication by designing an efficient and interpretable Meta-Command Communication-based framework, dubbed MCC, for accomplishing effective human-agent collaboration in MOBA games. The MCC framework consists of two pivotal modules: 1) an interpretable communication protocol, i.e., the Meta-Command, to bridge the communication gap between humans and agents; 2) a meta-command value estimator, i.e., the Meta-Command Selector, to select a valuable meta-command for each agent to achieve effective human-agent collaboration. Experimental results in Honor of Kings demonstrate that MCC agents can collaborate reasonably well with human teammates and even generalize to collaborate with different levels and numbers of human teammates. Videos are available at https://sites.google.com/view/mcc-demo.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Reinforcement Learning (eg, decision and control, planning, hierarchical RL, robotics)",
      "paperhash": "gao|towards_effective_and_interpretable_humanagent_collaboration_in_moba_games_a_communication_perspective",
      "pdf": "/pdf/05be94f6c3da0d1f97a06aaecf42515ddc07d159.pdf",
      "_bibtex": "@inproceedings{\ngao2023towards,\ntitle={Towards Effective and Interpretable Human-Agent Collaboration in {MOBA} Games: A Communication Perspective},\nauthor={Yiming Gao and Feiyu Liu and Liang Wang and Zhenjie Lian and Weixuan Wang and Siqin Li and Xianliang Wang and Xianhan Zeng and Rundong Wang and jiawei wang and QIANG FU and Yang Wei and Lanxiao Huang and Wei Liu},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=q3F0UBAruO}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279177418,
    "odate": 1664468100000,
    "details": {
      "replyCount": 19,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "isiQ5KIXbjj",
    "original": "bmsgPWA3S5O",
    "number": 1686,
    "cdate": 1663849990736,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849990736,
    "tmdate": 1760256778820,
    "tddate": null,
    "forum": "isiQ5KIXbjj",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "QuAnt: Quantum Annealing with Learnt Couplings",
      "authorids": [
        "~Marcel_Seelbach_Benkner1",
        "~Maximilian_Krahn1",
        "~Edith_Tretschk1",
        "~Zorah_Lähner1",
        "~Michael_Moeller1",
        "~Vladislav_Golyanik1"
      ],
      "authors": [
        "Marcel Seelbach Benkner",
        "Maximilian Krahn",
        "Edith Tretschk",
        "Zorah Lähner",
        "Michael Moeller",
        "Vladislav Golyanik"
      ],
      "keywords": [],
      "abstract": "Modern quantum annealers can find high-quality solutions to combinatorial optimisation objectives given as quadratic unconstrained binary optimisation (QUBO) problems. Unfortunately, obtaining suitable QUBO forms in computer vision remains challenging and currently requires problem-specific analytical derivations. Moreover, such explicit formulations impose tangible constraints on solution encodings. In stark contrast to prior work, this paper proposes to learn QUBO forms from data through gradient backpropagation instead of deriving them. As a result, the solution encodings can be  chosen flexibly and compactly. Furthermore, our methodology is general and virtually independent of the specifics of the target problem type. We demonstrate the advantages of learnt  QUBOs on the diverse problem types of graph matching, 2D point cloud alignment and 3D rotation estimation. Our results are competitive with the previous quantum state of the art while requiring much fewer logical and physical qubits, enabling our method to scale to larger problems. The code and the new dataset are available at https://4dqv.mpi-inf.mpg.de/QuAnt/.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "benkner|quant_quantum_annealing_with_learnt_couplings",
      "pdf": "/pdf/abe676c41d571977f05ff1eee049ec4fb86d0301.pdf",
      "supplementary_material": "/attachment/69bcf435d6d94f220c81aabae3488d501d198582.zip",
      "_bibtex": "@inproceedings{\nbenkner2023quant,\ntitle={QuAnt: Quantum Annealing with Learnt Couplings},\nauthor={Marcel Seelbach Benkner and Maximilian Krahn and Edith Tretschk and Zorah L{\\\"a}hner and Michael Moeller and Vladislav Golyanik},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=isiQ5KIXbjj}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/quant-quantum-annealing-with-learnt-couplings/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279179046,
    "odate": 1664468100000,
    "details": {
      "replyCount": 9,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "IxmWsm4xrua",
    "original": "eV5XZpzAlO0",
    "number": 1740,
    "cdate": 1663849997232,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663849997232,
    "tmdate": 1760256776723,
    "tddate": null,
    "forum": "IxmWsm4xrua",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Toeplitz Neural Network for Sequence Modeling",
      "authorids": [
        "~Zhen_Qin6",
        "~Xiaodong_Han3",
        "~Weixuan_Sun1",
        "~Bowen_He3",
        "~Dong_Li11",
        "~Dongxu_Li2",
        "~Yuchao_Dai1",
        "~Lingpeng_Kong1",
        "~Yiran_Zhong1"
      ],
      "authors": [
        "Zhen Qin",
        "Xiaodong Han",
        "Weixuan Sun",
        "Bowen He",
        "Dong Li",
        "Dongxu Li",
        "Yuchao Dai",
        "Lingpeng Kong",
        "Yiran Zhong"
      ],
      "keywords": [
        "Toeplitz Matrix",
        "Sequence Modeling",
        "Relative position"
      ],
      "abstract": "Sequence modeling has important applications in natural language processing and computer vision. Recently, the transformer-based models have shown strong performance on various sequence modeling tasks, which rely on attention to capture pairwise token relations, and position embedding to inject positional information. While showing good performance, the transformer models are inefficient to scale to long input sequences, mainly due to the quadratic space-time complexity of attention. To overcome this inefficiency, we propose to model sequences with a relative position encoded Toeplitz matrix and use a Toeplitz matrix-vector production trick to reduce the space-time complexity of the sequence modeling to log linear. A lightweight sub-network called relative position encoder is proposed to generate relative position coefficients with a fixed budget of parameters, enabling the proposed Toeplitz neural network to deal with varying sequence lengths. In addition, despite being trained on 512-token sequences, our model can extrapolate input sequence length up to 14K tokens in inference with consistent performance. Extensive experiments on autoregressive and bidirectional language modeling, image modeling, and the challenging Long-range Arena Benchmark show that our method achieves better performance than its competitors in most downstream tasks while being significantly faster.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "qin|toeplitz_neural_network_for_sequence_modeling",
      "TL;DR": "An efficient method that uses Toeplitz matrices to model sequences.",
      "pdf": "/pdf/2a7e1fcbcfe67f92df33295ecde966d4a9095dda.pdf",
      "_bibtex": "@inproceedings{\nqin2023toeplitz,\ntitle={Toeplitz Neural Network for Sequence Modeling},\nauthor={Zhen Qin and Xiaodong Han and Weixuan Sun and Bowen He and Dong Li and Dongxu Li and Yuchao Dai and Lingpeng Kong and Yiran Zhong},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=IxmWsm4xrua}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/toeplitz-neural-network-for-sequence-modeling/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279182048,
    "odate": 1664468100000,
    "details": {
      "replyCount": 19,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "vhFu1Acb0xb",
    "original": "RxFuN-y-7kJ",
    "number": 1759,
    "cdate": 1663850000565,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663850000565,
    "tmdate": 1760256775989,
    "tddate": null,
    "forum": "vhFu1Acb0xb",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Transformers are Sample-Efficient World Models",
      "authorids": [
        "~Vincent_Micheli1",
        "~Eloi_Alonso1",
        "~François_Fleuret2"
      ],
      "authors": [
        "Vincent Micheli",
        "Eloi Alonso",
        "François Fleuret"
      ],
      "keywords": [
        "deep learning",
        "reinforcement learning",
        "model-based reinforcement learning",
        "world models",
        "learning in imagination",
        "transformers",
        "discrete autoencoders",
        "generative modeling",
        "sequence modeling"
      ],
      "TL;DR": "We introduce a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer.",
      "abstract": "Deep reinforcement learning agents are notoriously sample inefficient, which considerably limits their application to real-world problems. Recently, many model-based methods have been designed to address this issue, with learning in the imagination of a world model being one of the most prominent approaches. However, while virtually unlimited interaction with a simulated environment sounds appealing, the world model has to be accurate over extended periods of time. Motivated by the success of Transformers in sequence modeling tasks, we introduce IRIS, a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer. With the equivalent of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, and outperforms humans on 10 out of 26 games, setting a new state of the art for methods without lookahead search. To foster future research on Transformers and world models for sample-efficient reinforcement learning, we release our code and models at https://github.com/eloialonso/iris.",
      "pdf": "/pdf/f23ea2080e754e26ad7f8a9f9a55865dd11f0a73.pdf",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Reinforcement Learning (eg, decision and control, planning, hierarchical RL, robotics)",
      "paperhash": "micheli|transformers_are_sampleefficient_world_models",
      "supplementary_material": "/attachment/b4793bcf47c6682c198e0bc06365b298c27344f9.zip",
      "_bibtex": "@inproceedings{\nmicheli2023transformers,\ntitle={Transformers are Sample-Efficient World Models},\nauthor={Vincent Micheli and Eloi Alonso and Fran{\\c{c}}ois Fleuret},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=vhFu1Acb0xb}\n}",
      "venue": "ICLR 2023 notable top 5%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/transformers-are-sample-efficient-world/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279182978,
    "odate": 1664468100000,
    "details": {
      "replyCount": 12,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "S07feAlQHgM",
    "original": "J75E_YoloIy",
    "number": 1762,
    "cdate": 1663850000929,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663850000929,
    "tmdate": 1760256775670,
    "tddate": null,
    "forum": "S07feAlQHgM",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "A Model or 603 Exemplars: Towards Memory-Efficient Class-Incremental Learning",
      "authorids": [
        "~Da-Wei_Zhou1",
        "~Qi-Wei_Wang1",
        "~Han-Jia_Ye1",
        "~De-Chuan_Zhan1"
      ],
      "authors": [
        "Da-Wei Zhou",
        "Qi-Wei Wang",
        "Han-Jia Ye",
        "De-Chuan Zhan"
      ],
      "keywords": [
        "class-incremental learning"
      ],
      "abstract": "Real-world applications require the classification model to adapt to new classes without forgetting old ones. Correspondingly, Class-Incremental Learning (CIL) aims to train a model with limited memory size to meet this requirement. Typical CIL methods tend to save representative exemplars from former classes to resist forgetting, while recent works find that storing models from history can substantially boost the performance. However, the stored models are not counted into the memory budget, which implicitly results in unfair comparisons. We find that when counting the model size into the total budget and comparing methods with aligned memory size, saving models do not consistently work, especially for the case with limited memory budgets. As a result, we need to holistically evaluate different CIL methods at different memory scales and simultaneously consider accuracy and memory size for measurement. On the other hand, we dive deeply into the construction of the memory buffer for memory efficiency. By analyzing the effect of different layers in the network, we find that shallow and deep layers have different characteristics in CIL. Motivated by this, we propose a simple yet effective baseline, denoted as MEMO for Memory-efficient Expandable MOdel. MEMO extends specialized layers based on the shared generalized representations, efficiently extracting diverse representations with modest cost and maintaining representative exemplars. Extensive experiments on benchmark datasets validate MEMO's competitive performance. Code is available at: https://github.com/wangkiw/ICLR23-MEMO",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "zhou|a_model_or_603_exemplars_towards_memoryefficient_classincremental_learning",
      "pdf": "/pdf/1b652f4ee2aba1681f8d0e268557ff8ce743d37d.pdf",
      "_bibtex": "@inproceedings{\nzhou2023a,\ntitle={A Model or 603 Exemplars: Towards Memory-Efficient Class-Incremental Learning},\nauthor={Da-Wei Zhou and Qi-Wei Wang and Han-Jia Ye and De-Chuan Zhan},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=S07feAlQHgM}\n}",
      "supplementary_material": "",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 3 code implementations](https://www.catalyzex.com/paper/a-model-or-603-exemplars-towards-memory/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279183192,
    "odate": 1664468100000,
    "details": {
      "replyCount": 19,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "VELL0PlWfc",
    "original": "OotgzXvP81j",
    "number": 1774,
    "cdate": 1663850002458,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663850002458,
    "tmdate": 1760256775266,
    "tddate": null,
    "forum": "VELL0PlWfc",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Tailoring Language Generation Models under Total Variation Distance",
      "authorids": [
        "~Haozhe_Ji2",
        "~Pei_Ke2",
        "~Zhipeng_Hu1",
        "~Rongsheng_Zhang1",
        "~Minlie_Huang1"
      ],
      "authors": [
        "Haozhe Ji",
        "Pei Ke",
        "Zhipeng Hu",
        "Rongsheng Zhang",
        "Minlie Huang"
      ],
      "keywords": [
        "language generation",
        "maximum likelihood estimation",
        "total variation distance",
        "text degeneration"
      ],
      "TL;DR": "We analyze total variation distance (TVD) as a robust metric to outliers and devise a new training objective based on TVD to alleviate text degeneration and improve the generation quality.",
      "abstract": "The standard paradigm of neural language generation adopts maximum likelihood estimation (MLE) as the optimizing method. From a distributional view, MLE in fact minimizes the Kullback-Leibler divergence (KLD) between the distribution of the real data and that of the model. However, this approach forces the model to distribute non-zero (sometimes large) probability mass to all training samples regardless of their quality. Moreover, in the attempt to cover the low-probability regions in the data distribution, the model systematically overestimates the probability of corrupted text sequences, which we conjecture is one of the main reasons for text degeneration during autoregressive decoding. To remedy this problem, we leverage the total variation distance (TVD) with its robustness to outliers, and develop practical bounds to apply it to language generation. Then, we introduce the TaiLr objective that balances the tradeoff of estimating TVD. Intuitively, TaiLr downweights real data samples that have low model probabilities with tunable penalization intensity. Experimental results show that our method alleviates the overestimation of degenerated sequences without sacrificing diversity and improves generation quality on a wide range of text generation tasks.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Applications (eg, speech processing, computer vision, NLP)",
      "paperhash": "ji|tailoring_language_generation_models_under_total_variation_distance",
      "pdf": "/pdf/222b0c66b1d6e4c664fc67e8d5d1348ae37c505e.pdf",
      "supplementary_material": "/attachment/ef08f4bee8dfab1f96ef459c9101e3e802a1e7fd.zip",
      "_bibtex": "@inproceedings{\nji2023tailoring,\ntitle={Tailoring Language Generation Models under Total Variation Distance},\nauthor={Haozhe Ji and Pei Ke and Zhipeng Hu and Rongsheng Zhang and Minlie Huang},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=VELL0PlWfc}\n}",
      "venue": "ICLR 2023 notable top 5%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 4 code implementations](https://www.catalyzex.com/paper/tailoring-language-generation-models-under/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279183953,
    "odate": 1664468100000,
    "details": {
      "replyCount": 11,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "4eJ43EN2g6l",
    "original": "sUuOIzHZGWo",
    "number": 1783,
    "cdate": 1663850003531,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663850003531,
    "tmdate": 1677632877994,
    "tddate": null,
    "forum": "4eJ43EN2g6l",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "SketchKnitter: Vectorized Sketch Generation with Diffusion Models",
      "authorids": [
        "wanqqiang@bupt.edu.cn",
        "denghaoge@bupt.edu.cn",
        "~Yonggang_Qi2",
        "~Da_Li3",
        "~Yi-Zhe_Song2"
      ],
      "authors": [
        "Qiang Wang",
        "Haoge Deng",
        "Yonggang Qi",
        "Da Li",
        "Yi-Zhe Song"
      ],
      "keywords": [],
      "abstract": "We show vectorized sketch generation can be identified as a reversal of the stroke deformation process. This relationship was established by means of a diffusion model that learns data distributions over the stroke-point locations and pen states of real human sketches. Given randomly scattered stroke-points, sketch generation becomes a process of deformation-based denoising, where the generator rectifies positions of stroke points at each timestep to converge at a recognizable sketch. A key innovation was to embed recognizability into the reverse time diffusion process. It was observed that the estimated noise during the reversal process is strongly correlated with sketch classification accuracy. An auxiliary recurrent neural network (RNN) was consequently used to quantify recognizability during data sampling. It follows that, based on the recognizability scores, a sampling shortcut function can also be devised that renders better quality sketches with fewer sampling steps. Finally it is shown that the model can be easily extended to a conditional generation framework, where given incomplete and unfaithful sketches, it yields one that is more visually appealing and with higher recognizability.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Generative models",
      "paperhash": "wang|sketchknitter_vectorized_sketch_generation_with_diffusion_models",
      "pdf": "/pdf/aa51f28767b5d95ceced7af0c79780b06d2fd1e0.pdf",
      "_bibtex": "@inproceedings{\nwang2023sketchknitter,\ntitle={SketchKnitter: Vectorized Sketch Generation with Diffusion Models},\nauthor={Qiang Wang and Haoge Deng and Yonggang Qi and Da Li and Yi-Zhe Song},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=4eJ43EN2g6l}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279184605,
    "odate": 1664468100000,
    "details": {
      "replyCount": 10,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "rZ-wylY5VI",
    "original": "zkHShZAE2D",
    "number": 1817,
    "cdate": 1663850007590,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663850007590,
    "tmdate": 1760256774117,
    "tddate": null,
    "forum": "rZ-wylY5VI",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Programmatically Grounded, Compositionally Generalizable Robotic Manipulation",
      "authorids": [
        "~Renhao_Wang1",
        "~Jiayuan_Mao1",
        "~Joy_Hsu2",
        "~Hang_Zhao1",
        "~Jiajun_Wu1",
        "~Yang_Gao1"
      ],
      "authors": [
        "Renhao Wang",
        "Jiayuan Mao",
        "Joy Hsu",
        "Hang Zhao",
        "Jiajun Wu",
        "Yang Gao"
      ],
      "keywords": [
        "Vision-Language-Action Grounding",
        "Zero-Shot Generalization",
        "Compositional Generalization",
        "Neurosymbolic Learning"
      ],
      "TL;DR": "We parse and execute semantically grounded neural programs for robotic manipulation, enabling better zero-shot and compositional generalizable to new manipulation behaviors.",
      "abstract": "Robots operating in the real world require both rich manipulation skills as well as the ability to semantically reason about when to apply those skills. Towards this goal, recent works have integrated semantic representations from large-scale pretrained vision-language (VL) models into manipulation models, imparting them with more general reasoning capabilities. However, we show that the conventional {\\it pretraining-finetuning} pipeline for integrating such representations entangles the learning of domain-specific action information and domain-general visual information, leading to less data-efficient training and poor generalization to unseen objects and tasks. To this end, we propose \\ours, a {\\it modular} approach to better leverage pretrained VL models by exploiting the syntactic and semantic structures of language instructions. Our framework uses a semantic parser to recover an executable program, composed of functional modules grounded on vision and action across different modalities. Each functional module is realized as a combination of deterministic computation and learnable neural networks. Program execution produces parameters to general manipulation primitives for a robotic end-effector. The entire modular network can be trained with end-to-end imitation learning objectives. Experiments show that our model successfully disentangles action and perception, translating to improved zero-shot and compositional generalization in a variety of manipulation behaviors. Project webpage at: \\url{https://progport.github.io}.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Reinforcement Learning (eg, decision and control, planning, hierarchical RL, robotics)",
      "paperhash": "wang|programmatically_grounded_compositionally_generalizable_robotic_manipulation",
      "pdf": "/pdf/3f955a8cf103d552d11f9329dde46d6173f574aa.pdf",
      "_bibtex": "@inproceedings{\nwang2023programmatically,\ntitle={Programmatically Grounded, Compositionally Generalizable Robotic Manipulation},\nauthor={Renhao Wang and Jiayuan Mao and Joy Hsu and Hang Zhao and Jiajun Wu and Yang Gao},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=rZ-wylY5VI}\n}",
      "venue": "ICLR 2023 notable top 25%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/programmatically-grounded-compositionally/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279186177,
    "odate": 1664468100000,
    "details": {
      "replyCount": 5,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "DeG07_TcZvT",
    "original": "2lVVtGpAa3d",
    "number": 1818,
    "cdate": 1663850007704,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663850007704,
    "tmdate": 1760256773965,
    "tddate": null,
    "forum": "DeG07_TcZvT",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task",
      "authorids": [
        "~Kenneth_Li1",
        "~Aspen_K_Hopkins1",
        "~David_Bau1",
        "~Fernanda_Viégas1",
        "~Hanspeter_Pfister1",
        "~Martin_Wattenberg1"
      ],
      "authors": [
        "Kenneth Li",
        "Aspen K Hopkins",
        "David Bau",
        "Fernanda Viégas",
        "Hanspeter Pfister",
        "Martin Wattenberg"
      ],
      "keywords": [
        "world representation",
        "GPT"
      ],
      "abstract": "Language models show a surprising range of capabilities, but the source of their apparent competence is unclear. Do these networks just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see? We investigate this question by applying a variant of the GPT model to the task of predicting legal moves in a simple board game, Othello. Although the network has no a priori knowledge of the game or its rules, we uncover evidence of an emergent nonlinear internal representation of the board state. Interventional experiments indicate this representation can be used to control the output of the network and create \"latent saliency maps\" that can help explain predictions in human terms.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Deep Learning and representational learning",
      "paperhash": "li|emergent_world_representations_exploring_a_sequence_model_trained_on_a_synthetic_task",
      "pdf": "/pdf/70fb51a26cffdf3304e24f4d2e803b729904fe20.pdf",
      "supplementary_material": "",
      "_bibtex": "@inproceedings{\nli2023emergent,\ntitle={Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task},\nauthor={Kenneth Li and Aspen K Hopkins and David Bau and Fernanda Vi{\\'e}gas and Hanspeter Pfister and Martin Wattenberg},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=DeG07_TcZvT}\n}",
      "venue": "ICLR 2023 notable top 5%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/emergent-world-representations-exploring-a/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279186211,
    "odate": 1664468100000,
    "details": {
      "replyCount": 14,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  },
  {
    "id": "LFHFQbjxIiP",
    "original": "AaUiSI7nhl0",
    "number": 1842,
    "cdate": 1663850010357,
    "mdate": null,
    "ddate": null,
    "tcdate": 1663850010357,
    "tmdate": 1760256773286,
    "tddate": null,
    "forum": "LFHFQbjxIiP",
    "replyto": null,
    "invitation": "ICLR.cc/2023/Conference/-/Blind_Submission",
    "content": {
      "title": "Conditional Antibody Design as 3D Equivariant Graph Translation",
      "authorids": [
        "~Xiangzhe_Kong1",
        "~Wenbing_Huang1",
        "~Yang_Liu19"
      ],
      "authors": [
        "Xiangzhe Kong",
        "Wenbing Huang",
        "Yang Liu"
      ],
      "keywords": [
        "conditional antibody generation",
        "equivariant",
        "multi-channel attention"
      ],
      "abstract": "Antibody design is valuable for therapeutic usage and biological research. Existing deep-learning-based methods encounter several key issues: 1) incomplete context for Complementarity-Determining Regions (CDRs) generation; 2) incapability of capturing the entire 3D geometry of the input structure; 3) inefficient prediction of the CDR sequences in an autoregressive manner. In this paper, we propose Multi-channel Equivariant Attention Network (MEAN) to co-design 1D sequences and 3D structures of CDRs. To be specific, MEAN formulates antibody design as a conditional graph translation problem by importing extra components including the target antigen and the light chain of the antibody. Then, MEAN resorts to E(3)-equivariant message passing along with a proposed attention mechanism to better capture the geometrical correlation between different components. Finally, it outputs both the 1D sequences and 3D structure via a multi-round progressive full-shot scheme, which enjoys more efficiency and precision against previous autoregressive approaches. Our method significantly surpasses state-of-the-art models in sequence and structure modeling, antigen-binding CDR design, and binding affinity optimization. Specifically, the relative improvement to baselines is about 23\\% in antigen-binding CDR design and 34\\% for affinity optimization.",
      "anonymous_url": "I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.",
      "no_acknowledgement_section": "I certify that there is no acknowledgement section in this submission for double blind review.",
      "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
      "submission_guidelines": "Yes",
      "resubmission": "",
      "student_author": "",
      "Please_choose_the_closest_area_that_your_submission_falls_into": "Machine Learning for Sciences (eg biology, physics, health sciences, social sciences, climate/sustainability )",
      "paperhash": "kong|conditional_antibody_design_as_3d_equivariant_graph_translation",
      "pdf": "/pdf/3ad0b04b8a9b31f816c7c80ce0cf71fad13fa636.pdf",
      "_bibtex": "@inproceedings{\nkong2023conditional,\ntitle={Conditional Antibody Design as 3D Equivariant Graph Translation},\nauthor={Xiangzhe Kong and Wenbing Huang and Yang Liu},\nbooktitle={The Eleventh International Conference on Learning Representations },\nyear={2023},\nurl={https://openreview.net/forum?id=LFHFQbjxIiP}\n}",
      "venue": "ICLR 2023 notable top 5%",
      "venueid": "ICLR.cc/2023/Conference",
      "community_implementations": "[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/conditional-antibody-design-as-3d-equivariant/code)"
    },
    "signatures": [
      "ICLR.cc/2023/Conference"
    ],
    "readers": [
      "everyone"
    ],
    "nonreaders": [],
    "writers": [
      "ICLR.cc/2023/Conference"
    ],
    "pdate": 1675279187169,
    "odate": 1664468100000,
    "details": {
      "replyCount": 12,
      "invitation": {
        "reply": {
          "writers": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "signatures": {
            "values": [
              "ICLR.cc/2023/Conference"
            ]
          },
          "content": {
            "title": {
              "value-regex": ".*",
              "order": 1
            },
            "authors": {
              "values": [
                "Anonymous"
              ],
              "order": 2
            },
            "authorids": {
              "values-regex": ".*",
              "order": 3
            },
            "keywords": {
              "value-regex": ".*",
              "order": 6
            },
            "TL;DR": {
              "value-regex": ".*",
              "order": 7
            },
            "abstract": {
              "value-regex": ".*",
              "order": 8
            },
            "pdf": {
              "value-regex": ".*",
              "order": 9
            },
            "anonymous_url": {
              "value-regex": ".*",
              "order": 10
            },
            "no_acknowledgement_section": {
              "value-regex": ".*",
              "order": 11
            },
            "supplementary_material": {
              "value-regex": ".*",
              "order": 12
            },
            "code_of_ethics": {
              "value-regex": ".*",
              "order": 13
            },
            "submission_guidelines": {
              "value-regex": ".*",
              "order": 14
            },
            "resubmission": {
              "value-regex": ".*",
              "order": 15
            },
            "student_author": {
              "value-regex": ".*",
              "order": 16
            },
            "Please_choose_the_closest_area_that_your_submission_falls_into": {
              "value-regex": ".*",
              "order": 17
            },
            "community_implementations": {
              "required": false,
              "description": "Optional link to open source implementations",
              "value-regex": ".*",
              "markdown": true,
              "order": 103
            }
          }
        },
        "replyForumViews": [],
        "expdate": 1682960123388,
        "signatures": [
          "ICLR.cc/2023/Conference"
        ],
        "readers": [
          "everyone"
        ],
        "writers": [
          "ICLR.cc/2023/Conference"
        ],
        "invitees": [
          "ICLR.cc/2023/Conference"
        ],
        "tcdate": 1663849799994,
        "tmdate": 1682960123442,
        "id": "ICLR.cc/2023/Conference/-/Blind_Submission",
        "type": "note"
      }
    }
  }
]